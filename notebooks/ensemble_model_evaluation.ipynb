{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ETV-tAYAlKO"
      },
      "source": [
        "# **ENSEMBLE EVALUATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMUtV0eoArdD"
      },
      "source": [
        "## 1. **SETUP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0rWqj4yAwUA"
      },
      "source": [
        "### 1.1 Clone the repository from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i6gPpBU2GZ8",
        "outputId": "ccb63b8a-a07d-4f80-9856-01886f455342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DLA_LLMSANALYSIS'...\n",
            "remote: Enumerating objects: 529, done.\u001b[K\n",
            "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 529 (delta 150), reused 151 (delta 77), pack-reused 301 (from 1)\u001b[K\n",
            "Receiving objects: 100% (529/529), 188.32 KiB | 2.51 MiB/s, done.\n",
            "Resolving deltas: 100% (368/368), done.\n",
            "/content/DLA_LLMSANALYSIS\n"
          ]
        }
      ],
      "source": [
        "!test -d DLA_LLMSANALYSIS && rm -rf DLA_LLMSANALYSIS\n",
        "!git clone https://github.com/wakaflocka17/DLA_LLMSANALYSIS.git\n",
        "%cd DLA_LLMSANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWS3UkOWA0la"
      },
      "source": [
        "### 1.2 Create a virtual environment for install all packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hspGseQ82QFt",
        "outputId": "5794effe-a21d-4ba6-8af5-adfe754c426f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (3.18.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (4.3.8)\n",
            "Downloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.9 virtualenv-20.31.2\n",
            "created virtual environment CPython3.11.12.final.0-64 in 362ms\n",
            "  creator CPython3Posix(dest=/content/DLA_LLMSANALYSIS/venv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==25.1.1, setuptools==80.3.1\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ],
      "source": [
        "!pip install virtualenv\n",
        "!python -m virtualenv venv\n",
        "!source venv/bin/activate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA0ThjYtA6ml"
      },
      "source": [
        "### 1.3 Install all requirements defined in the 'requirements.txt' file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XznV7cBT2TXV",
        "outputId": "07ef6ee6-ff66-4bd2-c1c7-9d4ea1b9bb41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: venv/bin/pip-compile: No such file or directory\n",
            "Collecting transformers==4.28.0 (from -r requirements.in (line 2))\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl.metadata (109 kB)\n",
            "Collecting optimum==1.20.0 (from optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading optimum-1.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting accelerate>=0.20.1 (from -r requirements.in (line 8))\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting filelock (from transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy>=1.17 (from transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting packaging>=20.0 (from transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pyyaml>=5.1 (from transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tqdm>=4.27 (from transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting coloredlogs (from optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting sympy (from optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting torch>=1.11 (from optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting datasets (from optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting onnx (from optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting onnxruntime>=1.11.0 (from optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting evaluate (from optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting protobuf>=3.20.1 (from optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading hf_xet-1.1.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting protobuf>=3.20.1 (from optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading protobuf-3.20.2-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting psutil (from accelerate>=0.20.1->-r requirements.in (line 8))\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting pyarrow>=15.0.0 (from datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting xxhash (from datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
            "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting flatbuffers (from onnxruntime>=1.11.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers==4.28.0->-r requirements.in (line 2))\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting networkx (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in ./venv/lib/python3.11/site-packages (from triton==3.3.0->torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5)) (80.3.1)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "INFO: pip is looking at multiple versions of onnx to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting onnx (from optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.20.0->optimum[onnxruntime]==1.20.0->-r requirements.in (line 5))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.20.0-py3-none-any.whl (418 kB)\n",
            "Downloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
            "Downloading hf_xet-1.1.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.5/25.5 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.2-py2.py3-none-any.whl (162 kB)\n",
            "Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
            "Downloading pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
            "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Installing collected packages: tokenizers, sentencepiece, pytz, nvidia-cusparselt-cu12, mpmath, flatbuffers, xxhash, urllib3, tzdata, typing-extensions, triton, tqdm, sympy, six, regex, pyyaml, pyarrow, psutil, protobuf, propcache, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, idna, humanfriendly, hf-xet, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, python-dateutil, onnx, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, jinja2, coloredlogs, aiosignal, pandas, onnxruntime, nvidia-cusolver-cu12, huggingface-hub, aiohttp, transformers, torch, datasets, accelerate, optimum, evaluate\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67/67\u001b[0m [evaluate]\n",
            "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 accelerate-0.20.3 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 attrs-25.3.0 certifi-2025.4.26 charset-normalizer-3.4.2 coloredlogs-15.0.1 datasets-3.6.0 dill-0.3.8 evaluate-0.4.3 filelock-3.18.0 flatbuffers-25.2.10 frozenlist-1.6.0 fsspec-2025.3.0 hf-xet-1.1.1 huggingface-hub-0.31.1 humanfriendly-10.0 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 multidict-6.4.3 multiprocess-0.70.16 networkx-3.4.2 numpy-2.2.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 onnx-1.17.0 onnxruntime-1.22.0 optimum-1.20.0 packaging-25.0 pandas-2.2.3 propcache-0.3.1 protobuf-3.20.2 psutil-7.0.0 pyarrow-20.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 sentencepiece-0.2.0 six-1.17.0 sympy-1.14.0 tokenizers-0.13.3 torch-2.7.0 tqdm-4.67.1 transformers-4.28.0 triton-3.3.0 typing-extensions-4.13.2 tzdata-2025.2 urllib3-2.4.0 xxhash-3.5.0 yarl-1.20.0\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/pip-compile requirements.in --output-file constraints.txt\n",
        "!venv/bin/pip install -r requirements.in --constraint constraints.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiDNu2gOBAvA"
      },
      "source": [
        "### 1.4 Login to HuggingFace with your own token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "d3f80df1bf494ff7aea55808a19b4236",
            "5e1b7a8f562e4969b4040297c0519f76",
            "68d96a4cce3d4bc087979fe852aa7052",
            "d3366bc64ac04c74856acdcaeb34397e",
            "fb7c07954aac451f915d40545fec5fc3",
            "7f9bcd0db3b44df4b86ac4a077e93999",
            "888217df5add4870b603b4ac8c27e9c7",
            "be48ccbc30b64b00b54785774f51900b",
            "4b0a3c2636db4cfea8a25fbc054c41b4",
            "9692c7b167bc481c9345e0a11cdb5b94",
            "571be4f1d834484bb5f7afd970129ac6",
            "0e7ada970f284b318fa00902fa2f4c46",
            "f9c45ee50f21477694bf697f7f6fa5b7",
            "20d2b4b7d65c4eee9ce769b85e71fd72",
            "49e95d0e540c4db99a83d6e76ac12328",
            "84e40885d7e24b98ba9fd7e5cc1f28e8",
            "289fff1886914a748746d18d7c9ee841",
            "5045867672374950bfdee9d8b7edd8b2",
            "c2bce89a060d4ed99bbcd9680170b6ab",
            "791a63731da645cbad65f0148980707d"
          ]
        },
        "id": "717QP7-o2V2s",
        "outputId": "bc156c8c-e5ad-4e5d-9f42-e46e4f52c148"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3f80df1bf494ff7aea55808a19b4236",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBO-8oEaBM4q"
      },
      "source": [
        "## 2. **DOWNLOAD ALL MODELS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4YO_ieuBYZl"
      },
      "source": [
        "### 2.1 Download the Bert model (Encoder Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s19-Zic138qk",
        "outputId": "72f23001-b5e6-482a-b8f4-69320f6c430f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⬇️ Scaricando wakaflocka17/bert-imdb-finetuned → models/bert_base_uncased\n",
            "/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 366/366 [00:00<00:00, 2.45MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 6.27MB/s]\n",
            "special_tokens_map.json: 100% 125/125 [00:00<00:00, 926kB/s]\n",
            "config.json: 100% 727/727 [00:00<00:00, 5.14MB/s]\n",
            "/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "pytorch_model.bin: 100% 438M/438M [00:01<00:00, 240MB/s]\n",
            "✅ Modello 'bert_base_uncased' salvato in models/bert_base_uncased\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python src/download_models.py bert_base_uncased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4O9hjYwBeZ3"
      },
      "source": [
        "### 2.2 Download the Bart model (Encoder-Decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bls6qa1BVIv",
        "outputId": "f19631b3-7fb1-43c7-e092-c6034f9483d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⬇️ Scaricando wakaflocka17/bart-imdb-finetuned → models/bart_base\n",
            "/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 1.31k/1.31k [00:00<00:00, 9.07MB/s]\n",
            "vocab.json: 100% 999k/999k [00:00<00:00, 16.4MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 36.1MB/s]\n",
            "special_tokens_map.json: 100% 957/957 [00:00<00:00, 6.54MB/s]\n",
            "config.json: 100% 1.75k/1.75k [00:00<00:00, 13.7MB/s]\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
            "/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "pytorch_model.bin: 100% 560M/560M [00:02<00:00, 229MB/s]\n",
            "✅ Modello 'bart_base' salvato in models/bart_base\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python src/download_models.py bart_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH_k7yQJBitH"
      },
      "source": [
        "### 2.3 Download the Gpt-Neo-2.7B model (Decoder-Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF9Gb67NBWYq",
        "outputId": "d875bc52-96da-4104-f1f8-913c8dbe3cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⬇️ Scaricando wakaflocka17/gptneo-imdb-finetuned → models/gpt_neo_2_7b\n",
            "/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 727/727 [00:00<00:00, 5.58MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 13.4MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 39.9MB/s]\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 23.3MB/s]\n",
            "special_tokens_map.json: 100% 131/131 [00:00<00:00, 1.12MB/s]\n",
            "config.json: 100% 1.61k/1.61k [00:00<00:00, 11.4MB/s]\n",
            "/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "pytorch_model.bin.index.json: 100% 38.7k/38.7k [00:00<00:00, 139MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/10.1G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 10.5M/10.1G [00:00<02:40, 62.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 31.5M/10.1G [00:00<01:33, 108MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 62.9M/10.1G [00:00<00:58, 170MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 83.9M/10.1G [00:00<00:55, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 115M/10.1G [00:00<00:47, 209MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 147M/10.1G [00:00<00:42, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 178M/10.1G [00:00<00:41, 239MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 210M/10.1G [00:00<00:38, 255MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 241M/10.1G [00:01<00:37, 265MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 273M/10.1G [00:01<00:37, 264MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 304M/10.1G [00:01<00:37, 264MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 336M/10.1G [00:01<00:36, 270MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 367M/10.1G [00:01<00:38, 250MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 398M/10.1G [00:01<00:37, 256MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 430M/10.1G [00:01<00:37, 258MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 461M/10.1G [00:01<00:36, 264MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 493M/10.1G [00:02<00:35, 270MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 524M/10.1G [00:02<00:35, 269MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 556M/10.1G [00:02<00:35, 269MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 587M/10.1G [00:02<00:34, 276MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 619M/10.1G [00:02<00:34, 273MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 650M/10.1G [00:02<00:34, 278MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 682M/10.1G [00:02<00:33, 282MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 713M/10.1G [00:02<00:33, 284MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 744M/10.1G [00:02<00:32, 286MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 776M/10.1G [00:03<00:32, 288MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 807M/10.1G [00:03<00:32, 289MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 839M/10.1G [00:03<00:31, 291MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 870M/10.1G [00:03<00:32, 283MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 902M/10.1G [00:03<00:33, 276MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 933M/10.1G [00:03<00:33, 272MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 965M/10.1G [00:03<00:33, 271MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 996M/10.1G [00:03<00:34, 267MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.03G/10.1G [00:04<00:35, 259MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.06G/10.1G [00:04<00:35, 257MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.09G/10.1G [00:04<00:34, 263MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.12G/10.1G [00:04<00:33, 268MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.15G/10.1G [00:04<00:32, 274MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.18G/10.1G [00:04<00:34, 256MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.22G/10.1G [00:04<00:34, 257MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.25G/10.1G [00:04<00:35, 251MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.28G/10.1G [00:04<00:34, 255MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.31G/10.1G [00:05<00:33, 263MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.34G/10.1G [00:05<00:36, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.37G/10.1G [00:05<00:35, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.41G/10.1G [00:05<00:36, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.44G/10.1G [00:05<00:39, 221MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.47G/10.1G [00:05<00:39, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.50G/10.1G [00:05<00:39, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.53G/10.1G [00:06<00:37, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.56G/10.1G [00:06<00:35, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.59G/10.1G [00:06<00:36, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.63G/10.1G [00:06<00:35, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.66G/10.1G [00:06<00:46, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.68G/10.1G [00:06<00:50, 166MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.71G/10.1G [00:07<00:47, 177MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.73G/10.1G [00:07<00:49, 169MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.75G/10.1G [00:07<00:52, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.77G/10.1G [00:07<00:52, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.79G/10.1G [00:07<00:52, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.81G/10.1G [00:07<00:52, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.84G/10.1G [00:07<00:51, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.86G/10.1G [00:08<00:51, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.88G/10.1G [00:08<00:51, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.90G/10.1G [00:08<00:51, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.92G/10.1G [00:08<00:51, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.94G/10.1G [00:08<00:51, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.96G/10.1G [00:08<00:50, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.98G/10.1G [00:08<00:50, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.00G/10.1G [00:08<00:50, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.02G/10.1G [00:09<00:50, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.04G/10.1G [00:09<00:49, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.07G/10.1G [00:09<00:49, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.09G/10.1G [00:09<00:49, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.11G/10.1G [00:09<00:49, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.13G/10.1G [00:09<00:49, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.15G/10.1G [00:09<00:49, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.17G/10.1G [00:09<00:49, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.19G/10.1G [00:10<00:49, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.21G/10.1G [00:10<00:48, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.23G/10.1G [00:10<00:58, 134MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.25G/10.1G [00:10<01:09, 113MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.28G/10.1G [00:11<01:28, 88.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.29G/10.1G [00:11<01:36, 80.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.30G/10.1G [00:11<01:32, 84.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.31G/10.1G [00:11<01:41, 76.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.32G/10.1G [00:11<01:49, 71.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.34G/10.1G [00:11<01:25, 90.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.36G/10.1G [00:12<01:20, 96.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.37G/10.1G [00:12<01:25, 90.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.39G/10.1G [00:12<01:12, 106MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.41G/10.1G [00:12<01:05, 118MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.43G/10.1G [00:12<01:00, 127MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.45G/10.1G [00:12<00:56, 135MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.47G/10.1G [00:12<00:54, 141MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.50G/10.1G [00:12<00:52, 146MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.52G/10.1G [00:13<00:50, 150MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.54G/10.1G [00:13<00:49, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.56G/10.1G [00:13<00:48, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.58G/10.1G [00:13<01:10, 107MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.60G/10.1G [00:13<01:12, 103MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.62G/10.1G [00:14<01:06, 112MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.64G/10.1G [00:14<01:04, 116MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.66G/10.1G [00:14<00:58, 127MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.68G/10.1G [00:14<01:00, 123MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.71G/10.1G [00:14<01:02, 119MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.73G/10.1G [00:15<01:11, 104MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.75G/10.1G [00:15<01:07, 109MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.77G/10.1G [00:15<01:00, 121MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.79G/10.1G [00:15<00:56, 130MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.81G/10.1G [00:15<00:52, 138MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.83G/10.1G [00:15<00:50, 145MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.85G/10.1G [00:15<00:48, 150MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.87G/10.1G [00:15<00:47, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.89G/10.1G [00:16<00:46, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.92G/10.1G [00:16<00:45, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.94G/10.1G [00:16<00:45, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.96G/10.1G [00:16<00:45, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.98G/10.1G [00:16<00:45, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.00G/10.1G [00:16<00:44, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.02G/10.1G [00:16<00:44, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.04G/10.1G [00:17<00:44, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.06G/10.1G [00:17<00:44, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.08G/10.1G [00:17<00:43, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.10G/10.1G [00:17<00:43, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.12G/10.1G [00:17<00:43, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.15G/10.1G [00:17<00:43, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.17G/10.1G [00:17<00:43, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.19G/10.1G [00:17<00:43, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.21G/10.1G [00:18<00:43, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.23G/10.1G [00:18<00:42, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.25G/10.1G [00:18<00:42, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.27G/10.1G [00:18<00:42, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.29G/10.1G [00:18<00:42, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.31G/10.1G [00:18<00:42, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.33G/10.1G [00:18<00:42, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.36G/10.1G [00:18<00:42, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.38G/10.1G [00:19<00:43, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.40G/10.1G [00:19<00:43, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.42G/10.1G [00:19<00:42, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.44G/10.1G [00:19<00:41, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.46G/10.1G [00:19<00:41, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.48G/10.1G [00:19<00:41, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.50G/10.1G [00:19<00:41, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.52G/10.1G [00:20<00:40, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.54G/10.1G [00:20<00:40, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.57G/10.1G [00:20<00:40, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.59G/10.1G [00:20<00:40, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.61G/10.1G [00:20<00:40, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.63G/10.1G [00:20<00:40, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.65G/10.1G [00:20<00:40, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.67G/10.1G [00:20<00:40, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.69G/10.1G [00:21<00:40, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.71G/10.1G [00:21<00:40, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.73G/10.1G [00:21<00:39, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.75G/10.1G [00:21<00:39, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.77G/10.1G [00:21<00:40, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.80G/10.1G [00:21<00:39, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.82G/10.1G [00:21<00:39, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.84G/10.1G [00:22<00:39, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.86G/10.1G [00:22<00:38, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.88G/10.1G [00:22<00:38, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.90G/10.1G [00:22<00:38, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.92G/10.1G [00:22<00:38, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.94G/10.1G [00:22<00:38, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.96G/10.1G [00:22<00:38, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.98G/10.1G [00:22<00:37, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.01G/10.1G [00:23<00:38, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.03G/10.1G [00:23<00:37, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.05G/10.1G [00:23<00:37, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.07G/10.1G [00:23<00:37, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.09G/10.1G [00:23<00:37, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.11G/10.1G [00:23<00:37, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.13G/10.1G [00:23<00:37, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.15G/10.1G [00:23<00:36, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.17G/10.1G [00:24<00:36, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.19G/10.1G [00:24<00:37, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.22G/10.1G [00:24<00:37, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.24G/10.1G [00:24<00:36, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.26G/10.1G [00:24<00:36, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.28G/10.1G [00:24<00:37, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.30G/10.1G [00:24<00:37, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.32G/10.1G [00:25<00:36, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.34G/10.1G [00:25<00:37, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.36G/10.1G [00:25<00:37, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.38G/10.1G [00:25<00:37, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.40G/10.1G [00:25<00:41, 139MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.42G/10.1G [00:25<00:39, 142MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.45G/10.1G [00:26<00:46, 123MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.47G/10.1G [00:26<00:42, 132MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.49G/10.1G [00:26<00:46, 122MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.51G/10.1G [00:26<00:46, 119MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.53G/10.1G [00:26<00:42, 130MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.55G/10.1G [00:26<00:50, 109MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.57G/10.1G [00:27<00:53, 104MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.59G/10.1G [00:27<00:50, 110MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.61G/10.1G [00:27<00:58, 93.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.62G/10.1G [00:27<01:04, 84.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.63G/10.1G [00:27<01:03, 86.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.66G/10.1G [00:28<00:52, 103MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.68G/10.1G [00:28<00:46, 116MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.70G/10.1G [00:28<00:45, 119MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.72G/10.1G [00:28<00:52, 103MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.74G/10.1G [00:28<00:57, 92.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.75G/10.1G [00:29<01:02, 86.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.76G/10.1G [00:29<01:01, 87.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.78G/10.1G [00:29<00:50, 105MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.80G/10.1G [00:29<00:44, 120MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.82G/10.1G [00:29<00:40, 131MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.84G/10.1G [00:29<00:37, 140MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.87G/10.1G [00:29<00:35, 146MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.89G/10.1G [00:29<00:34, 151MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.91G/10.1G [00:30<00:33, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.93G/10.1G [00:30<00:33, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.95G/10.1G [00:30<00:32, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.97G/10.1G [00:30<00:32, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.99G/10.1G [00:30<00:32, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.01G/10.1G [00:30<00:31, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.03G/10.1G [00:30<00:31, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.05G/10.1G [00:30<00:31, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.08G/10.1G [00:31<00:31, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.10G/10.1G [00:31<00:31, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.12G/10.1G [00:31<00:31, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.14G/10.1G [00:31<00:30, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.16G/10.1G [00:31<00:30, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.18G/10.1G [00:31<00:30, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.20G/10.1G [00:31<00:30, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.22G/10.1G [00:32<00:30, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.24G/10.1G [00:32<00:30, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.26G/10.1G [00:32<00:30, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.28G/10.1G [00:32<00:30, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.31G/10.1G [00:32<00:30, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.33G/10.1G [00:32<00:30, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.35G/10.1G [00:32<00:30, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.37G/10.1G [00:32<00:30, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.39G/10.1G [00:33<00:30, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.41G/10.1G [00:33<00:30, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.43G/10.1G [00:33<00:30, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.45G/10.1G [00:33<00:30, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.47G/10.1G [00:33<00:30, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.49G/10.1G [00:33<00:29, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.52G/10.1G [00:33<00:29, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.54G/10.1G [00:34<00:29, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.56G/10.1G [00:34<00:28, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.58G/10.1G [00:34<00:28, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.60G/10.1G [00:34<00:28, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.62G/10.1G [00:34<00:28, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.64G/10.1G [00:34<00:28, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.66G/10.1G [00:34<00:28, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.68G/10.1G [00:34<00:28, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.70G/10.1G [00:35<00:28, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.73G/10.1G [00:35<00:28, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.75G/10.1G [00:35<00:27, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.77G/10.1G [00:35<00:27, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.79G/10.1G [00:35<00:27, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.81G/10.1G [00:35<00:26, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.83G/10.1G [00:35<00:26, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.85G/10.1G [00:36<00:26, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.87G/10.1G [00:36<00:26, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.89G/10.1G [00:36<00:26, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.91G/10.1G [00:36<00:25, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.93G/10.1G [00:36<00:25, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.96G/10.1G [00:36<00:25, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.98G/10.1G [00:36<00:25, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 6.00G/10.1G [00:36<00:25, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.02G/10.1G [00:37<00:25, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.04G/10.1G [00:37<00:25, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.06G/10.1G [00:37<00:24, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.08G/10.1G [00:37<00:24, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.10G/10.1G [00:37<00:24, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.12G/10.1G [00:37<00:24, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.14G/10.1G [00:37<00:24, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.17G/10.1G [00:37<00:24, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.19G/10.1G [00:38<00:24, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.21G/10.1G [00:38<00:24, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.23G/10.1G [00:38<00:24, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.25G/10.1G [00:38<00:24, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.27G/10.1G [00:38<00:24, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.29G/10.1G [00:38<00:24, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.31G/10.1G [00:38<00:23, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.33G/10.1G [00:39<00:23, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.35G/10.1G [00:39<00:23, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.38G/10.1G [00:39<00:24, 151MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.40G/10.1G [00:39<00:32, 114MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.42G/10.1G [00:39<00:30, 122MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.44G/10.1G [00:39<00:27, 131MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.46G/10.1G [00:40<00:31, 117MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.48G/10.1G [00:40<00:28, 127MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.50G/10.1G [00:40<00:26, 136MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.52G/10.1G [00:40<00:25, 140MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.54G/10.1G [00:40<00:24, 146MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.56G/10.1G [00:40<00:27, 130MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.59G/10.1G [00:41<00:30, 116MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.61G/10.1G [00:41<00:31, 111MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.63G/10.1G [00:41<00:33, 104MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.65G/10.1G [00:41<00:30, 112MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.67G/10.1G [00:41<00:31, 108MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.69G/10.1G [00:41<00:28, 119MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.71G/10.1G [00:42<00:26, 128MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.73G/10.1G [00:42<00:24, 135MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.75G/10.1G [00:42<00:23, 140MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.77G/10.1G [00:42<00:23, 144MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.79G/10.1G [00:42<00:22, 148MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.82G/10.1G [00:42<00:21, 151MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.84G/10.1G [00:42<00:21, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.86G/10.1G [00:43<00:20, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.88G/10.1G [00:43<00:20, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.90G/10.1G [00:43<00:20, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.92G/10.1G [00:43<00:20, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.94G/10.1G [00:43<00:19, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.96G/10.1G [00:43<00:19, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.98G/10.1G [00:43<00:19, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 7.00G/10.1G [00:43<00:19, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.03G/10.1G [00:44<00:19, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.05G/10.1G [00:44<00:19, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.07G/10.1G [00:44<00:19, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.09G/10.1G [00:44<00:19, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.11G/10.1G [00:44<00:18, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.13G/10.1G [00:44<00:18, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.15G/10.1G [00:44<00:18, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.17G/10.1G [00:45<00:18, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.19G/10.1G [00:45<00:18, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.21G/10.1G [00:45<00:18, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.24G/10.1G [00:45<00:18, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.26G/10.1G [00:45<00:18, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.28G/10.1G [00:45<00:18, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.30G/10.1G [00:45<00:18, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.32G/10.1G [00:45<00:18, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.34G/10.1G [00:46<00:18, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.36G/10.1G [00:46<00:18, 151MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.38G/10.1G [00:46<00:18, 151MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.40G/10.1G [00:46<00:17, 151MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.42G/10.1G [00:46<00:17, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.44G/10.1G [00:46<00:17, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.47G/10.1G [00:46<00:17, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.49G/10.1G [00:47<00:17, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.51G/10.1G [00:47<00:17, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.53G/10.1G [00:47<00:16, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.55G/10.1G [00:47<00:16, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.57G/10.1G [00:47<00:16, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.59G/10.1G [00:47<00:15, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.61G/10.1G [00:47<00:15, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.63G/10.1G [00:48<00:15, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.65G/10.1G [00:48<00:23, 106MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.68G/10.1G [00:48<00:20, 119MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.70G/10.1G [00:48<00:18, 129MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.72G/10.1G [00:48<00:17, 137MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.74G/10.1G [00:48<00:16, 144MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.76G/10.1G [00:49<00:15, 149MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.78G/10.1G [00:49<00:15, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.80G/10.1G [00:49<00:14, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.82G/10.1G [00:49<00:14, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.84G/10.1G [00:49<00:14, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.86G/10.1G [00:49<00:14, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.89G/10.1G [00:49<00:13, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.91G/10.1G [00:49<00:13, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.93G/10.1G [00:50<00:13, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.95G/10.1G [00:50<00:13, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.97G/10.1G [00:50<00:13, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.99G/10.1G [00:50<00:13, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 8.01G/10.1G [00:50<00:13, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 8.03G/10.1G [00:50<00:13, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.05G/10.1G [00:50<00:12, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.07G/10.1G [00:50<00:12, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.10G/10.1G [00:51<00:12, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.12G/10.1G [00:51<00:12, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.14G/10.1G [00:51<00:12, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.16G/10.1G [00:51<00:12, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.18G/10.1G [00:51<00:11, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.20G/10.1G [00:51<00:12, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.22G/10.1G [00:51<00:11, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.24G/10.1G [00:52<00:11, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.26G/10.1G [00:52<00:11, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.28G/10.1G [00:52<00:11, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.30G/10.1G [00:52<00:11, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.33G/10.1G [00:52<00:11, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.35G/10.1G [00:52<00:10, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.37G/10.1G [00:52<00:10, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.39G/10.1G [00:52<00:10, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.41G/10.1G [00:53<00:10, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.43G/10.1G [00:53<00:10, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.45G/10.1G [00:53<00:10, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.47G/10.1G [00:53<00:10, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.49G/10.1G [00:53<00:10, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.51G/10.1G [00:53<00:10, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.54G/10.1G [00:53<00:10, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.56G/10.1G [00:54<00:10, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.58G/10.1G [00:54<00:09, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.60G/10.1G [00:54<00:09, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.62G/10.1G [00:56<00:53, 27.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.64G/10.1G [00:56<00:39, 36.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.66G/10.1G [00:56<00:30, 48.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.68G/10.1G [00:56<00:23, 60.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.70G/10.1G [00:57<00:18, 74.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.72G/10.1G [00:57<00:15, 89.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.75G/10.1G [00:57<00:13, 103MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.77G/10.1G [00:57<00:11, 116MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.79G/10.1G [00:57<00:10, 127MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.81G/10.1G [00:57<00:09, 136MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.83G/10.1G [00:57<00:08, 143MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.85G/10.1G [00:57<00:08, 148MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.87G/10.1G [00:58<00:08, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.89G/10.1G [00:58<00:08, 151MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.91G/10.1G [00:58<00:07, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.93G/10.1G [00:58<00:07, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.95G/10.1G [00:58<00:07, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.98G/10.1G [00:58<00:07, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 9.00G/10.1G [00:59<00:10, 107MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 9.02G/10.1G [00:59<00:09, 119MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 9.04G/10.1G [00:59<00:08, 129MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.06G/10.1G [00:59<00:07, 135MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.08G/10.1G [00:59<00:07, 142MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.10G/10.1G [00:59<00:06, 146MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.12G/10.1G [00:59<00:06, 150MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.14G/10.1G [00:59<00:06, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.16G/10.1G [01:00<00:06, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.19G/10.1G [01:00<00:05, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.21G/10.1G [01:00<00:05, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.23G/10.1G [01:00<00:05, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.25G/10.1G [01:00<00:05, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.27G/10.1G [01:00<00:05, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.29G/10.1G [01:00<00:05, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.31G/10.1G [01:01<00:04, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.33G/10.1G [01:01<00:04, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.35G/10.1G [01:01<00:04, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.37G/10.1G [01:01<00:04, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.40G/10.1G [01:01<00:04, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.42G/10.1G [01:01<00:04, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.44G/10.1G [01:01<00:04, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.46G/10.1G [01:01<00:04, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.48G/10.1G [01:02<00:03, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.50G/10.1G [01:02<00:03, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.52G/10.1G [01:02<00:03, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.54G/10.1G [01:02<00:03, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.56G/10.1G [01:02<00:03, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.58G/10.1G [01:02<00:03, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.60G/10.1G [01:02<00:03, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.63G/10.1G [01:02<00:02, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.65G/10.1G [01:03<00:02, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.67G/10.1G [01:03<00:02, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.69G/10.1G [01:03<00:02, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.71G/10.1G [01:03<00:02, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.73G/10.1G [01:03<00:02, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.75G/10.1G [01:03<00:02, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.77G/10.1G [01:03<00:02, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.79G/10.1G [01:04<00:02, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.81G/10.1G [01:04<00:01, 151MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.84G/10.1G [01:04<00:01, 148MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.86G/10.1G [01:04<00:01, 148MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.88G/10.1G [01:04<00:01, 148MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.90G/10.1G [01:04<00:01, 148MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.92G/10.1G [01:04<00:01, 149MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.94G/10.1G [01:05<00:01, 150MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.96G/10.1G [01:05<00:00, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.98G/10.1G [01:05<00:00, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 10.0G/10.1G [01:05<00:00, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 10.0G/10.1G [01:05<00:00, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 10.0G/10.1G [01:05<00:00, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 10.1G/10.1G [01:05<00:00, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 10.1G/10.1G [01:05<00:00, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 10.1G/10.1G [01:06<00:00, 153MB/s]\n",
            "Downloading shards:  50% 1/2 [01:06<01:06, 66.32s/it]\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/634M [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 10.5M/634M [00:00<00:12, 50.9MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 31.5M/634M [00:00<00:06, 97.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 52.4M/634M [00:00<00:04, 122MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 73.4M/634M [00:00<00:04, 136MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 94.4M/634M [00:00<00:03, 145MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 115M/634M [00:00<00:03, 150MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 136M/634M [00:01<00:03, 153MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 157M/634M [00:01<00:03, 156MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 178M/634M [00:01<00:02, 158MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 199M/634M [00:01<00:02, 159MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 220M/634M [00:01<00:02, 161MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 241M/634M [00:01<00:02, 161MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 262M/634M [00:01<00:02, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 283M/634M [00:01<00:02, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 304M/634M [00:02<00:02, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 325M/634M [00:02<00:01, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 346M/634M [00:02<00:01, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 367M/634M [00:02<00:01, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 388M/634M [00:02<00:01, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 409M/634M [00:02<00:01, 160MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 430M/634M [00:02<00:01, 160MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 451M/634M [00:02<00:01, 160MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 472M/634M [00:03<00:01, 161MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 493M/634M [00:03<00:00, 161MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 514M/634M [00:03<00:00, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 535M/634M [00:03<00:00, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 556M/634M [00:03<00:00, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 577M/634M [00:03<00:00, 161MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 598M/634M [00:03<00:00, 160MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 634M/634M [00:04<00:00, 155MB/s]\n",
            "Downloading shards: 100% 2/2 [01:10<00:00, 35.26s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.54s/it]\n",
            "✅ Modello 'gpt_neo_2_7b' salvato in models/gpt_neo_2_7b\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python src/download_models.py gpt_neo_2_7b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quW8yBnABn6b"
      },
      "source": [
        "## 3. **ENSEMBLE EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKw57ncp9EWr",
        "outputId": "7a65fa18-00f0-428f-ac9b-5b59a78b0b06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Accelerator initialized. Device: cuda, Mixed Precision: no\n",
            "INFO:__main__:Usando la configurazione di default: {'model_names': ['bert_base_uncased', 'bart_base', 'gpt_neo_2_7b'], 'repo': 'models/ensemble_majority_voting', 'eval_batch_size': 64}\n",
            "INFO:__main__:Effective evaluation batch size: 64\n",
            "INFO:__main__:I modelli verranno salvati in: models/ensemble_majority_voting\n",
            "INFO:datasets:PyTorch version 2.7.0 available.\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Initializing EnsembleMajorityVoting with models: ['bert_base_uncased', 'bart_base', 'gpt_neo_2_7b'] on device: cuda\n",
            "INFO:src.architectures.model_ensemble_majority_voting:AMP: False, BetterTransformer: False, ONNXRuntime: False\n",
            "INFO:src.utils:[bert_base_uncased] 'repo_finetuned' is specified: models/bert_base_uncased. Strict checks will be applied.\n",
            "INFO:src.utils:Attempting to load bert_base_uncased from: models/bert_base_uncased (source: fine-tuned (local), local_files_only=True)\n",
            "INFO:src.utils:Successfully loaded PyTorch model bert_base_uncased from models/bert_base_uncased (fine-tuned (local))\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Successfully loaded and configured model: bert_base_uncased (ONNX: False)\n",
            "INFO:src.utils:[bart_base] 'repo_finetuned' is specified: models/bart_base. Strict checks will be applied.\n",
            "INFO:src.utils:Attempting to load bart_base from: models/bart_base (source: fine-tuned (local), local_files_only=True)\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
            "INFO:src.utils:Successfully loaded PyTorch model bart_base from models/bart_base (fine-tuned (local))\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Successfully loaded and configured model: bart_base (ONNX: False)\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Configuring gpt_neo_2_7b for 8-bit quantization (override).\n",
            "INFO:src.utils:[gpt_neo_2_7b] 'repo_finetuned' is specified: models/gpt_neo_2_7b. Strict checks will be applied.\n",
            "INFO:src.utils:[gpt_neo_2_7b] 8-bit loading is specifically enabled.\n",
            "INFO:src.utils:Attempting to load gpt_neo_2_7b from: models/gpt_neo_2_7b (source: fine-tuned (local), local_files_only=True)\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "Loading checkpoint shards: 100% 2/2 [00:08<00:00,  4.41s/it]\n",
            "INFO:src.utils:Successfully loaded PyTorch model gpt_neo_2_7b from models/gpt_neo_2_7b (fine-tuned (local))\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Successfully loaded and configured model: gpt_neo_2_7b (ONNX: False)\n",
            "INFO:src.architectures.model_ensemble_majority_voting:EnsembleMajorityVoting initialized successfully.\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Preparing dataset for EnsembleMajorityVoting (loading IMDB).\n",
            "INFO:root:Caricamento dataset IMDb direttamente da Stanford...\n",
            "INFO:root:Downloading dataset from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz...\n",
            "INFO:root:Download completed.\n",
            "INFO:root:Extracting dataset...\n",
            "INFO:root:Extraction completed.\n",
            "INFO:root:Processing training data...\n",
            "INFO:root:Processing test data...\n",
            "INFO:root:Dataset caricato con successo: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "})\n",
            "INFO:root:Split disponibili: dict_keys(['train', 'test'])\n",
            "INFO:root:Numero di esempi - Train: 25000, Test: 25000\n",
            "INFO:root:Creazione split train/val/test...\n",
            "INFO:root:Train size: 20000, Val size: 5000, Test size: 25000\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Dataset prepared for EnsembleMajorityVoting. Test set size: 25000\n",
            "INFO:__main__:Modalità EVAL: avvio dell'evaluation per il modello ensemble_majority_voting.\n",
            "INFO:src.architectures.model_ensemble_majority_voting:CUDA detected. DataLoader: num_workers=4, pin_memory=True\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Starting evaluation of ensemble on 25000 samples with batch size 64.\n",
            "Evaluating Ensemble: 100% 391/391 [19:35<00:00,  3.01s/it]\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Ensemble Evaluation Complete. Accuracy: 0.9330 (23324/25000)\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Ensemble evaluation results saved to experiments/results/evaluation/ensemble-majority-voting-imdb.json\n",
            "INFO:__main__:Risultati evaluation: {'accuracy': 0.93296, 'total_samples': 25000, 'correct_predictions': 23324}\n",
            "INFO:__main__:Attempting to save ensemble model to: models/ensemble_majority_voting\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Saving ensemble to models/ensemble_majority_voting on main process...\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Saved ensemble metadata to models/ensemble_majority_voting/metadata.json\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Saved ensemble config to models/ensemble_majority_voting/config.json\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Saved tokenizer from bert_base_uncased to models/ensemble_majority_voting\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Saved PyTorch model bert_base_uncased to models/ensemble_majority_voting/bert_base_uncased\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Saved PyTorch model bart_base to models/ensemble_majority_voting/bart_base\n",
            "/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:1712: UserWarning: You are calling `save_pretrained` to a 8-bit converted model you may likely encounter unexepected behaviors. If you want to save 8-bit models, make sure to have `bitsandbytes>0.37.2` installed.\n",
            "  warnings.warn(\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Saved PyTorch model gpt_neo_2_7b to models/ensemble_majority_voting/gpt_neo_2_7b\n",
            "INFO:src.architectures.model_ensemble_majority_voting:Ensemble saving process complete for models/ensemble_majority_voting.\n",
            "INFO:__main__:Ensemble model saving process initiated. Check logs for details. Target path: models/ensemble_majority_voting\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python main.py --model_config_key ensemble_majority_voting --mode eval --eval_type fine_tuned --output_json_path \"experiments/results/evaluation/ensemble-majority-voting-imdb.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ambDGXOGCTLa"
      },
      "source": [
        "## 4. **ENSEMBLE UPLOAD ON HUGGINGFACE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpkurQ3zCYyw",
        "outputId": "7756a11a-c74b-43b3-9c6d-0387ef66de4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Preparazione upload per: ensemble_majority_voting da models/ensemble_majority_voting\n",
            "✅ Copiato evaluation: experiments/results/evaluation/ensemble-majority-voting-imdb.json → models/ensemble_majority_voting/evaluation\n",
            "⚠️ Validation mancante: experiments/results/validation/ensemble-majority-voting-imdb_metrics.json\n",
            "INFO:root:Uploading models/ensemble_majority_voting to Hugging Face as wakaflocka17/ensemble-majority-voting-imdb...\n",
            "INFO:root:Upload completato per wakaflocka17/ensemble-majority-voting-imdb\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python src/upload_models.py --only ensemble_majority_voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZCiBR19ORvh"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e7ada970f284b318fa00902fa2f4c46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20d2b4b7d65c4eee9ce769b85e71fd72": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "289fff1886914a748746d18d7c9ee841": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49e95d0e540c4db99a83d6e76ac12328": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4b0a3c2636db4cfea8a25fbc054c41b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5045867672374950bfdee9d8b7edd8b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2bce89a060d4ed99bbcd9680170b6ab",
            "placeholder": "​",
            "style": "IPY_MODEL_791a63731da645cbad65f0148980707d",
            "value": "Connecting..."
          }
        },
        "571be4f1d834484bb5f7afd970129ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e1b7a8f562e4969b4040297c0519f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be48ccbc30b64b00b54785774f51900b",
            "placeholder": "​",
            "style": "IPY_MODEL_4b0a3c2636db4cfea8a25fbc054c41b4",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "68d96a4cce3d4bc087979fe852aa7052": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_9692c7b167bc481c9345e0a11cdb5b94",
            "placeholder": "​",
            "style": "IPY_MODEL_571be4f1d834484bb5f7afd970129ac6",
            "value": ""
          }
        },
        "791a63731da645cbad65f0148980707d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f9bcd0db3b44df4b86ac4a077e93999": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84e40885d7e24b98ba9fd7e5cc1f28e8",
            "placeholder": "​",
            "style": "IPY_MODEL_289fff1886914a748746d18d7c9ee841",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "84e40885d7e24b98ba9fd7e5cc1f28e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888217df5add4870b603b4ac8c27e9c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "9692c7b167bc481c9345e0a11cdb5b94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be48ccbc30b64b00b54785774f51900b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2bce89a060d4ed99bbcd9680170b6ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3366bc64ac04c74856acdcaeb34397e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_0e7ada970f284b318fa00902fa2f4c46",
            "style": "IPY_MODEL_f9c45ee50f21477694bf697f7f6fa5b7",
            "value": true
          }
        },
        "d3f80df1bf494ff7aea55808a19b4236": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_888217df5add4870b603b4ac8c27e9c7"
          }
        },
        "f9c45ee50f21477694bf697f7f6fa5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb7c07954aac451f915d40545fec5fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_20d2b4b7d65c4eee9ce769b85e71fd72",
            "style": "IPY_MODEL_49e95d0e540c4db99a83d6e76ac12328",
            "tooltip": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
