{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E921PGFJz1ws"
      },
      "source": [
        "# **LLMs Analysis on IMDb Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2RUd3Gwz7h1"
      },
      "source": [
        "## 1. **SETUP**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We check the GPU status"
      ],
      "metadata": {
        "id": "8ibYBusBWIhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!watch -n 1 nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I-CSvjaWHTs",
        "outputId": "15a98832-1cc3-48f0-bfd2-49930479ff75"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?1l\u001b>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBbD9TyM0KBN"
      },
      "source": [
        "### Now we are going to verify that there are no other past instances of our project within the Colab directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYqz_PwqmA-R",
        "outputId": "e2c1d839-5f47-4872-c2ea-81846da2167e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz_PWoPE0Nu_"
      },
      "source": [
        "### We check that the directory is not already there, and pull the files needed for training and evaluation from GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzHa7V6puo2L",
        "outputId": "b0e0fb4e-3266-4ab7-b76f-88ce423c778d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DLA_LLMSANALYSIS'...\n",
            "remote: Enumerating objects: 391, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 391 (delta 66), reused 57 (delta 36), pack-reused 301 (from 1)\u001b[K\n",
            "Receiving objects: 100% (391/391), 82.22 KiB | 20.56 MiB/s, done.\n",
            "Resolving deltas: 100% (284/284), done.\n",
            "/content/DLA_LLMSANALYSIS\n"
          ]
        }
      ],
      "source": [
        "!test -d DLA_LLMSANALYSIS && rm -rf DLA_LLMSANALYSIS\n",
        "!git clone https://github.com/wakaflocka17/DLA_LLMSANALYSIS.git\n",
        "%cd DLA_LLMSANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmCndrnf0c5_"
      },
      "source": [
        "### We are now going to create our virtual environment using venv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QK27pExqgsn",
        "outputId": "f26567df-2dcb-4cce-db8c-432a55af6740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.30.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (3.18.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (4.3.7)\n",
            "Downloading virtualenv-20.30.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m141.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/469.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.9 virtualenv-20.30.0\n",
            "created virtual environment CPython3.11.12.final.0-64 in 386ms\n",
            "  creator CPython3Posix(dest=/content/DLA_LLMSANALYSIS/venv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==25.0.1, setuptools==78.1.0, wheel==0.45.1\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ],
      "source": [
        "!pip install virtualenv\n",
        "!python -m virtualenv venv\n",
        "!source venv/bin/activate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giBowUky0ewy"
      },
      "source": [
        "### Now we are going to install all the libraries defined within our requirements.txt file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJW6X7NNqnv8",
        "outputId": "05c6c345-e5b4-4811-bdec-3fcc2459a13c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2.0.0 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scikit-learn==1.2.2 (from -r requirements.txt (line 2))\n",
            "  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting transformers==4.28.0 (from -r requirements.txt (line 3))\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl.metadata (109 kB)\n",
            "Collecting datasets==2.10.0 (from -r requirements.txt (line 4))\n",
            "  Downloading datasets-2.10.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting torch==2.0.0 (from -r requirements.txt (line 5))\n",
            "  Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting tensorflow==2.12.0 (from -r requirements.txt (line 6))\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting fsspec==2023.1.0 (from -r requirements.txt (line 7))\n",
            "  Downloading fsspec-2023.1.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting evaluate==0.4.0 (from -r requirements.txt (line 8))\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting huggingface_hub (from -r requirements.txt (line 9))\n",
            "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting matplotlib (from -r requirements.txt (line 10))\n",
            "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting tqdm (from -r requirements.txt (line 11))\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting scipy>=1.3.2 (from scikit-learn==1.2.2->-r requirements.txt (line 2))\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting joblib>=1.1.1 (from scikit-learn==1.2.2->-r requirements.txt (line 2))\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.2.2->-r requirements.txt (line 2))\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting filelock (from transformers==4.28.0->-r requirements.txt (line 3))\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting packaging>=20.0 (from transformers==4.28.0->-r requirements.txt (line 3))\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pyyaml>=5.1 (from transformers==4.28.0->-r requirements.txt (line 3))\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.28.0->-r requirements.txt (line 3))\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers==4.28.0->-r requirements.txt (line 3))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0->-r requirements.txt (line 3))\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pyarrow>=6.0.0 (from datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting pandas (from datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting xxhash (from datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting aiohttp (from datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading aiohttp-3.11.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting responses<0.19 (from datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting typing-extensions (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=2.0 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting h5py>=2.9.0 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting numpy<2.0.0 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in ./venv/lib/python3.11/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 6)) (78.1.0)\n",
            "Collecting six>=1.12.0 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: wheel in ./venv/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirements.txt (line 5)) (0.45.1)\n",
            "Collecting cmake (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting huggingface_hub (from -r requirements.txt (line 9))\n",
            "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.30.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
            "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.23.1-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.22.1-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.22.0-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.21.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.21.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.21.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.20.0-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading huggingface_hub-0.19.3-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading huggingface_hub-0.19.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.19.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 10))\n",
            "  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 10))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 10))\n",
            "  Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 10))\n",
            "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting pillow>=8 (from matplotlib->-r requirements.txt (line 10))\n",
            "  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 10))\n",
            "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->-r requirements.txt (line 10))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading yarl-1.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting ml_dtypes>=0.4.0 (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.2,>=0.5.1 (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jaxlib-0.5.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers==4.28.0->-r requirements.txt (line 3))\n",
            "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers==4.28.0->-r requirements.txt (line 3))\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.28.0->-r requirements.txt (line 3))\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers==4.28.0->-r requirements.txt (line 3))\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets==2.10.0->-r requirements.txt (line 4))\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 6))\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.10.0-py3-none-any.whl (469 kB)\n",
            "Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
            "Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m160.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m143.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m139.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m137.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m135.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m148.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
            "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
            "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown-3.8-py3-none-any.whl (106 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Downloading yarl-1.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
            "Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m183.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Installing collected packages: tokenizers, pytz, mpmath, lit, libclang, flatbuffers, xxhash, wrapt, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, sympy, six, regex, pyyaml, pyparsing, pyasn1, pyarrow, protobuf, propcache, pillow, packaging, opt-einsum, oauthlib, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, multidict, MarkupSafe, markdown, kiwisolver, keras, joblib, idna, grpcio, gast, fsspec, frozenlist, fonttools, filelock, dill, cycler, cmake, charset-normalizer, certifi, cachetools, attrs, aiohappyeyeballs, absl-py, yarl, werkzeug, scipy, rsa, requests, python-dateutil, pyasn1-modules, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, ml_dtypes, jinja2, h5py, google-pasta, contourpy, astunparse, aiosignal, scikit-learn, responses, requests-oauthlib, pandas, matplotlib, jaxlib, huggingface_hub, google-auth, aiohttp, transformers, jax, google-auth-oauthlib, tensorboard, datasets, tensorflow, evaluate, triton, torch\n",
            "Successfully installed MarkupSafe-3.0.2 absl-py-2.2.2 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 astunparse-1.6.3 attrs-25.3.0 cachetools-5.5.2 certifi-2025.1.31 charset-normalizer-3.4.1 cmake-4.0.0 contourpy-1.3.2 cycler-0.12.1 datasets-2.10.0 dill-0.3.6 evaluate-0.4.0 filelock-3.18.0 flatbuffers-25.2.10 fonttools-4.57.0 frozenlist-1.5.0 fsspec-2023.1.0 gast-0.4.0 google-auth-2.39.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 huggingface_hub-0.17.3 idna-3.10 jax-0.4.30 jaxlib-0.4.30 jinja2-3.1.6 joblib-1.4.2 keras-2.12.0 kiwisolver-1.4.8 libclang-18.1.1 lit-18.1.8 markdown-3.8 matplotlib-3.10.1 ml_dtypes-0.5.1 mpmath-1.3.0 multidict-6.4.3 multiprocess-0.70.14 networkx-3.4.2 numpy-1.23.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 oauthlib-3.2.2 opt-einsum-3.4.0 packaging-24.2 pandas-2.2.3 pillow-11.2.1 propcache-0.3.1 protobuf-4.25.6 pyarrow-19.0.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pyparsing-3.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 requests-oauthlib-2.0.0 responses-0.18.0 rsa-4.9.1 scikit-learn-1.2.2 scipy-1.15.2 six-1.17.0 sympy-1.13.3 tensorboard-2.12.3 tensorboard-data-server-0.7.2 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.0.1 threadpoolctl-3.6.0 tokenizers-0.13.3 torch-2.0.0 tqdm-4.67.1 transformers-4.28.0 triton-2.0.0 typing-extensions-4.13.2 tzdata-2025.2 urllib3-2.4.0 werkzeug-3.1.3 wrapt-1.14.1 xxhash-3.5.0 yarl-1.19.0\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpm4eSmh0iGU"
      },
      "source": [
        "## 2. **HUGGINGFACE LOGIN USING TOKEN ACCOUNT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "486570958be54bffbbd4c78b7af876b8",
            "a866c168021242c38b2179d6479f43f7",
            "6d043ef893114fb9a490759f3e5747df",
            "0bfab857f2d848c68829c93b1d9848bf",
            "583b175b871f4d57a65abb6d768efdcb",
            "a8d34e608a7f4fce9a48a3ebbbd54bcc",
            "59220456973f4921ad674326033ccacd",
            "2f4cd9d5920946448d22e0a0b4b2cb68",
            "adaf34cf19f44622a0ad2f830b83f481",
            "72301ed6b97f42ed8db78b431f77757b",
            "a27ab2715a1349fa9e8e8cc197425f22",
            "3a91c1c825b84cbc800cac7f3dc68ce1",
            "6f711c1b111842c2a23caea512df0738",
            "2758d76e240b45f2a21e45186ebba5f2",
            "68cd9951e169457489b6b25bcf99de49",
            "7be167703cd14338ab90898b7363893e",
            "11db7eb028c0421abbf24d03fd6b064c",
            "e1f6cfe894fe4002948b616b05ffec3a",
            "7fe779a4eea341c88d8d03123b683f9c",
            "43f884b1d57b4905a1ab9507761391ba"
          ]
        },
        "id": "wx4mbuhRiY5n",
        "outputId": "c055b301-fd41-47ac-cbc0-6e1a1786633f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "486570958be54bffbbd4c78b7af876b8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbhShR0F0nFz"
      },
      "source": [
        "## 3. **TRAINING AND EVALUATION MODELS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edhMPNTr03UX"
      },
      "source": [
        "### 3.1 **Train & Validation encoder-only**: google-bert/bert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!venv/bin/python main.py --model_config_key bert_base_uncased --mode train"
      ],
      "metadata": {
        "id": "3P3tkFTt9xo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2941e63d-f7bc-46f5-aa74-9c480c248fb5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Usando la configurazione di default: {'model_name': 'google-bert/bert-base-uncased', 'epochs': 5, 'train_batch_size': 8, 'eval_batch_size': 4, 'learning_rate': 2e-05, 'repo_pretrained': 'models/pretrained/bert-base-uncased', 'repo_finetuned': 'models/finetuned/bert-base-uncased-imdb'}\n",
            "INFO:__main__:I modelli verranno salvati in: models/bert_base_uncased\n",
            "2025-04-16 16:07:03.778649: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-16 16:07:03.835406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-16 16:07:04.711295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading vocab.txt: 100% 232k/232k [00:00<00:00, 39.6MB/s]\n",
            "Downloading tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 293kB/s]\n",
            "Downloading config.json: 100% 570/570 [00:00<00:00, 3.30MB/s]\n",
            "Downloading pytorch_model.bin: 100% 440M/440M [00:01<00:00, 327MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:root:Caricamento dataset IMDb direttamente da Stanford...\n",
            "INFO:root:Downloading dataset from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz...\n",
            "INFO:root:Download completed.\n",
            "INFO:root:Extracting dataset...\n",
            "INFO:root:Extraction completed.\n",
            "INFO:root:Processing training data...\n",
            "INFO:root:Processing test data...\n",
            "INFO:root:Dataset caricato con successo: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "})\n",
            "INFO:root:Split disponibili: dict_keys(['train', 'test'])\n",
            "INFO:root:Numero di esempi - Train: 25000, Test: 25000\n",
            "INFO:root:Creazione split train/val/test...\n",
            "INFO:root:Train size: 20000, Val size: 5000, Test size: 25000\n",
            "INFO:__main__:Modalità TRAIN: avvio del training per il modello bert_base_uncased.\n",
            "/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Training:   0% 0/12500 [00:00<?, ?it/s[0.0]]INFO:src.utils:Inizio training: 12500 step totali.\n",
            "Training:   1% 100/12500 [00:06<14:14, 14.52it/s[0.0]]{'loss': 0.7037, 'learning_rate': 4.96e-05, 'epoch': 0.04}\n",
            "Training:   2% 200/12500 [00:13<13:20, 15.36it/s[0.7037]]{'loss': 0.6403, 'learning_rate': 4.92e-05, 'epoch': 0.08}\n",
            "Training:   2% 300/12500 [00:19<12:58, 15.67it/s[0.6403]]{'loss': 0.5428, 'learning_rate': 4.88e-05, 'epoch': 0.12}\n",
            "Training:   3% 400/12500 [00:25<12:46, 15.78it/s[0.5428]]{'loss': 0.528, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.16}\n",
            "Training:   4% 500/12500 [00:31<12:35, 15.88it/s[0.528]] {'loss': 0.4614, 'learning_rate': 4.8e-05, 'epoch': 0.2}\n",
            "Training:   5% 600/12500 [00:38<12:27, 15.92it/s[0.4614]]{'loss': 0.4324, 'learning_rate': 4.76e-05, 'epoch': 0.24}\n",
            "Training:   6% 700/12500 [00:44<12:18, 15.98it/s[0.4324]]{'loss': 0.4732, 'learning_rate': 4.72e-05, 'epoch': 0.28}\n",
            "Training:   6% 800/12500 [00:50<12:13, 15.95it/s[0.4732]]{'loss': 0.4722, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.32}\n",
            "Training:   7% 900/12500 [00:56<12:05, 15.99it/s[0.4722]]{'loss': 0.4329, 'learning_rate': 4.64e-05, 'epoch': 0.36}\n",
            "Training:   8% 1000/12500 [01:03<11:58, 16.01it/s[0.4329]]{'loss': 0.3969, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.4}\n",
            "Training:   9% 1100/12500 [01:09<11:53, 15.99it/s[0.3969]]{'loss': 0.4219, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.44}\n",
            "Training:  10% 1200/12500 [01:15<11:46, 16.00it/s[0.4219]]{'loss': 0.4058, 'learning_rate': 4.52e-05, 'epoch': 0.48}\n",
            "Training:  10% 1300/12500 [01:21<11:39, 16.00it/s[0.4058]]{'loss': 0.4187, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.52}\n",
            "Training:  11% 1400/12500 [01:28<11:32, 16.02it/s[0.4187]]{'loss': 0.4051, 'learning_rate': 4.44e-05, 'epoch': 0.56}\n",
            "Training:  12% 1500/12500 [01:34<11:27, 15.99it/s[0.4051]]{'loss': 0.4245, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.6}\n",
            "Training:  13% 1600/12500 [01:40<11:22, 15.97it/s[0.4245]]{'loss': 0.3941, 'learning_rate': 4.36e-05, 'epoch': 0.64}\n",
            "Training:  14% 1700/12500 [01:46<11:17, 15.95it/s[0.3941]]{'loss': 0.4201, 'learning_rate': 4.32e-05, 'epoch': 0.68}\n",
            "Training:  14% 1800/12500 [01:53<11:09, 15.99it/s[0.4201]]{'loss': 0.4049, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.72}\n",
            "Training:  15% 1900/12500 [01:59<11:03, 15.98it/s[0.4049]]{'loss': 0.3882, 'learning_rate': 4.24e-05, 'epoch': 0.76}\n",
            "Training:  16% 2000/12500 [02:05<10:57, 15.96it/s[0.3882]]{'loss': 0.3949, 'learning_rate': 4.2e-05, 'epoch': 0.8}\n",
            "Training:  17% 2100/12500 [02:11<10:51, 15.96it/s[0.3949]]{'loss': 0.3917, 'learning_rate': 4.16e-05, 'epoch': 0.84}\n",
            "Training:  18% 2200/12500 [02:18<10:45, 15.96it/s[0.3917]]{'loss': 0.3748, 'learning_rate': 4.12e-05, 'epoch': 0.88}\n",
            "Training:  18% 2300/12500 [02:24<10:38, 15.98it/s[0.3748]]{'loss': 0.3559, 'learning_rate': 4.08e-05, 'epoch': 0.92}\n",
            "Training:  19% 2400/12500 [02:30<10:31, 16.00it/s[0.3559]]{'loss': 0.3725, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.96}\n",
            "Training:  20% 2500/12500 [02:36<10:24, 16.02it/s[0.3725]]{'loss': 0.3733, 'learning_rate': 4e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.32812437415122986, 'eval_accuracy': 0.87, 'eval_runtime': 10.8517, 'eval_samples_per_second': 460.757, 'eval_steps_per_second': 57.595, 'epoch': 1.0}\n",
            "Training:  21% 2600/12500 [02:55<16:39,  9.90it/s[0.3725]]{'loss': 0.2679, 'learning_rate': 3.960000000000001e-05, 'epoch': 1.04}\n",
            "Training:  22% 2700/12500 [03:02<14:35, 11.19it/s[0.2679]]{'loss': 0.2965, 'learning_rate': 3.9200000000000004e-05, 'epoch': 1.08}\n",
            "Training:  22% 2800/12500 [03:08<13:08, 12.29it/s[0.2965]]{'loss': 0.3136, 'learning_rate': 3.88e-05, 'epoch': 1.12}\n",
            "Training:  23% 2900/12500 [03:14<12:06, 13.21it/s[0.3136]]{'loss': 0.2939, 'learning_rate': 3.8400000000000005e-05, 'epoch': 1.16}\n",
            "Training:  24% 3000/12500 [03:20<11:21, 13.94it/s[0.2939]]{'loss': 0.2492, 'learning_rate': 3.8e-05, 'epoch': 1.2}\n",
            "Training:  25% 3100/12500 [03:27<10:48, 14.50it/s[0.2492]]{'loss': 0.2885, 'learning_rate': 3.76e-05, 'epoch': 1.24}\n",
            "Training:  26% 3200/12500 [03:33<10:22, 14.94it/s[0.2885]]{'loss': 0.3531, 'learning_rate': 3.72e-05, 'epoch': 1.28}\n",
            "Training:  26% 3300/12500 [03:39<10:03, 15.25it/s[0.3531]]{'loss': 0.2984, 'learning_rate': 3.68e-05, 'epoch': 1.32}\n",
            "Training:  27% 3400/12500 [03:45<09:48, 15.46it/s[0.2984]]{'loss': 0.2486, 'learning_rate': 3.6400000000000004e-05, 'epoch': 1.36}\n",
            "Training:  28% 3500/12500 [03:52<09:38, 15.55it/s[0.2486]]{'loss': 0.2928, 'learning_rate': 3.6e-05, 'epoch': 1.4}\n",
            "Training:  29% 3600/12500 [03:58<09:27, 15.69it/s[0.2928]]{'loss': 0.3174, 'learning_rate': 3.56e-05, 'epoch': 1.44}\n",
            "Training:  30% 3700/12500 [04:04<09:19, 15.72it/s[0.3174]]{'loss': 0.3064, 'learning_rate': 3.52e-05, 'epoch': 1.48}\n",
            "Training:  30% 3800/12500 [04:11<09:11, 15.77it/s[0.3064]]{'loss': 0.3058, 'learning_rate': 3.48e-05, 'epoch': 1.52}\n",
            "Training:  31% 3900/12500 [04:17<09:03, 15.83it/s[0.3058]]{'loss': 0.3027, 'learning_rate': 3.4399999999999996e-05, 'epoch': 1.56}\n",
            "Training:  32% 4000/12500 [04:23<08:56, 15.83it/s[0.3027]]{'loss': 0.3043, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.6}\n",
            "Training:  33% 4100/12500 [04:30<08:49, 15.87it/s[0.3043]]{'loss': 0.2768, 'learning_rate': 3.3600000000000004e-05, 'epoch': 1.64}\n",
            "Training:  34% 4200/12500 [04:36<08:43, 15.84it/s[0.2768]]{'loss': 0.3332, 'learning_rate': 3.32e-05, 'epoch': 1.68}\n",
            "Training:  34% 4300/12500 [04:42<08:37, 15.86it/s[0.3332]]{'loss': 0.2833, 'learning_rate': 3.2800000000000004e-05, 'epoch': 1.72}\n",
            "Training:  35% 4400/12500 [04:48<08:29, 15.90it/s[0.2833]]{'loss': 0.2934, 'learning_rate': 3.24e-05, 'epoch': 1.76}\n",
            "Training:  36% 4500/12500 [04:55<08:21, 15.97it/s[0.2934]]{'loss': 0.3451, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.8}\n",
            "Training:  37% 4600/12500 [05:01<08:14, 15.97it/s[0.3451]]{'loss': 0.2495, 'learning_rate': 3.16e-05, 'epoch': 1.84}\n",
            "Training:  38% 4700/12500 [05:07<08:07, 16.01it/s[0.2495]]{'loss': 0.2737, 'learning_rate': 3.12e-05, 'epoch': 1.88}\n",
            "Training:  38% 4800/12500 [05:13<08:00, 16.03it/s[0.2737]]{'loss': 0.3108, 'learning_rate': 3.08e-05, 'epoch': 1.92}\n",
            "Training:  39% 4900/12500 [05:20<07:54, 16.03it/s[0.3108]]{'loss': 0.3293, 'learning_rate': 3.04e-05, 'epoch': 1.96}\n",
            "Training:  40% 5000/12500 [05:26<07:48, 16.02it/s[0.3293]]{'loss': 0.2759, 'learning_rate': 3e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4422077238559723, 'eval_accuracy': 0.8746, 'eval_runtime': 10.8361, 'eval_samples_per_second': 461.423, 'eval_steps_per_second': 57.678, 'epoch': 2.0}\n",
            "Training:  41% 5100/12500 [05:45<12:26,  9.91it/s[0.3293]]{'loss': 0.1304, 'learning_rate': 2.96e-05, 'epoch': 2.04}\n",
            "Training:  42% 5200/12500 [05:51<10:51, 11.21it/s[0.1304]]{'loss': 0.1293, 'learning_rate': 2.9199999999999998e-05, 'epoch': 2.08}\n",
            "Training:  42% 5300/12500 [05:57<09:44, 12.31it/s[0.1293]]{'loss': 0.135, 'learning_rate': 2.88e-05, 'epoch': 2.12}\n",
            "Training:  43% 5400/12500 [06:03<08:56, 13.24it/s[0.135]] {'loss': 0.1415, 'learning_rate': 2.84e-05, 'epoch': 2.16}\n",
            "Training:  44% 5500/12500 [06:10<08:20, 13.97it/s[0.1415]]{'loss': 0.1637, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.2}\n",
            "Training:  45% 5600/12500 [06:16<07:56, 14.48it/s[0.1637]]{'loss': 0.1651, 'learning_rate': 2.7600000000000003e-05, 'epoch': 2.24}\n",
            "Training:  46% 5700/12500 [06:22<07:36, 14.91it/s[0.1651]]{'loss': 0.1564, 'learning_rate': 2.7200000000000004e-05, 'epoch': 2.28}\n",
            "Training:  46% 5800/12500 [06:29<07:19, 15.25it/s[0.1564]]{'loss': 0.1822, 'learning_rate': 2.6800000000000004e-05, 'epoch': 2.32}\n",
            "Training:  47% 5900/12500 [06:35<07:05, 15.50it/s[0.1822]]{'loss': 0.1447, 'learning_rate': 2.64e-05, 'epoch': 2.36}\n",
            "Training:  48% 6000/12500 [06:41<06:56, 15.61it/s[0.1447]]{'loss': 0.1972, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.4}\n",
            "Training:  49% 6100/12500 [06:47<06:47, 15.69it/s[0.1972]]{'loss': 0.1479, 'learning_rate': 2.5600000000000002e-05, 'epoch': 2.44}\n",
            "Training:  50% 6200/12500 [06:54<06:38, 15.79it/s[0.1479]]{'loss': 0.1285, 'learning_rate': 2.5200000000000003e-05, 'epoch': 2.48}\n",
            "Training:  50% 6300/12500 [07:00<06:30, 15.88it/s[0.1285]]{'loss': 0.2019, 'learning_rate': 2.48e-05, 'epoch': 2.52}\n",
            "Training:  51% 6400/12500 [07:06<06:23, 15.91it/s[0.2019]]{'loss': 0.1654, 'learning_rate': 2.44e-05, 'epoch': 2.56}\n",
            "Training:  52% 6500/12500 [07:12<06:15, 15.96it/s[0.1654]]{'loss': 0.1906, 'learning_rate': 2.4e-05, 'epoch': 2.6}\n",
            "Training:  53% 6600/12500 [07:18<06:09, 15.98it/s[0.1906]]{'loss': 0.1421, 'learning_rate': 2.36e-05, 'epoch': 2.64}\n",
            "Training:  54% 6700/12500 [07:25<06:02, 16.00it/s[0.1421]]{'loss': 0.1533, 'learning_rate': 2.32e-05, 'epoch': 2.68}\n",
            "Training:  54% 6800/12500 [07:31<05:55, 16.01it/s[0.1533]]{'loss': 0.1359, 'learning_rate': 2.2800000000000002e-05, 'epoch': 2.72}\n",
            "Training:  55% 6900/12500 [07:37<05:50, 15.99it/s[0.1359]]{'loss': 0.1618, 'learning_rate': 2.2400000000000002e-05, 'epoch': 2.76}\n",
            "Training:  56% 7000/12500 [07:43<05:43, 16.01it/s[0.1618]]{'loss': 0.16, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.8}\n",
            "Training:  57% 7100/12500 [07:50<05:36, 16.03it/s[0.16]]  {'loss': 0.1524, 'learning_rate': 2.16e-05, 'epoch': 2.84}\n",
            "Training:  58% 7200/12500 [07:56<05:30, 16.05it/s[0.1524]]{'loss': 0.2025, 'learning_rate': 2.12e-05, 'epoch': 2.88}\n",
            "Training:  58% 7300/12500 [08:02<05:24, 16.00it/s[0.2025]]{'loss': 0.1519, 'learning_rate': 2.08e-05, 'epoch': 2.92}\n",
            "Training:  59% 7400/12500 [08:08<05:19, 15.95it/s[0.1519]]{'loss': 0.2049, 'learning_rate': 2.04e-05, 'epoch': 2.96}\n",
            "Training:  60% 7500/12500 [08:15<05:13, 15.93it/s[0.2049]]{'loss': 0.1221, 'learning_rate': 2e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.6053758859634399, 'eval_accuracy': 0.8778, 'eval_runtime': 10.8028, 'eval_samples_per_second': 462.845, 'eval_steps_per_second': 57.856, 'epoch': 3.0}\n",
            "Training:  61% 7600/12500 [08:34<08:14,  9.90it/s[0.2049]]{'loss': 0.0626, 'learning_rate': 1.9600000000000002e-05, 'epoch': 3.04}\n",
            "Training:  62% 7700/12500 [08:40<07:09, 11.19it/s[0.0626]]{'loss': 0.0787, 'learning_rate': 1.9200000000000003e-05, 'epoch': 3.08}\n",
            "Training:  62% 7800/12500 [08:46<06:24, 12.22it/s[0.0787]]{'loss': 0.0233, 'learning_rate': 1.88e-05, 'epoch': 3.12}\n",
            "Training:  63% 7900/12500 [08:53<05:49, 13.15it/s[0.0233]]{'loss': 0.0566, 'learning_rate': 1.84e-05, 'epoch': 3.16}\n",
            "Training:  64% 8000/12500 [08:59<05:26, 13.78it/s[0.0566]]{'loss': 0.0615, 'learning_rate': 1.8e-05, 'epoch': 3.2}\n",
            "Training:  65% 8100/12500 [09:05<05:05, 14.39it/s[0.0615]]{'loss': 0.0704, 'learning_rate': 1.76e-05, 'epoch': 3.24}\n",
            "Training:  66% 8200/12500 [09:12<04:49, 14.84it/s[0.0704]]{'loss': 0.0995, 'learning_rate': 1.7199999999999998e-05, 'epoch': 3.28}\n",
            "Training:  66% 8300/12500 [09:18<04:37, 15.14it/s[0.0995]]{'loss': 0.0538, 'learning_rate': 1.6800000000000002e-05, 'epoch': 3.32}\n",
            "Training:  67% 8400/12500 [09:24<04:26, 15.40it/s[0.0538]]{'loss': 0.071, 'learning_rate': 1.6400000000000002e-05, 'epoch': 3.36}\n",
            "Training:  68% 8500/12500 [09:30<04:17, 15.56it/s[0.071]] {'loss': 0.1314, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.4}\n",
            "Training:  69% 8600/12500 [09:37<04:08, 15.70it/s[0.1314]]{'loss': 0.0807, 'learning_rate': 1.56e-05, 'epoch': 3.44}\n",
            "Training:  70% 8700/12500 [09:43<04:00, 15.80it/s[0.0807]]{'loss': 0.0832, 'learning_rate': 1.52e-05, 'epoch': 3.48}\n",
            "Training:  70% 8800/12500 [09:49<03:53, 15.84it/s[0.0832]]{'loss': 0.1066, 'learning_rate': 1.48e-05, 'epoch': 3.52}\n",
            "Training:  71% 8900/12500 [09:55<03:47, 15.84it/s[0.1066]]{'loss': 0.0886, 'learning_rate': 1.44e-05, 'epoch': 3.56}\n",
            "Training:  72% 9000/12500 [10:02<03:40, 15.91it/s[0.0886]]{'loss': 0.0424, 'learning_rate': 1.4000000000000001e-05, 'epoch': 3.6}\n",
            "Training:  73% 9100/12500 [10:08<03:33, 15.92it/s[0.0424]]{'loss': 0.0312, 'learning_rate': 1.3600000000000002e-05, 'epoch': 3.64}\n",
            "Training:  74% 9200/12500 [10:14<03:26, 15.95it/s[0.0312]]{'loss': 0.0577, 'learning_rate': 1.32e-05, 'epoch': 3.68}\n",
            "Training:  74% 9300/12500 [10:20<03:20, 15.96it/s[0.0577]]{'loss': 0.076, 'learning_rate': 1.2800000000000001e-05, 'epoch': 3.72}\n",
            "Training:  75% 9400/12500 [10:27<03:14, 15.96it/s[0.076]] {'loss': 0.0558, 'learning_rate': 1.24e-05, 'epoch': 3.76}\n",
            "Training:  76% 9500/12500 [10:33<03:07, 15.97it/s[0.0558]]{'loss': 0.0351, 'learning_rate': 1.2e-05, 'epoch': 3.8}\n",
            "Training:  77% 9600/12500 [10:39<03:01, 15.95it/s[0.0351]]{'loss': 0.0822, 'learning_rate': 1.16e-05, 'epoch': 3.84}\n",
            "Training:  78% 9700/12500 [10:46<02:55, 15.99it/s[0.0822]]{'loss': 0.0458, 'learning_rate': 1.1200000000000001e-05, 'epoch': 3.88}\n",
            "Training:  78% 9800/12500 [10:52<02:48, 16.02it/s[0.0458]]{'loss': 0.0734, 'learning_rate': 1.08e-05, 'epoch': 3.92}\n",
            "Training:  79% 9900/12500 [10:58<02:42, 16.01it/s[0.0734]]{'loss': 0.033, 'learning_rate': 1.04e-05, 'epoch': 3.96}\n",
            "Training:  80% 10000/12500 [11:04<02:36, 16.01it/s[0.033]]{'loss': 0.0573, 'learning_rate': 1e-05, 'epoch': 4.0}\n",
            "{'eval_loss': 0.7817962765693665, 'eval_accuracy': 0.8812, 'eval_runtime': 10.7946, 'eval_samples_per_second': 463.194, 'eval_steps_per_second': 57.899, 'epoch': 4.0}\n",
            "Training:  81% 10100/12500 [11:23<04:01,  9.93it/s[0.033]]{'loss': 0.0438, 'learning_rate': 9.600000000000001e-06, 'epoch': 4.04}\n",
            "Training:  82% 10200/12500 [11:30<03:25, 11.17it/s[0.0438]]{'loss': 0.0236, 'learning_rate': 9.2e-06, 'epoch': 4.08}\n",
            "Training:  82% 10300/12500 [11:36<02:59, 12.27it/s[0.0236]]{'loss': 0.0131, 'learning_rate': 8.8e-06, 'epoch': 4.12}\n",
            "Training:  83% 10400/12500 [11:42<02:39, 13.20it/s[0.0131]]{'loss': 0.0345, 'learning_rate': 8.400000000000001e-06, 'epoch': 4.16}\n",
            "Training:  84% 10500/12500 [11:48<02:24, 13.88it/s[0.0345]]{'loss': 0.0006, 'learning_rate': 8.000000000000001e-06, 'epoch': 4.2}\n",
            "Training:  85% 10600/12500 [11:55<02:11, 14.46it/s[0.0006]]{'loss': 0.0349, 'learning_rate': 7.6e-06, 'epoch': 4.24}\n",
            "Training:  86% 10700/12500 [12:01<02:01, 14.84it/s[0.0349]]{'loss': 0.0096, 'learning_rate': 7.2e-06, 'epoch': 4.28}\n",
            "Training:  86% 10800/12500 [12:07<01:51, 15.19it/s[0.0096]]{'loss': 0.0441, 'learning_rate': 6.800000000000001e-06, 'epoch': 4.32}\n",
            "Training:  87% 10900/12500 [12:13<01:43, 15.44it/s[0.0441]]{'loss': 0.0254, 'learning_rate': 6.4000000000000006e-06, 'epoch': 4.36}\n",
            "Training:  88% 11000/12500 [12:20<01:36, 15.60it/s[0.0254]]{'loss': 0.0193, 'learning_rate': 6e-06, 'epoch': 4.4}\n",
            "Training:  89% 11100/12500 [12:26<01:29, 15.65it/s[0.0193]]{'loss': 0.0073, 'learning_rate': 5.600000000000001e-06, 'epoch': 4.44}\n",
            "Training:  90% 11200/12500 [12:32<01:22, 15.67it/s[0.0073]]{'loss': 0.0458, 'learning_rate': 5.2e-06, 'epoch': 4.48}\n",
            "Training:  90% 11300/12500 [12:39<01:16, 15.71it/s[0.0458]]{'loss': 0.0419, 'learning_rate': 4.800000000000001e-06, 'epoch': 4.52}\n",
            "Training:  91% 11400/12500 [12:45<01:09, 15.80it/s[0.0419]]{'loss': 0.0309, 'learning_rate': 4.4e-06, 'epoch': 4.56}\n",
            "Training:  92% 11500/12500 [12:51<01:03, 15.86it/s[0.0309]]{'loss': 0.0299, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.6}\n",
            "Training:  93% 11600/12500 [12:57<00:56, 15.93it/s[0.0299]]{'loss': 0.0232, 'learning_rate': 3.6e-06, 'epoch': 4.64}\n",
            "Training:  94% 11700/12500 [13:04<00:50, 15.99it/s[0.0232]]{'loss': 0.0367, 'learning_rate': 3.2000000000000003e-06, 'epoch': 4.68}\n",
            "Training:  94% 11800/12500 [13:10<00:43, 15.98it/s[0.0367]]{'loss': 0.0325, 'learning_rate': 2.8000000000000003e-06, 'epoch': 4.72}\n",
            "Training:  95% 11900/12500 [13:16<00:37, 16.02it/s[0.0325]]{'loss': 0.0193, 'learning_rate': 2.4000000000000003e-06, 'epoch': 4.76}\n",
            "Training:  96% 12000/12500 [13:22<00:31, 16.04it/s[0.0193]]{'loss': 0.025, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.8}\n",
            "Training:  97% 12100/12500 [13:29<00:24, 16.02it/s[0.025]] {'loss': 0.0664, 'learning_rate': 1.6000000000000001e-06, 'epoch': 4.84}\n",
            "Training:  98% 12200/12500 [13:35<00:18, 16.02it/s[0.0664]]{'loss': 0.0249, 'learning_rate': 1.2000000000000002e-06, 'epoch': 4.88}\n",
            "Training:  98% 12300/12500 [13:41<00:12, 16.03it/s[0.0249]]{'loss': 0.047, 'learning_rate': 8.000000000000001e-07, 'epoch': 4.92}\n",
            "Training:  99% 12400/12500 [13:47<00:06, 16.02it/s[0.047]] {'loss': 0.0116, 'learning_rate': 4.0000000000000003e-07, 'epoch': 4.96}\n",
            "Training: 100% 12500/12500 [13:54<00:00, 15.99it/s[0.0116]]{'loss': 0.0263, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "{'eval_loss': 0.7965031266212463, 'eval_accuracy': 0.8818, 'eval_runtime': 10.8332, 'eval_samples_per_second': 461.543, 'eval_steps_per_second': 57.693, 'epoch': 5.0}\n",
            "{'train_runtime': 847.216, 'train_samples_per_second': 118.034, 'train_steps_per_second': 14.754, 'train_loss': 0.19825982205212117, 'epoch': 5.0}\n",
            "Training: 100% 12500/12500 [14:07<00:00, 14.75it/s[0.0116]]\n",
            "INFO:src.utils:Training completato.\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:Training completato con metriche finali: {'train_runtime': 847.216, 'train_samples_per_second': 118.034, 'train_steps_per_second': 14.754, 'total_flos': 6577776384000000.0, 'train_loss': 0.19825982205212117, 'epoch': 5.0, 'step': 12500}\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:Training metrics salvate in results/validation/finetuned/bert-base-uncased-imdb_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtWwSsYF1WKC"
      },
      "source": [
        "### 3.1 **Evaluation encoder-only pre-trained**: google-bert/bert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRwSlr3VaSS3",
        "outputId": "0b180723-f251-49b2-ae9e-e93a147d2ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Usando la configurazione di default: {'model_name': 'google-bert/bert-base-uncased', 'epochs': 5, 'train_batch_size': 8, 'eval_batch_size': 4, 'learning_rate': 2e-05, 'repo_pretrained': 'models/pretrained/bert-base-uncased', 'repo_finetuned': 'models/finetuned/bert-base-uncased-imdb'}\n",
            "INFO:__main__:I modelli verranno salvati in: models/bert_base_uncased\n",
            "2025-04-16 16:24:57.783420: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-16 16:24:57.842510: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-16 16:24:58.721673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:root:Caricamento dataset IMDb direttamente da Stanford...\n",
            "INFO:root:Dataset archive already exists, skipping download.\n",
            "INFO:root:Dataset already extracted, skipping extraction.\n",
            "INFO:root:Processing training data...\n",
            "INFO:root:Processing test data...\n",
            "INFO:root:Dataset caricato con successo: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "})\n",
            "INFO:root:Split disponibili: dict_keys(['train', 'test'])\n",
            "INFO:root:Numero di esempi - Train: 25000, Test: 25000\n",
            "INFO:root:Creazione split train/val/test...\n",
            "INFO:root:Train size: 20000, Val size: 5000, Test size: 25000\n",
            "INFO:__main__:Modalità EVAL: avvio dell'evaluation per il modello bert_base_uncased.\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:Valutazione sul modello pre-addestrato (test set)...\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:Carico il modello pre-addestrato da bert-base-uncased\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:Inizio valutazione completa (pre-addestrato)...\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:Esecuzione valutazione finale completa sul test set...\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:\n",
            "Metriche finali complete (test set):\n",
            "Accuracy: 0.4554\n",
            "Precision: 0.4745\n",
            "Recall: 0.8298\n",
            "F1 Score: 0.6038\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:\n",
            "Classification Report Test:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo     0.3226    0.0810    0.1295     12500\n",
            "    positivo     0.4745    0.8298    0.6038     12500\n",
            "\n",
            "    accuracy                         0.4554     25000\n",
            "   macro avg     0.3986    0.4554    0.3667     25000\n",
            "weighted avg     0.3986    0.4554    0.3667     25000\n",
            "\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:Saved pretrained evaluation results to results/evaluation/pretrained/bert-base-uncased-imdb.json\n",
            "INFO:__main__:Risultati evaluation: {'accuracy': 0.45544, 'precision': 0.47451967063129, 'recall': 0.82984, 'f1': 0.6037834691501746}\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python main.py --model_config_key bert_base_uncased --mode eval --eval_type pretrained --output_json_path \"results/evaluation/pretrained/bert-base-uncased-imdb.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6tNutpu1dr3"
      },
      "source": [
        "### 3.1 **Evaluation encoder-only fine-tuned**: google-bert/bert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9bRmHPY_9kW",
        "outputId": "ac226508-c49b-4bac-deeb-97f852e43b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Usando la configurazione di default: {'model_name': 'google-bert/bert-base-uncased', 'epochs': 5, 'train_batch_size': 8, 'eval_batch_size': 4, 'learning_rate': 2e-05, 'repo_pretrained': 'models/pretrained/bert-base-uncased', 'repo_finetuned': 'models/finetuned/bert-base-uncased-imdb'}\n",
            "INFO:__main__:I modelli verranno salvati in: models/bert_base_uncased\n",
            "2025-04-16 16:28:57.770326: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-16 16:28:57.830208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-16 16:28:58.709143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:root:Caricamento dataset IMDb direttamente da Stanford...\n",
            "INFO:root:Dataset archive already exists, skipping download.\n",
            "INFO:root:Dataset already extracted, skipping extraction.\n",
            "INFO:root:Processing training data...\n",
            "INFO:root:Processing test data...\n",
            "INFO:root:Dataset caricato con successo: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "})\n",
            "INFO:root:Split disponibili: dict_keys(['train', 'test'])\n",
            "INFO:root:Numero di esempi - Train: 25000, Test: 25000\n",
            "INFO:root:Creazione split train/val/test...\n",
            "INFO:root:Train size: 20000, Val size: 5000, Test size: 25000\n",
            "INFO:__main__:Modalità EVAL: avvio dell'evaluation per il modello bert_base_uncased.\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:Carico il modello fine-tunato da models/finetuned/bert-base-uncased-imdb\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:Inizio valutazione completa (fine-tunato)...\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:Esecuzione valutazione finale completa sul test set...\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:\n",
            "Metriche finali complete (test set):\n",
            "Accuracy: 0.8734\n",
            "Precision: 0.8661\n",
            "Recall: 0.8834\n",
            "F1 Score: 0.8746\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:\n",
            "Classification Report Test:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo     0.8810    0.8634    0.8721     12500\n",
            "    positivo     0.8661    0.8834    0.8746     12500\n",
            "\n",
            "    accuracy                         0.8734     25000\n",
            "   macro avg     0.8735    0.8734    0.8734     25000\n",
            "weighted avg     0.8735    0.8734    0.8734     25000\n",
            "\n",
            "INFO:src.architectures.model_bert_base_uncased_imdb:Saved fine-tuned evaluation results to results/evaluation/finetuned/bert-base-uncased-imdb.json\n",
            "INFO:__main__:Risultati evaluation: {'accuracy': 0.8734, 'precision': 0.866107145658483, 'recall': 0.88336, 'f1': 0.8746485009307299}\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python main.py --model_config_key bert_base_uncased --mode eval --eval_type fine_tuned --output_json_path \"results/evaluation/finetuned/bert-base-uncased-imdb.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL6jXaBN14N_"
      },
      "source": [
        "### 3.2 **Train & Validation encoder-decoder**: facebook/bart-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BLO4f2go2Chw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9051231-379c-4732-c121-764a70e76da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Usando la configurazione di default: {'model_name': 'facebook/bart-base', 'epochs': 5, 'train_batch_size': 8, 'eval_batch_size': 4, 'learning_rate': 2e-05, 'repo_pretrained': 'models/pretrained/bart-base', 'repo_finetuned': 'models/finetuned/bart-base-imdb'}\n",
            "INFO:__main__:I modelli verranno salvati in: models/bart_base\n",
            "2025-04-16 16:32:57.259728: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-16 16:32:57.319807: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-16 16:32:58.214641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading vocab.json: 100% 899k/899k [00:00<00:00, 1.97MB/s]\n",
            "Downloading merges.txt: 100% 456k/456k [00:00<00:00, 8.39MB/s]\n",
            "Downloading config.json: 100% 1.72k/1.72k [00:00<00:00, 9.58MB/s]\n",
            "Downloading pytorch_model.bin: 100% 558M/558M [00:01<00:00, 395MB/s]\n",
            "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:root:Caricamento dataset IMDb direttamente da Stanford...\n",
            "INFO:root:Dataset archive already exists, skipping download.\n",
            "INFO:root:Dataset already extracted, skipping extraction.\n",
            "INFO:root:Processing training data...\n",
            "INFO:root:Processing test data...\n",
            "INFO:root:Dataset caricato con successo: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "})\n",
            "INFO:root:Split disponibili: dict_keys(['train', 'test'])\n",
            "INFO:root:Numero di esempi - Train: 25000, Test: 25000\n",
            "INFO:root:Creazione split train/val/test...\n",
            "INFO:root:Train size: 20000, Val size: 5000, Test size: 25000\n",
            "INFO:__main__:Modalità TRAIN: avvio del training per il modello bart_base.\n",
            "/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Training:   0% 0/12500 [00:00<?, ?it/s[0.0]]INFO:src.utils:Inizio training: 12500 step totali.\n",
            "Training:   1% 100/12500 [00:08<17:46, 11.63it/s[0.0]]{'loss': 0.6041, 'learning_rate': 4.96e-05, 'epoch': 0.04}\n",
            "Training:   2% 200/12500 [00:16<16:47, 12.20it/s[0.6041]]{'loss': 0.5209, 'learning_rate': 4.92e-05, 'epoch': 0.08}\n",
            "Training:   2% 300/12500 [00:24<16:25, 12.38it/s[0.5209]]{'loss': 0.4233, 'learning_rate': 4.88e-05, 'epoch': 0.12}\n",
            "Training:   3% 400/12500 [00:32<16:12, 12.45it/s[0.4233]]{'loss': 0.5037, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.16}\n",
            "Training:   4% 500/12500 [00:40<15:59, 12.50it/s[0.5037]]{'loss': 0.4433, 'learning_rate': 4.8e-05, 'epoch': 0.2}\n",
            "Training:   5% 600/12500 [00:48<15:52, 12.49it/s[0.4433]]{'loss': 0.4772, 'learning_rate': 4.76e-05, 'epoch': 0.24}\n",
            "Training:   6% 700/12500 [00:56<15:46, 12.47it/s[0.4772]]{'loss': 0.4241, 'learning_rate': 4.72e-05, 'epoch': 0.28}\n",
            "Training:   6% 800/12500 [01:04<15:36, 12.49it/s[0.4241]]{'loss': 0.4217, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.32}\n",
            "Training:   7% 900/12500 [01:12<15:30, 12.47it/s[0.4217]]{'loss': 0.4178, 'learning_rate': 4.64e-05, 'epoch': 0.36}\n",
            "Training:   8% 1000/12500 [01:20<15:21, 12.49it/s[0.4178]]{'loss': 0.401, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.4}\n",
            "Training:   9% 1100/12500 [01:28<15:17, 12.43it/s[0.401]] {'loss': 0.4261, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.44}\n",
            "Training:  10% 1200/12500 [01:36<15:12, 12.38it/s[0.4261]]{'loss': 0.4151, 'learning_rate': 4.52e-05, 'epoch': 0.48}\n",
            "Training:  10% 1300/12500 [01:44<15:05, 12.37it/s[0.4151]]{'loss': 0.3708, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.52}\n",
            "Training:  11% 1400/12500 [01:52<14:58, 12.35it/s[0.3708]]{'loss': 0.3927, 'learning_rate': 4.44e-05, 'epoch': 0.56}\n",
            "Training:  12% 1500/12500 [02:01<14:49, 12.36it/s[0.3927]]{'loss': 0.4078, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.6}\n",
            "Training:  13% 1600/12500 [02:09<14:42, 12.35it/s[0.4078]]{'loss': 0.3713, 'learning_rate': 4.36e-05, 'epoch': 0.64}\n",
            "Training:  14% 1700/12500 [02:17<14:34, 12.35it/s[0.3713]]{'loss': 0.3777, 'learning_rate': 4.32e-05, 'epoch': 0.68}\n",
            "Training:  14% 1800/12500 [02:25<14:26, 12.35it/s[0.3777]]{'loss': 0.411, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.72}\n",
            "Training:  15% 1900/12500 [02:33<14:18, 12.34it/s[0.411]] {'loss': 0.3814, 'learning_rate': 4.24e-05, 'epoch': 0.76}\n",
            "Training:  16% 2000/12500 [02:41<14:12, 12.32it/s[0.3814]]{'loss': 0.3919, 'learning_rate': 4.2e-05, 'epoch': 0.8}\n",
            "Training:  17% 2100/12500 [02:49<14:03, 12.32it/s[0.3919]]{'loss': 0.3652, 'learning_rate': 4.16e-05, 'epoch': 0.84}\n",
            "Training:  18% 2200/12500 [02:58<14:02, 12.23it/s[0.3652]]{'loss': 0.3796, 'learning_rate': 4.12e-05, 'epoch': 0.88}\n",
            "Training:  18% 2300/12500 [03:06<13:50, 12.28it/s[0.3796]]{'loss': 0.3786, 'learning_rate': 4.08e-05, 'epoch': 0.92}\n",
            "Training:  19% 2400/12500 [03:14<13:41, 12.30it/s[0.3786]]{'loss': 0.3473, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.96}\n",
            "Training:  20% 2500/12500 [03:22<13:31, 12.32it/s[0.3473]]{'loss': 0.3614, 'learning_rate': 4e-05, 'epoch': 1.0}\n",
            "Training:  20% 2500/12500 [03:39<13:31, 12.32it/s[0.3473]]Type of logits: <class 'numpy.ndarray'>\n",
            "Shape logits: (5000, 2)\n",
            "Shape labels: (5000,)\n",
            "{'eval_loss': 0.3348940908908844, 'eval_accuracy': 0.877, 'eval_runtime': 21.2829, 'eval_samples_per_second': 234.931, 'eval_steps_per_second': 29.366, 'epoch': 1.0}\n",
            "Training:  21% 2600/12500 [03:54<25:22,  6.50it/s[0.3473]]{'loss': 0.2709, 'learning_rate': 3.960000000000001e-05, 'epoch': 1.04}\n",
            "Training:  22% 2700/12500 [04:02<21:33,  7.58it/s[0.2709]]{'loss': 0.3136, 'learning_rate': 3.9200000000000004e-05, 'epoch': 1.08}\n",
            "Training:  22% 2800/12500 [04:10<18:53,  8.56it/s[0.3136]]{'loss': 0.308, 'learning_rate': 3.88e-05, 'epoch': 1.12}\n",
            "Training:  23% 2900/12500 [04:18<16:59,  9.41it/s[0.308]] {'loss': 0.2667, 'learning_rate': 3.8400000000000005e-05, 'epoch': 1.16}\n",
            "Training:  24% 3000/12500 [04:27<15:38, 10.12it/s[0.2667]]{'loss': 0.2971, 'learning_rate': 3.8e-05, 'epoch': 1.2}\n",
            "Training:  25% 3100/12500 [04:35<14:39, 10.69it/s[0.2971]]{'loss': 0.2902, 'learning_rate': 3.76e-05, 'epoch': 1.24}\n",
            "Training:  26% 3200/12500 [04:43<13:55, 11.13it/s[0.2902]]{'loss': 0.324, 'learning_rate': 3.72e-05, 'epoch': 1.28}\n",
            "Training:  26% 3300/12500 [04:51<13:23, 11.45it/s[0.324]] {'loss': 0.291, 'learning_rate': 3.68e-05, 'epoch': 1.32}\n",
            "Training:  27% 3400/12500 [04:59<12:58, 11.69it/s[0.291]]{'loss': 0.2522, 'learning_rate': 3.6400000000000004e-05, 'epoch': 1.36}\n",
            "Training:  28% 3500/12500 [05:07<12:39, 11.85it/s[0.2522]]{'loss': 0.2756, 'learning_rate': 3.6e-05, 'epoch': 1.4}\n",
            "Training:  29% 3600/12500 [05:15<12:22, 11.99it/s[0.2756]]{'loss': 0.3068, 'learning_rate': 3.56e-05, 'epoch': 1.44}\n",
            "Training:  30% 3700/12500 [05:24<12:08, 12.07it/s[0.3068]]{'loss': 0.3102, 'learning_rate': 3.52e-05, 'epoch': 1.48}\n",
            "Training:  30% 3800/12500 [05:32<11:56, 12.14it/s[0.3102]]{'loss': 0.3165, 'learning_rate': 3.48e-05, 'epoch': 1.52}\n",
            "Training:  31% 3900/12500 [05:40<11:46, 12.18it/s[0.3165]]{'loss': 0.3054, 'learning_rate': 3.4399999999999996e-05, 'epoch': 1.56}\n",
            "Training:  32% 4000/12500 [05:48<11:36, 12.21it/s[0.3054]]{'loss': 0.306, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.6}\n",
            "Training:  33% 4100/12500 [05:56<11:25, 12.25it/s[0.306]] {'loss': 0.2712, 'learning_rate': 3.3600000000000004e-05, 'epoch': 1.64}\n",
            "Training:  34% 4200/12500 [06:04<11:16, 12.27it/s[0.2712]]{'loss': 0.3108, 'learning_rate': 3.32e-05, 'epoch': 1.68}\n",
            "Training:  34% 4300/12500 [06:12<11:06, 12.30it/s[0.3108]]{'loss': 0.2824, 'learning_rate': 3.2800000000000004e-05, 'epoch': 1.72}\n",
            "Training:  35% 4400/12500 [06:21<11:03, 12.20it/s[0.2824]]{'loss': 0.2654, 'learning_rate': 3.24e-05, 'epoch': 1.76}\n",
            "Training:  36% 4500/12500 [06:29<10:54, 12.23it/s[0.2654]]{'loss': 0.337, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.8}\n",
            "Training:  37% 4600/12500 [06:37<10:44, 12.26it/s[0.337]] {'loss': 0.2684, 'learning_rate': 3.16e-05, 'epoch': 1.84}\n",
            "Training:  38% 4700/12500 [06:45<10:34, 12.29it/s[0.2684]]{'loss': 0.2993, 'learning_rate': 3.12e-05, 'epoch': 1.88}\n",
            "Training:  38% 4800/12500 [06:53<10:26, 12.29it/s[0.2993]]{'loss': 0.3057, 'learning_rate': 3.08e-05, 'epoch': 1.92}\n",
            "Training:  39% 4900/12500 [07:01<10:18, 12.29it/s[0.3057]]{'loss': 0.3327, 'learning_rate': 3.04e-05, 'epoch': 1.96}\n",
            "Training:  40% 5000/12500 [07:09<10:09, 12.31it/s[0.3327]]{'loss': 0.2699, 'learning_rate': 3e-05, 'epoch': 2.0}\n",
            "Training:  40% 5000/12500 [07:19<10:09, 12.31it/s[0.3327]]Type of logits: <class 'numpy.ndarray'>\n",
            "Shape logits: (5000, 2)\n",
            "Shape labels: (5000,)\n",
            "{'eval_loss': 0.5084385275840759, 'eval_accuracy': 0.88, 'eval_runtime': 15.3223, 'eval_samples_per_second': 326.323, 'eval_steps_per_second': 40.79, 'epoch': 2.0}\n",
            "Training:  41% 5100/12500 [07:36<16:44,  7.37it/s[0.3327]]{'loss': 0.186, 'learning_rate': 2.96e-05, 'epoch': 2.04}\n",
            "Training:  42% 5200/12500 [07:44<14:30,  8.39it/s[0.186]] {'loss': 0.2181, 'learning_rate': 2.9199999999999998e-05, 'epoch': 2.08}\n",
            "Training:  42% 5300/12500 [07:52<12:55,  9.28it/s[0.2181]]{'loss': 0.1703, 'learning_rate': 2.88e-05, 'epoch': 2.12}\n",
            "Training:  43% 5400/12500 [08:00<11:48, 10.02it/s[0.1703]]{'loss': 0.1659, 'learning_rate': 2.84e-05, 'epoch': 2.16}\n",
            "Training:  44% 5500/12500 [08:08<10:58, 10.63it/s[0.1659]]{'loss': 0.1902, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.2}\n",
            "Training:  45% 5600/12500 [08:16<10:21, 11.09it/s[0.1902]]{'loss': 0.1995, 'learning_rate': 2.7600000000000003e-05, 'epoch': 2.24}\n",
            "Training:  46% 5700/12500 [08:24<09:55, 11.42it/s[0.1995]]{'loss': 0.1832, 'learning_rate': 2.7200000000000004e-05, 'epoch': 2.28}\n",
            "Training:  46% 5800/12500 [08:32<09:33, 11.68it/s[0.1832]]{'loss': 0.1706, 'learning_rate': 2.6800000000000004e-05, 'epoch': 2.32}\n",
            "Training:  47% 5900/12500 [08:41<09:16, 11.86it/s[0.1706]]{'loss': 0.2174, 'learning_rate': 2.64e-05, 'epoch': 2.36}\n",
            "Training:  48% 6000/12500 [08:49<09:02, 11.99it/s[0.2174]]{'loss': 0.2241, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.4}\n",
            "Training:  49% 6100/12500 [08:57<08:49, 12.08it/s[0.2241]]{'loss': 0.2098, 'learning_rate': 2.5600000000000002e-05, 'epoch': 2.44}\n",
            "Training:  50% 6200/12500 [09:05<08:39, 12.14it/s[0.2098]]{'loss': 0.1512, 'learning_rate': 2.5200000000000003e-05, 'epoch': 2.48}\n",
            "Training:  50% 6300/12500 [09:13<08:28, 12.19it/s[0.1512]]{'loss': 0.2344, 'learning_rate': 2.48e-05, 'epoch': 2.52}\n",
            "Training:  51% 6400/12500 [09:21<08:19, 12.22it/s[0.2344]]{'loss': 0.1956, 'learning_rate': 2.44e-05, 'epoch': 2.56}\n",
            "Training:  52% 6500/12500 [09:29<08:09, 12.24it/s[0.1956]]{'loss': 0.1951, 'learning_rate': 2.4e-05, 'epoch': 2.6}\n",
            "Training:  53% 6600/12500 [09:38<08:03, 12.19it/s[0.1951]]{'loss': 0.1961, 'learning_rate': 2.36e-05, 'epoch': 2.64}\n",
            "Training:  54% 6700/12500 [09:46<07:53, 12.24it/s[0.1961]]{'loss': 0.174, 'learning_rate': 2.32e-05, 'epoch': 2.68}\n",
            "Training:  54% 6800/12500 [09:54<07:44, 12.26it/s[0.174]] {'loss': 0.1917, 'learning_rate': 2.2800000000000002e-05, 'epoch': 2.72}\n",
            "Training:  55% 6900/12500 [10:02<07:35, 12.29it/s[0.1917]]{'loss': 0.181, 'learning_rate': 2.2400000000000002e-05, 'epoch': 2.76}\n",
            "Training:  56% 7000/12500 [10:10<07:27, 12.28it/s[0.181]] {'loss': 0.2041, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.8}\n",
            "Training:  57% 7100/12500 [10:18<07:19, 12.30it/s[0.2041]]{'loss': 0.199, 'learning_rate': 2.16e-05, 'epoch': 2.84}\n",
            "Training:  58% 7200/12500 [10:26<07:10, 12.32it/s[0.199]] {'loss': 0.2355, 'learning_rate': 2.12e-05, 'epoch': 2.88}\n",
            "Training:  58% 7300/12500 [10:34<07:01, 12.34it/s[0.2355]]{'loss': 0.1697, 'learning_rate': 2.08e-05, 'epoch': 2.92}\n",
            "Training:  59% 7400/12500 [10:42<06:53, 12.34it/s[0.1697]]{'loss': 0.1874, 'learning_rate': 2.04e-05, 'epoch': 2.96}\n",
            "Training:  60% 7500/12500 [10:51<06:45, 12.33it/s[0.1874]]{'loss': 0.1661, 'learning_rate': 2e-05, 'epoch': 3.0}\n",
            "Type of logits: <class 'numpy.ndarray'>\n",
            "Shape logits: (5000, 2)\n",
            "Shape labels: (5000,)\n",
            "{'eval_loss': 0.5631409883499146, 'eval_accuracy': 0.8748, 'eval_runtime': 15.3171, 'eval_samples_per_second': 326.433, 'eval_steps_per_second': 40.804, 'epoch': 3.0}\n",
            "Training:  61% 7600/12500 [11:17<11:03,  7.38it/s[0.1874]]{'loss': 0.0818, 'learning_rate': 1.9600000000000002e-05, 'epoch': 3.04}\n",
            "Training:  62% 7700/12500 [11:25<09:32,  8.39it/s[0.0818]]{'loss': 0.1223, 'learning_rate': 1.9200000000000003e-05, 'epoch': 3.08}\n",
            "Training:  62% 7800/12500 [11:33<08:26,  9.28it/s[0.1223]]{'loss': 0.0665, 'learning_rate': 1.88e-05, 'epoch': 3.12}\n",
            "Training:  63% 7900/12500 [11:41<07:39, 10.02it/s[0.0665]]{'loss': 0.1201, 'learning_rate': 1.84e-05, 'epoch': 3.16}\n",
            "Training:  64% 8000/12500 [11:49<07:03, 10.61it/s[0.1201]]{'loss': 0.1272, 'learning_rate': 1.8e-05, 'epoch': 3.2}\n",
            "Training:  65% 8100/12500 [11:57<06:36, 11.09it/s[0.1272]]{'loss': 0.1297, 'learning_rate': 1.76e-05, 'epoch': 3.24}\n",
            "Training:  66% 8200/12500 [12:05<06:15, 11.44it/s[0.1297]]{'loss': 0.1356, 'learning_rate': 1.7199999999999998e-05, 'epoch': 3.28}\n",
            "Training:  66% 8300/12500 [12:14<05:59, 11.70it/s[0.1356]]{'loss': 0.1045, 'learning_rate': 1.6800000000000002e-05, 'epoch': 3.32}\n",
            "Training:  67% 8400/12500 [12:22<05:45, 11.88it/s[0.1045]]{'loss': 0.0732, 'learning_rate': 1.6400000000000002e-05, 'epoch': 3.36}\n",
            "Training:  68% 8500/12500 [12:30<05:32, 12.02it/s[0.0732]]{'loss': 0.1483, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.4}\n",
            "Training:  69% 8600/12500 [12:38<05:22, 12.10it/s[0.1483]]{'loss': 0.1237, 'learning_rate': 1.56e-05, 'epoch': 3.44}\n",
            "Training:  70% 8700/12500 [12:46<05:14, 12.09it/s[0.1237]]{'loss': 0.1001, 'learning_rate': 1.52e-05, 'epoch': 3.48}\n",
            "Training:  70% 8800/12500 [12:54<05:03, 12.18it/s[0.1001]]{'loss': 0.1757, 'learning_rate': 1.48e-05, 'epoch': 3.52}\n",
            "Training:  71% 8900/12500 [13:02<04:54, 12.24it/s[0.1757]]{'loss': 0.1236, 'learning_rate': 1.44e-05, 'epoch': 3.56}\n",
            "Training:  72% 9000/12500 [13:10<04:45, 12.27it/s[0.1236]]{'loss': 0.1255, 'learning_rate': 1.4000000000000001e-05, 'epoch': 3.6}\n",
            "Training:  73% 9100/12500 [13:19<04:36, 12.30it/s[0.1255]]{'loss': 0.0719, 'learning_rate': 1.3600000000000002e-05, 'epoch': 3.64}\n",
            "Training:  74% 9200/12500 [13:26<04:26, 12.37it/s[0.0719]]{'loss': 0.108, 'learning_rate': 1.32e-05, 'epoch': 3.68}\n",
            "Training:  74% 9300/12500 [13:34<04:16, 12.45it/s[0.108]] {'loss': 0.1364, 'learning_rate': 1.2800000000000001e-05, 'epoch': 3.72}\n",
            "Training:  75% 9400/12500 [13:42<04:07, 12.51it/s[0.1364]]{'loss': 0.0662, 'learning_rate': 1.24e-05, 'epoch': 3.76}\n",
            "Training:  76% 9500/12500 [13:50<03:58, 12.57it/s[0.0662]]{'loss': 0.1386, 'learning_rate': 1.2e-05, 'epoch': 3.8}\n",
            "Training:  77% 9600/12500 [13:58<03:50, 12.58it/s[0.1386]]{'loss': 0.1232, 'learning_rate': 1.16e-05, 'epoch': 3.84}\n",
            "Training:  78% 9700/12500 [14:06<03:42, 12.58it/s[0.1232]]{'loss': 0.1021, 'learning_rate': 1.1200000000000001e-05, 'epoch': 3.88}\n",
            "Training:  78% 9800/12500 [14:14<03:33, 12.63it/s[0.1021]]{'loss': 0.0687, 'learning_rate': 1.08e-05, 'epoch': 3.92}\n",
            "Training:  79% 9900/12500 [14:22<03:25, 12.65it/s[0.0687]]{'loss': 0.1154, 'learning_rate': 1.04e-05, 'epoch': 3.96}\n",
            "Training:  80% 10000/12500 [14:30<03:17, 12.67it/s[0.1154]]{'loss': 0.0793, 'learning_rate': 1e-05, 'epoch': 4.0}\n",
            "Type of logits: <class 'numpy.ndarray'>\n",
            "Shape logits: (5000, 2)\n",
            "Shape labels: (5000,)\n",
            "{'eval_loss': 0.6433278322219849, 'eval_accuracy': 0.8848, 'eval_runtime': 14.9452, 'eval_samples_per_second': 334.554, 'eval_steps_per_second': 41.819, 'epoch': 4.0}\n",
            "Training:  81% 10100/12500 [14:56<05:19,  7.52it/s[0.1154]]{'loss': 0.0458, 'learning_rate': 9.600000000000001e-06, 'epoch': 4.04}\n",
            "Training:  82% 10200/12500 [15:04<04:30,  8.51it/s[0.0458]]{'loss': 0.0631, 'learning_rate': 9.2e-06, 'epoch': 4.08}\n",
            "Training:  82% 10300/12500 [15:12<03:54,  9.38it/s[0.0631]]{'loss': 0.0713, 'learning_rate': 8.8e-06, 'epoch': 4.12}\n",
            "Training:  83% 10400/12500 [15:20<03:28, 10.09it/s[0.0713]]{'loss': 0.0299, 'learning_rate': 8.400000000000001e-06, 'epoch': 4.16}\n",
            "Training:  84% 10500/12500 [15:28<03:07, 10.67it/s[0.0299]]{'loss': 0.0614, 'learning_rate': 8.000000000000001e-06, 'epoch': 4.2}\n",
            "Training:  85% 10600/12500 [15:36<02:50, 11.11it/s[0.0614]]{'loss': 0.0618, 'learning_rate': 7.6e-06, 'epoch': 4.24}\n",
            "Training:  86% 10700/12500 [15:44<02:37, 11.46it/s[0.0618]]{'loss': 0.0571, 'learning_rate': 7.2e-06, 'epoch': 4.28}\n",
            "Training:  86% 10800/12500 [15:52<02:25, 11.70it/s[0.0571]]{'loss': 0.0365, 'learning_rate': 6.800000000000001e-06, 'epoch': 4.32}\n",
            "Training:  87% 10900/12500 [16:01<02:15, 11.77it/s[0.0365]]{'loss': 0.0361, 'learning_rate': 6.4000000000000006e-06, 'epoch': 4.36}\n",
            "Training:  88% 11000/12500 [16:09<02:05, 11.93it/s[0.0361]]{'loss': 0.0566, 'learning_rate': 6e-06, 'epoch': 4.4}\n",
            "Training:  89% 11100/12500 [16:17<01:56, 12.02it/s[0.0566]]{'loss': 0.0951, 'learning_rate': 5.600000000000001e-06, 'epoch': 4.44}\n",
            "Training:  90% 11200/12500 [16:25<01:47, 12.11it/s[0.0951]]{'loss': 0.059, 'learning_rate': 5.2e-06, 'epoch': 4.48}\n",
            "Training:  90% 11300/12500 [16:33<01:38, 12.17it/s[0.059]] {'loss': 0.0859, 'learning_rate': 4.800000000000001e-06, 'epoch': 4.52}\n",
            "Training:  91% 11400/12500 [16:41<01:30, 12.19it/s[0.0859]]{'loss': 0.0459, 'learning_rate': 4.4e-06, 'epoch': 4.56}\n",
            "Training:  92% 11500/12500 [16:50<01:21, 12.20it/s[0.0459]]{'loss': 0.0662, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.6}\n",
            "Training:  93% 11600/12500 [16:58<01:13, 12.24it/s[0.0662]]{'loss': 0.0751, 'learning_rate': 3.6e-06, 'epoch': 4.64}\n",
            "Training:  94% 11700/12500 [17:06<01:05, 12.27it/s[0.0751]]{'loss': 0.0779, 'learning_rate': 3.2000000000000003e-06, 'epoch': 4.68}\n",
            "Training:  94% 11800/12500 [17:14<00:56, 12.29it/s[0.0779]]{'loss': 0.0626, 'learning_rate': 2.8000000000000003e-06, 'epoch': 4.72}\n",
            "Training:  95% 11900/12500 [17:22<00:48, 12.30it/s[0.0626]]{'loss': 0.0658, 'learning_rate': 2.4000000000000003e-06, 'epoch': 4.76}\n",
            "Training:  96% 12000/12500 [17:30<00:40, 12.30it/s[0.0658]]{'loss': 0.0771, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.8}\n",
            "Training:  97% 12100/12500 [17:38<00:32, 12.30it/s[0.0771]]{'loss': 0.055, 'learning_rate': 1.6000000000000001e-06, 'epoch': 4.84}\n",
            "Training:  98% 12200/12500 [17:46<00:24, 12.31it/s[0.055]] {'loss': 0.0754, 'learning_rate': 1.2000000000000002e-06, 'epoch': 4.88}\n",
            "Training:  98% 12300/12500 [17:55<00:16, 12.32it/s[0.0754]]{'loss': 0.0384, 'learning_rate': 8.000000000000001e-07, 'epoch': 4.92}\n",
            "Training:  99% 12400/12500 [18:03<00:08, 12.31it/s[0.0384]]{'loss': 0.059, 'learning_rate': 4.0000000000000003e-07, 'epoch': 4.96}\n",
            "Training: 100% 12500/12500 [18:11<00:00, 12.31it/s[0.059]] {'loss': 0.0588, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "Type of logits: <class 'numpy.ndarray'>\n",
            "Shape logits: (5000, 2)\n",
            "Shape labels: (5000,)\n",
            "{'eval_loss': 0.6770138144493103, 'eval_accuracy': 0.8826, 'eval_runtime': 15.3605, 'eval_samples_per_second': 325.51, 'eval_steps_per_second': 40.689, 'epoch': 5.0}\n",
            "Training: 100% 12500/12500 [18:29<00:00, 12.31it/s[0.059]]{'train_runtime': 1110.2182, 'train_samples_per_second': 90.072, 'train_steps_per_second': 11.259, 'train_loss': 0.215138486328125, 'epoch': 5.0}\n",
            "Training: 100% 12500/12500 [18:30<00:00, 11.26it/s[0.059]]\n",
            "INFO:src.utils:Training completato.\n",
            "INFO:src.architectures.model_bart_base_imdb:Training completato con metriche finali: {'train_runtime': 1110.2182, 'train_samples_per_second': 90.072, 'train_steps_per_second': 11.259, 'total_flos': 7667181312000000.0, 'train_loss': 0.215138486328125, 'epoch': 5.0, 'step': 12500}\n",
            "INFO:src.architectures.model_bart_base_imdb:Training metrics salvate in results/validation/finetuned/bart-base-imdb_metrics.json\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python main.py --model_config_key bart_base --mode train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCgC4WZ62Jg_"
      },
      "source": [
        "### 3.2 **Evaluation encoder-decoder pre-trained**: facebook/bart-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3x2ABypb2SwM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "495c83c4-f657-4e06-f0c3-34c6b6e75085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Usando la configurazione di default: {'model_name': 'facebook/bart-base', 'epochs': 5, 'train_batch_size': 8, 'eval_batch_size': 4, 'learning_rate': 2e-05, 'repo_pretrained': 'models/pretrained/bart-base', 'repo_finetuned': 'models/finetuned/bart-base-imdb'}\n",
            "INFO:__main__:I modelli verranno salvati in: models/bart_base\n",
            "2025-04-16 16:53:06.586690: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-16 16:53:06.647920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-16 16:53:07.583221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:root:Caricamento dataset IMDb direttamente da Stanford...\n",
            "INFO:root:Dataset archive already exists, skipping download.\n",
            "INFO:root:Dataset already extracted, skipping extraction.\n",
            "INFO:root:Processing training data...\n",
            "INFO:root:Processing test data...\n",
            "INFO:root:Dataset caricato con successo: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "})\n",
            "INFO:root:Split disponibili: dict_keys(['train', 'test'])\n",
            "INFO:root:Numero di esempi - Train: 25000, Test: 25000\n",
            "INFO:root:Creazione split train/val/test...\n",
            "INFO:root:Train size: 20000, Val size: 5000, Test size: 25000\n",
            "INFO:__main__:Modalità EVAL: avvio dell'evaluation per il modello bart_base.\n",
            "INFO:src.architectures.model_bart_base_imdb:Valutazione sul modello pre-addestrato (test set)...\n",
            "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:src.architectures.model_bart_base_imdb:Carico il modello pre-addestrato da facebook/bart-base\n",
            "INFO:src.architectures.model_bart_base_imdb:Inizio valutazione sul test set (modello pre-addestrato)...\n",
            "INFO:src.architectures.model_bart_base_imdb:Esecuzione valutazione finale completa sul test set...\n",
            "INFO:src.architectures.model_bart_base_imdb:\n",
            "Metriche finali complete (test set):\n",
            "Accuracy: 0.5157\n",
            "Precision: 0.5269\n",
            "Recall: 0.3079\n",
            "F1 Score: 0.3887\n",
            "INFO:src.architectures.model_bart_base_imdb:\n",
            "Classification Report Test:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo     0.5111    0.7235    0.5990     12500\n",
            "    positivo     0.5269    0.3079    0.3887     12500\n",
            "\n",
            "    accuracy                         0.5157     25000\n",
            "   macro avg     0.5190    0.5157    0.4939     25000\n",
            "weighted avg     0.5190    0.5157    0.4939     25000\n",
            "\n",
            "INFO:src.architectures.model_bart_base_imdb:Saved pretrained evaluation results to results/evaluation/pretrained/bart-base-imdb.json\n",
            "INFO:__main__:Risultati evaluation: {'accuracy': 0.51572, 'precision': 0.5268993839835729, 'recall': 0.30792, 'f1': 0.38868972481696545}\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python main.py --model_config_key bart_base --mode eval --eval_type pretrained --output_json_path \"results/evaluation/pretrained/bart-base-imdb.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWpKpVBG2aB_"
      },
      "source": [
        "### 3.2 **Evaluation encoder-decoder fine-tuned**: facebook/bart-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0XdYl80n6JSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eda36b3-eec4-41fd-842d-da377bb333dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Usando la configurazione di default: {'model_name': 'facebook/bart-base', 'epochs': 5, 'train_batch_size': 8, 'eval_batch_size': 4, 'learning_rate': 2e-05, 'repo_pretrained': 'models/pretrained/bart-base', 'repo_finetuned': 'models/finetuned/bart-base-imdb'}\n",
            "INFO:__main__:I modelli verranno salvati in: models/bart_base\n",
            "2025-04-16 17:02:27.095064: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-16 17:02:27.156016: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-16 17:02:28.096695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.weight', 'classification_head.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:root:Caricamento dataset IMDb direttamente da Stanford...\n",
            "INFO:root:Dataset archive already exists, skipping download.\n",
            "INFO:root:Dataset already extracted, skipping extraction.\n",
            "INFO:root:Processing training data...\n",
            "INFO:root:Processing test data...\n",
            "INFO:root:Dataset caricato con successo: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "})\n",
            "INFO:root:Split disponibili: dict_keys(['train', 'test'])\n",
            "INFO:root:Numero di esempi - Train: 25000, Test: 25000\n",
            "INFO:root:Creazione split train/val/test...\n",
            "INFO:root:Train size: 20000, Val size: 5000, Test size: 25000\n",
            "INFO:__main__:Modalità EVAL: avvio dell'evaluation per il modello bart_base.\n",
            "INFO:src.architectures.model_bart_base_imdb:Carico il modello fine-tunato da models/finetuned/bart-base-imdb\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
            "INFO:src.architectures.model_bart_base_imdb:Inizio valutazione sul test set (modello fine-tunato)...\n",
            "INFO:src.architectures.model_bart_base_imdb:Esecuzione valutazione finale completa sul test set...\n",
            "INFO:src.architectures.model_bart_base_imdb:\n",
            "Metriche finali complete (test set):\n",
            "Accuracy: 0.8797\n",
            "Precision: 0.8839\n",
            "Recall: 0.8742\n",
            "F1 Score: 0.8790\n",
            "INFO:src.architectures.model_bart_base_imdb:\n",
            "Classification Report Test:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo     0.8756    0.8851    0.8803     12500\n",
            "    positivo     0.8839    0.8742    0.8790     12500\n",
            "\n",
            "    accuracy                         0.8797     25000\n",
            "   macro avg     0.8797    0.8797    0.8797     25000\n",
            "weighted avg     0.8797    0.8797    0.8797     25000\n",
            "\n",
            "INFO:src.architectures.model_bart_base_imdb:Valutazione completata con risultati: {'accuracy': 0.87968, 'precision': 0.8838563571659657, 'recall': 0.87424, 'f1': 0.8790218790218789}\n",
            "INFO:src.architectures.model_bart_base_imdb:Saved fine-tuned evaluation results to results/evaluation/finetuned/bart-base-imdb.json\n",
            "INFO:__main__:Risultati evaluation: {'accuracy': 0.87968, 'precision': 0.8838563571659657, 'recall': 0.87424, 'f1': 0.8790218790218789}\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python main.py --model_config_key bart_base --mode eval --eval_type fine_tuned --output_json_path \"results/evaluation/finetuned/bart-base-imdb.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQa_K-962iT1"
      },
      "source": [
        "### 3.3 **Train & Validation decoder-only**: EleutherAI/gpt-neo-2.7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AKtiGH-m_9RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ff346e-93ac-48b9-f87f-d157f8943a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Usando la configurazione di default: {'model_name': 'EleutherAI/gpt-neo-2.7B', 'epochs': 3, 'train_batch_size': 1, 'eval_batch_size': 1, 'learning_rate': 0.0005, 'gradient_accumulation_steps': 8, 'repo_pretrained': 'models/pretrained/gpt-neo-2.7B', 'repo_finetuned': 'models/finetuned/gpt-neo-2.7B-imdb'}\n",
            "INFO:__main__:I modelli verranno salvati in: models/gpt_neo_2_7b\n",
            "2025-04-16 17:51:04.049820: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-16 17:51:04.109433: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-16 17:51:05.032173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-2.7B and are newly initialized: ['transformer.h.19.attn.attention.bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.17.attn.attention.bias', 'transformer.h.27.attn.attention.bias', 'transformer.h.31.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.13.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.11.attn.attention.bias', 'score.weight', 'transformer.h.5.attn.attention.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.25.attn.attention.bias', 'transformer.h.29.attn.attention.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:PAD token id (tokenizer): 50256\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:PAD token id (model config): 50256\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Congelo i primi 31 blocchi di GPT-Neo.\n",
            "INFO:root:Caricamento dataset IMDb direttamente da Stanford...\n",
            "INFO:root:Dataset archive already exists, skipping download.\n",
            "INFO:root:Dataset already extracted, skipping extraction.\n",
            "INFO:root:Processing training data...\n",
            "INFO:root:Processing test data...\n",
            "INFO:root:Dataset caricato con successo: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "})\n",
            "INFO:root:Split disponibili: dict_keys(['train', 'test'])\n",
            "INFO:root:Numero di esempi - Train: 25000, Test: 25000\n",
            "INFO:root:Creazione split train/val/test...\n",
            "INFO:root:Train size: 20000, Val size: 5000, Test size: 25000\n",
            "INFO:__main__:Modalità TRAIN: avvio del training per il modello gpt_neo_2_7b.\n",
            "/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Training:   0% 0/60000 [00:00<?, ?it/s[0.0]]INFO:src.utils:Inizio training: 60000 step totali.\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Training:   0% 100/60000 [00:17<2:55:23,  5.69it/s[0.0]]{'loss': 2.0587, 'learning_rate': 4.991666666666667e-05, 'epoch': 0.01}\n",
            "Training:   0% 200/60000 [00:34<2:53:47,  5.74it/s[2.0587]]{'loss': 1.4871, 'learning_rate': 4.9833333333333336e-05, 'epoch': 0.01}\n",
            "Training:   0% 300/60000 [00:52<2:52:29,  5.77it/s[1.4871]]{'loss': 1.2961, 'learning_rate': 4.975e-05, 'epoch': 0.01}\n",
            "Training:   1% 400/60000 [01:09<2:51:10,  5.80it/s[1.2961]]{'loss': 1.2899, 'learning_rate': 4.966666666666667e-05, 'epoch': 0.02}\n",
            "Training:   1% 500/60000 [01:26<2:50:28,  5.82it/s[1.2899]]{'loss': 1.3594, 'learning_rate': 4.958333333333334e-05, 'epoch': 0.03}\n",
            "Training:   1% 600/60000 [01:43<2:49:53,  5.83it/s[1.3594]]{'loss': 1.0756, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.03}\n",
            "Training:   1% 700/60000 [02:00<2:49:31,  5.83it/s[1.0756]]{'loss': 1.7463, 'learning_rate': 4.9416666666666664e-05, 'epoch': 0.04}\n",
            "Training:   1% 800/60000 [02:17<2:49:15,  5.83it/s[1.7463]]{'loss': 1.4529, 'learning_rate': 4.933333333333334e-05, 'epoch': 0.04}\n",
            "Training:   2% 900/60000 [02:34<2:48:45,  5.84it/s[1.4529]]{'loss': 1.3358, 'learning_rate': 4.9250000000000004e-05, 'epoch': 0.04}\n",
            "Training:   2% 1000/60000 [02:51<2:48:22,  5.84it/s[1.3358]]{'loss': 1.0217, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.05}\n",
            "Training:   2% 1100/60000 [03:08<2:47:56,  5.85it/s[1.0217]]{'loss': 1.5605, 'learning_rate': 4.908333333333334e-05, 'epoch': 0.06}\n",
            "Training:   2% 1200/60000 [03:26<2:47:45,  5.84it/s[1.5605]]{'loss': 0.7723, 'learning_rate': 4.9e-05, 'epoch': 0.06}\n",
            "Training:   2% 1300/60000 [03:43<2:47:24,  5.84it/s[0.7723]]{'loss': 1.7398, 'learning_rate': 4.891666666666667e-05, 'epoch': 0.07}\n",
            "Training:   2% 1400/60000 [04:00<2:47:07,  5.84it/s[1.7398]]{'loss': 1.1367, 'learning_rate': 4.883333333333334e-05, 'epoch': 0.07}\n",
            "Training:   2% 1500/60000 [04:17<2:46:39,  5.85it/s[1.1367]]{'loss': 0.6408, 'learning_rate': 4.875e-05, 'epoch': 0.07}\n",
            "Training:   3% 1600/60000 [04:34<2:46:29,  5.85it/s[0.6408]]{'loss': 1.2607, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.08}\n",
            "Training:   3% 1700/60000 [04:51<2:46:12,  5.85it/s[1.2607]]{'loss': 1.0662, 'learning_rate': 4.858333333333333e-05, 'epoch': 0.09}\n",
            "Training:   3% 1800/60000 [05:08<2:45:47,  5.85it/s[1.0662]]{'loss': 1.0904, 'learning_rate': 4.85e-05, 'epoch': 0.09}\n",
            "Training:   3% 1900/60000 [05:25<2:45:29,  5.85it/s[1.0904]]{'loss': 0.8527, 'learning_rate': 4.8416666666666673e-05, 'epoch': 0.1}\n",
            "Training:   3% 2000/60000 [05:42<2:45:29,  5.84it/s[0.8527]]{'loss': 1.0514, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.1}\n",
            "Training:   4% 2100/60000 [06:00<2:45:23,  5.83it/s[1.0514]]{'loss': 0.9799, 'learning_rate': 4.825e-05, 'epoch': 0.1}\n",
            "Training:   4% 2200/60000 [06:17<2:45:31,  5.82it/s[0.9799]]{'loss': 1.0557, 'learning_rate': 4.8166666666666674e-05, 'epoch': 0.11}\n",
            "Training:   4% 2300/60000 [06:34<2:45:08,  5.82it/s[1.0557]]{'loss': 0.6908, 'learning_rate': 4.8083333333333334e-05, 'epoch': 0.12}\n",
            "Training:   4% 2400/60000 [06:51<2:44:50,  5.82it/s[0.6908]]{'loss': 0.9751, 'learning_rate': 4.8e-05, 'epoch': 0.12}\n",
            "Training:   4% 2500/60000 [07:08<2:44:24,  5.83it/s[0.9751]]{'loss': 1.0973, 'learning_rate': 4.791666666666667e-05, 'epoch': 0.12}\n",
            "Training:   4% 2600/60000 [07:26<2:44:13,  5.83it/s[1.0973]]{'loss': 1.3268, 'learning_rate': 4.7833333333333335e-05, 'epoch': 0.13}\n",
            "Training:   4% 2700/60000 [07:43<2:43:43,  5.83it/s[1.3268]]{'loss': 1.0841, 'learning_rate': 4.775e-05, 'epoch': 0.14}\n",
            "Training:   5% 2800/60000 [08:00<2:43:29,  5.83it/s[1.0841]]{'loss': 1.4282, 'learning_rate': 4.766666666666667e-05, 'epoch': 0.14}\n",
            "Training:   5% 2900/60000 [08:17<2:43:11,  5.83it/s[1.4282]]{'loss': 0.8994, 'learning_rate': 4.7583333333333336e-05, 'epoch': 0.14}\n",
            "Training:   5% 3000/60000 [08:34<2:42:54,  5.83it/s[0.8994]]{'loss': 1.2161, 'learning_rate': 4.75e-05, 'epoch': 0.15}\n",
            "Training:   5% 3100/60000 [08:51<2:42:40,  5.83it/s[1.2161]]{'loss': 0.8764, 'learning_rate': 4.741666666666667e-05, 'epoch': 0.15}\n",
            "Training:   5% 3200/60000 [09:08<2:42:25,  5.83it/s[0.8764]]{'loss': 0.6382, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.16}\n",
            "Training:   6% 3300/60000 [09:26<2:42:03,  5.83it/s[0.6382]]{'loss': 0.7304, 'learning_rate': 4.7249999999999997e-05, 'epoch': 0.17}\n",
            "Training:   6% 3400/60000 [09:43<2:41:35,  5.84it/s[0.7304]]{'loss': 0.5389, 'learning_rate': 4.716666666666667e-05, 'epoch': 0.17}\n",
            "Training:   6% 3500/60000 [10:00<2:41:15,  5.84it/s[0.5389]]{'loss': 0.6282, 'learning_rate': 4.708333333333334e-05, 'epoch': 0.17}\n",
            "Training:   6% 3600/60000 [10:17<2:41:25,  5.82it/s[0.6282]]{'loss': 1.4229, 'learning_rate': 4.7e-05, 'epoch': 0.18}\n",
            "Training:   6% 3700/60000 [10:34<2:41:03,  5.83it/s[1.4229]]{'loss': 1.08, 'learning_rate': 4.691666666666667e-05, 'epoch': 0.18}\n",
            "Training:   6% 3800/60000 [10:51<2:40:42,  5.83it/s[1.08]]{'loss': 0.7994, 'learning_rate': 4.683333333333334e-05, 'epoch': 0.19}\n",
            "Training:   6% 3900/60000 [11:08<2:40:26,  5.83it/s[0.7994]]{'loss': 0.6779, 'learning_rate': 4.6750000000000005e-05, 'epoch': 0.2}\n",
            "Training:   7% 4000/60000 [11:26<2:40:08,  5.83it/s[0.6779]]{'loss': 0.6911, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}\n",
            "Training:   7% 4100/60000 [11:43<2:39:43,  5.83it/s[0.6911]]{'loss': 1.1897, 'learning_rate': 4.658333333333333e-05, 'epoch': 0.2}\n",
            "Training:   7% 4200/60000 [12:00<2:39:31,  5.83it/s[1.1897]]{'loss': 0.9493, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.21}\n",
            "Training:   7% 4300/60000 [12:17<2:39:05,  5.84it/s[0.9493]]{'loss': 1.1738, 'learning_rate': 4.641666666666667e-05, 'epoch': 0.21}\n",
            "Training:   7% 4400/60000 [12:34<2:38:54,  5.83it/s[1.1738]]{'loss': 0.8217, 'learning_rate': 4.633333333333333e-05, 'epoch': 0.22}\n",
            "Training:   8% 4500/60000 [12:51<2:38:52,  5.82it/s[0.8217]]{'loss': 1.6405, 'learning_rate': 4.6250000000000006e-05, 'epoch': 0.23}\n",
            "Training:   8% 4600/60000 [13:09<2:38:41,  5.82it/s[1.6405]]{'loss': 0.7354, 'learning_rate': 4.6166666666666666e-05, 'epoch': 0.23}\n",
            "Training:   8% 4700/60000 [13:26<2:38:39,  5.81it/s[0.7354]]{'loss': 0.8686, 'learning_rate': 4.608333333333333e-05, 'epoch': 0.23}\n",
            "Training:   8% 4800/60000 [13:43<2:38:25,  5.81it/s[0.8686]]{'loss': 0.8334, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.24}\n",
            "Training:   8% 4900/60000 [14:00<2:38:07,  5.81it/s[0.8334]]{'loss': 1.3061, 'learning_rate': 4.591666666666667e-05, 'epoch': 0.24}\n",
            "Training:   8% 5000/60000 [14:18<2:38:00,  5.80it/s[1.3061]]{'loss': 1.1039, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n",
            "Training:   8% 5100/60000 [14:35<2:37:36,  5.81it/s[1.1039]]{'loss': 0.9722, 'learning_rate': 4.575e-05, 'epoch': 0.26}\n",
            "Training:   9% 5200/60000 [14:52<2:37:24,  5.80it/s[0.9722]]{'loss': 0.5424, 'learning_rate': 4.566666666666667e-05, 'epoch': 0.26}\n",
            "Training:   9% 5300/60000 [15:09<2:37:04,  5.80it/s[0.5424]]{'loss': 1.0609, 'learning_rate': 4.5583333333333335e-05, 'epoch': 0.27}\n",
            "Training:   9% 5400/60000 [15:27<2:36:46,  5.80it/s[1.0609]]{'loss': 0.8194, 'learning_rate': 4.55e-05, 'epoch': 0.27}\n",
            "Training:   9% 5500/60000 [15:44<2:36:27,  5.81it/s[0.8194]]{'loss': 0.8444, 'learning_rate': 4.541666666666667e-05, 'epoch': 0.28}\n",
            "Training:   9% 5600/60000 [16:01<2:35:45,  5.82it/s[0.8444]]{'loss': 0.8106, 'learning_rate': 4.5333333333333335e-05, 'epoch': 0.28}\n",
            "Training:  10% 5700/60000 [16:18<2:35:13,  5.83it/s[0.8106]]{'loss': 0.4718, 'learning_rate': 4.525e-05, 'epoch': 0.28}\n",
            "Training:  10% 5800/60000 [16:35<2:34:54,  5.83it/s[0.4718]]{'loss': 1.2393, 'learning_rate': 4.516666666666667e-05, 'epoch': 0.29}\n",
            "Training:  10% 5900/60000 [16:52<2:34:43,  5.83it/s[1.2393]]{'loss': 0.7626, 'learning_rate': 4.5083333333333336e-05, 'epoch': 0.29}\n",
            "Training:  10% 6000/60000 [17:09<2:34:22,  5.83it/s[0.7626]]{'loss': 0.5782, 'learning_rate': 4.5e-05, 'epoch': 0.3}\n",
            "Training:  10% 6100/60000 [17:27<2:34:11,  5.83it/s[0.5782]]{'loss': 1.21, 'learning_rate': 4.491666666666667e-05, 'epoch': 0.3}\n",
            "Training:  10% 6200/60000 [17:44<2:33:53,  5.83it/s[1.21]]{'loss': 0.7057, 'learning_rate': 4.483333333333333e-05, 'epoch': 0.31}\n",
            "Training:  10% 6300/60000 [18:01<2:33:31,  5.83it/s[0.7057]]{'loss': 1.3764, 'learning_rate': 4.4750000000000004e-05, 'epoch': 0.32}\n",
            "Training:  11% 6400/60000 [18:18<2:33:17,  5.83it/s[1.3764]]{'loss': 1.0291, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.32}\n",
            "Training:  11% 6500/60000 [18:35<2:32:58,  5.83it/s[1.0291]]{'loss': 0.9566, 'learning_rate': 4.458333333333334e-05, 'epoch': 0.33}\n",
            "Training:  11% 6600/60000 [18:52<2:32:39,  5.83it/s[0.9566]]{'loss': 0.8585, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.33}\n",
            "Training:  11% 6700/60000 [19:09<2:32:17,  5.83it/s[0.8585]]{'loss': 0.6281, 'learning_rate': 4.4416666666666664e-05, 'epoch': 0.34}\n",
            "Training:  11% 6800/60000 [19:27<2:31:51,  5.84it/s[0.6281]]{'loss': 1.0282, 'learning_rate': 4.433333333333334e-05, 'epoch': 0.34}\n",
            "Training:  12% 6900/60000 [19:44<2:31:26,  5.84it/s[1.0282]]{'loss': 0.7653, 'learning_rate': 4.4250000000000005e-05, 'epoch': 0.34}\n",
            "Training:  12% 7000/60000 [20:01<2:31:08,  5.84it/s[0.7653]]{'loss': 0.8545, 'learning_rate': 4.4166666666666665e-05, 'epoch': 0.35}\n",
            "Training:  12% 7100/60000 [20:18<2:30:46,  5.85it/s[0.8545]]{'loss': 0.9249, 'learning_rate': 4.408333333333334e-05, 'epoch': 0.35}\n",
            "Training:  12% 7200/60000 [20:35<2:30:35,  5.84it/s[0.9249]]{'loss': 0.6543, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.36}\n",
            "Training:  12% 7300/60000 [20:52<2:30:22,  5.84it/s[0.6543]]{'loss': 0.8855, 'learning_rate': 4.3916666666666666e-05, 'epoch': 0.36}\n",
            "Training:  12% 7400/60000 [21:09<2:30:07,  5.84it/s[0.8855]]{'loss': 1.305, 'learning_rate': 4.383333333333334e-05, 'epoch': 0.37}\n",
            "Training:  12% 7500/60000 [21:26<2:29:43,  5.84it/s[1.305]]{'loss': 0.5166, 'learning_rate': 4.375e-05, 'epoch': 0.38}\n",
            "Training:  13% 7600/60000 [21:43<2:29:30,  5.84it/s[0.5166]]{'loss': 0.795, 'learning_rate': 4.3666666666666666e-05, 'epoch': 0.38}\n",
            "Training:  13% 7700/60000 [22:01<2:29:11,  5.84it/s[0.795]]{'loss': 0.8554, 'learning_rate': 4.358333333333334e-05, 'epoch': 0.39}\n",
            "Training:  13% 7800/60000 [22:18<2:29:15,  5.83it/s[0.8554]]{'loss': 1.1174, 'learning_rate': 4.35e-05, 'epoch': 0.39}\n",
            "Training:  13% 7900/60000 [22:35<2:28:50,  5.83it/s[1.1174]]{'loss': 0.5134, 'learning_rate': 4.341666666666667e-05, 'epoch': 0.4}\n",
            "Training:  13% 8000/60000 [22:52<2:28:34,  5.83it/s[0.5134]]{'loss': 0.9255, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n",
            "Training:  14% 8100/60000 [23:09<2:28:12,  5.84it/s[0.9255]]{'loss': 0.6407, 'learning_rate': 4.325e-05, 'epoch': 0.41}\n",
            "Training:  14% 8200/60000 [23:26<2:27:53,  5.84it/s[0.6407]]{'loss': 0.696, 'learning_rate': 4.316666666666667e-05, 'epoch': 0.41}\n",
            "Training:  14% 8300/60000 [23:43<2:27:28,  5.84it/s[0.696]]{'loss': 0.5128, 'learning_rate': 4.3083333333333335e-05, 'epoch': 0.41}\n",
            "Training:  14% 8400/60000 [24:00<2:27:06,  5.85it/s[0.5128]]{'loss': 0.8603, 'learning_rate': 4.3e-05, 'epoch': 0.42}\n",
            "Training:  14% 8500/60000 [24:18<2:26:55,  5.84it/s[0.8603]]{'loss': 1.0746, 'learning_rate': 4.291666666666667e-05, 'epoch': 0.42}\n",
            "Training:  14% 8600/60000 [24:35<2:26:58,  5.83it/s[1.0746]]{'loss': 1.5365, 'learning_rate': 4.2833333333333335e-05, 'epoch': 0.43}\n",
            "Training:  14% 8700/60000 [24:52<2:26:50,  5.82it/s[1.5365]]{'loss': 0.8434, 'learning_rate': 4.275e-05, 'epoch': 0.43}\n",
            "Training:  15% 8800/60000 [25:09<2:26:28,  5.83it/s[0.8434]]{'loss': 0.5959, 'learning_rate': 4.266666666666667e-05, 'epoch': 0.44}\n",
            "Training:  15% 8900/60000 [25:26<2:26:21,  5.82it/s[0.5959]]{'loss': 1.0329, 'learning_rate': 4.2583333333333336e-05, 'epoch': 0.45}\n",
            "Training:  15% 9000/60000 [25:44<2:26:04,  5.82it/s[1.0329]]{'loss': 0.9748, 'learning_rate': 4.25e-05, 'epoch': 0.45}\n",
            "Training:  15% 9100/60000 [26:01<2:25:35,  5.83it/s[0.9748]]{'loss': 1.1766, 'learning_rate': 4.241666666666667e-05, 'epoch': 0.46}\n",
            "Training:  15% 9200/60000 [26:18<2:25:16,  5.83it/s[1.1766]]{'loss': 0.7562, 'learning_rate': 4.233333333333334e-05, 'epoch': 0.46}\n",
            "Training:  16% 9300/60000 [26:35<2:25:00,  5.83it/s[0.7562]]{'loss': 0.5474, 'learning_rate': 4.2250000000000004e-05, 'epoch': 0.47}\n",
            "Training:  16% 9400/60000 [26:52<2:24:38,  5.83it/s[0.5474]]{'loss': 1.3924, 'learning_rate': 4.216666666666667e-05, 'epoch': 0.47}\n",
            "Training:  16% 9500/60000 [27:09<2:24:15,  5.83it/s[1.3924]]{'loss': 0.8966, 'learning_rate': 4.208333333333334e-05, 'epoch': 0.47}\n",
            "Training:  16% 9600/60000 [27:26<2:24:01,  5.83it/s[0.8966]]{'loss': 0.638, 'learning_rate': 4.2e-05, 'epoch': 0.48}\n",
            "Training:  16% 9700/60000 [27:44<2:23:55,  5.82it/s[0.638]]{'loss': 0.6493, 'learning_rate': 4.191666666666667e-05, 'epoch': 0.48}\n",
            "Training:  16% 9800/60000 [28:01<2:23:40,  5.82it/s[0.6493]]{'loss': 0.827, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.49}\n",
            "Training:  16% 9900/60000 [28:18<2:23:16,  5.83it/s[0.827]] {'loss': 0.409, 'learning_rate': 4.175e-05, 'epoch': 0.49}\n",
            "Training:  17% 10000/60000 [28:35<2:23:06,  5.82it/s[0.409]]{'loss': 0.869, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
            "Training:  17% 10100/60000 [28:52<2:22:56,  5.82it/s[0.869]]{'loss': 0.8706, 'learning_rate': 4.158333333333333e-05, 'epoch': 0.51}\n",
            "Training:  17% 10200/60000 [29:10<2:22:42,  5.82it/s[0.8706]]{'loss': 1.1007, 'learning_rate': 4.15e-05, 'epoch': 0.51}\n",
            "Training:  17% 10300/60000 [29:27<2:22:36,  5.81it/s[1.1007]]{'loss': 0.5222, 'learning_rate': 4.141666666666667e-05, 'epoch': 0.52}\n",
            "Training:  17% 10400/60000 [29:44<2:22:19,  5.81it/s[0.5222]]{'loss': 1.1526, 'learning_rate': 4.133333333333333e-05, 'epoch': 0.52}\n",
            "Training:  18% 10500/60000 [30:01<2:21:51,  5.82it/s[1.1526]]{'loss': 0.9673, 'learning_rate': 4.125e-05, 'epoch': 0.53}\n",
            "Training:  18% 10600/60000 [30:18<2:21:38,  5.81it/s[0.9673]]{'loss': 1.2056, 'learning_rate': 4.116666666666667e-05, 'epoch': 0.53}\n",
            "Training:  18% 10700/60000 [30:36<2:21:18,  5.81it/s[1.2056]]{'loss': 0.5879, 'learning_rate': 4.1083333333333334e-05, 'epoch': 0.54}\n",
            "Training:  18% 10800/60000 [30:53<2:21:08,  5.81it/s[0.5879]]{'loss': 0.5604, 'learning_rate': 4.1e-05, 'epoch': 0.54}\n",
            "Training:  18% 10900/60000 [31:10<2:20:44,  5.81it/s[0.5604]]{'loss': 0.4255, 'learning_rate': 4.091666666666667e-05, 'epoch': 0.55}\n",
            "Training:  18% 11000/60000 [31:27<2:20:22,  5.82it/s[0.4255]]{'loss': 1.005, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.55}\n",
            "Training:  18% 11100/60000 [31:44<2:20:04,  5.82it/s[1.005]]{'loss': 0.6959, 'learning_rate': 4.075e-05, 'epoch': 0.56}\n",
            "Training:  19% 11200/60000 [32:02<2:19:51,  5.82it/s[0.6959]]{'loss': 1.3532, 'learning_rate': 4.066666666666667e-05, 'epoch': 0.56}\n",
            "Training:  19% 11300/60000 [32:19<2:19:35,  5.81it/s[1.3532]]{'loss': 0.6579, 'learning_rate': 4.0583333333333335e-05, 'epoch': 0.56}\n",
            "Training:  19% 11400/60000 [32:36<2:19:19,  5.81it/s[0.6579]]{'loss': 0.8746, 'learning_rate': 4.05e-05, 'epoch': 0.57}\n",
            "Training:  19% 11500/60000 [32:53<2:18:51,  5.82it/s[0.8746]]{'loss': 0.8284, 'learning_rate': 4.041666666666667e-05, 'epoch': 0.57}\n",
            "Training:  19% 11600/60000 [33:10<2:18:28,  5.83it/s[0.8284]]{'loss': 0.5866, 'learning_rate': 4.0333333333333336e-05, 'epoch': 0.58}\n",
            "Training:  20% 11700/60000 [33:27<2:18:02,  5.83it/s[0.5866]]{'loss': 1.0959, 'learning_rate': 4.025e-05, 'epoch': 0.58}\n",
            "Training:  20% 11800/60000 [33:45<2:17:51,  5.83it/s[1.0959]]{'loss': 0.9951, 'learning_rate': 4.016666666666667e-05, 'epoch': 0.59}\n",
            "Training:  20% 11900/60000 [34:02<2:17:31,  5.83it/s[0.9951]]{'loss': 1.1713, 'learning_rate': 4.0083333333333336e-05, 'epoch': 0.59}\n",
            "Training:  20% 12000/60000 [34:19<2:17:09,  5.83it/s[1.1713]]{'loss': 1.2682, 'learning_rate': 4e-05, 'epoch': 0.6}\n",
            "Training:  20% 12100/60000 [34:36<2:16:56,  5.83it/s[1.2682]]{'loss': 0.8518, 'learning_rate': 3.991666666666667e-05, 'epoch': 0.6}\n",
            "Training:  20% 12200/60000 [34:53<2:16:45,  5.83it/s[0.8518]]{'loss': 0.9197, 'learning_rate': 3.983333333333333e-05, 'epoch': 0.61}\n",
            "Training:  20% 12300/60000 [35:10<2:16:36,  5.82it/s[0.9197]]{'loss': 0.5318, 'learning_rate': 3.9750000000000004e-05, 'epoch': 0.61}\n",
            "Training:  21% 12400/60000 [35:28<2:16:32,  5.81it/s[0.5318]]{'loss': 0.4932, 'learning_rate': 3.966666666666667e-05, 'epoch': 0.62}\n",
            "Training:  21% 12500/60000 [35:45<2:16:16,  5.81it/s[0.4932]]{'loss': 1.1686, 'learning_rate': 3.958333333333333e-05, 'epoch': 0.62}\n",
            "Training:  21% 12600/60000 [36:02<2:15:57,  5.81it/s[1.1686]]{'loss': 0.419, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.63}\n",
            "Training:  21% 12700/60000 [36:19<2:15:32,  5.82it/s[0.419]] {'loss': 0.8947, 'learning_rate': 3.941666666666667e-05, 'epoch': 0.64}\n",
            "Training:  21% 12800/60000 [36:37<2:15:19,  5.81it/s[0.8947]]{'loss': 0.6651, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.64}\n",
            "Training:  22% 12900/60000 [36:54<2:14:56,  5.82it/s[0.6651]]{'loss': 0.9711, 'learning_rate': 3.9250000000000005e-05, 'epoch': 0.65}\n",
            "Training:  22% 13000/60000 [37:11<2:14:34,  5.82it/s[0.9711]]{'loss': 1.015, 'learning_rate': 3.9166666666666665e-05, 'epoch': 0.65}\n",
            "Training:  22% 13100/60000 [37:28<2:14:14,  5.82it/s[1.015]] {'loss': 0.8075, 'learning_rate': 3.908333333333333e-05, 'epoch': 0.66}\n",
            "Training:  22% 13200/60000 [37:45<2:14:07,  5.82it/s[0.8075]]{'loss': 0.7178, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.66}\n",
            "Training:  22% 13300/60000 [38:02<2:13:48,  5.82it/s[0.7178]]{'loss': 0.751, 'learning_rate': 3.8916666666666666e-05, 'epoch': 0.67}\n",
            "Training:  22% 13400/60000 [38:20<2:13:25,  5.82it/s[0.751]]{'loss': 0.7175, 'learning_rate': 3.883333333333333e-05, 'epoch': 0.67}\n",
            "Training:  22% 13500/60000 [38:37<2:13:06,  5.82it/s[0.7175]]{'loss': 0.9704, 'learning_rate': 3.875e-05, 'epoch': 0.68}\n",
            "Training:  23% 13600/60000 [38:54<2:13:03,  5.81it/s[0.9704]]{'loss': 0.7206, 'learning_rate': 3.866666666666667e-05, 'epoch': 0.68}\n",
            "Training:  23% 13700/60000 [39:11<2:12:38,  5.82it/s[0.7206]]{'loss': 0.8437, 'learning_rate': 3.8583333333333334e-05, 'epoch': 0.69}\n",
            "Training:  23% 13800/60000 [39:28<2:12:32,  5.81it/s[0.8437]]{'loss': 0.7885, 'learning_rate': 3.85e-05, 'epoch': 0.69}\n",
            "Training:  23% 13900/60000 [39:46<2:12:19,  5.81it/s[0.7885]]{'loss': 0.6191, 'learning_rate': 3.841666666666667e-05, 'epoch': 0.69}\n",
            "Training:  23% 14000/60000 [40:03<2:11:59,  5.81it/s[0.6191]]{'loss': 0.9255, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.7}\n",
            "Training:  24% 14100/60000 [40:20<2:11:43,  5.81it/s[0.9255]]{'loss': 0.9632, 'learning_rate': 3.825e-05, 'epoch': 0.7}\n",
            "Training:  24% 14200/60000 [40:37<2:11:30,  5.80it/s[0.9632]]{'loss': 0.6192, 'learning_rate': 3.816666666666667e-05, 'epoch': 0.71}\n",
            "Training:  24% 14300/60000 [40:55<2:11:26,  5.79it/s[0.6192]]{'loss': 0.7614, 'learning_rate': 3.8083333333333335e-05, 'epoch': 0.71}\n",
            "Training:  24% 14400/60000 [41:12<2:11:14,  5.79it/s[0.7614]]{'loss': 0.8662, 'learning_rate': 3.8e-05, 'epoch': 0.72}\n",
            "Training:  24% 14500/60000 [41:29<2:11:01,  5.79it/s[0.8662]]{'loss': 0.7856, 'learning_rate': 3.791666666666667e-05, 'epoch': 0.72}\n",
            "Training:  24% 14600/60000 [41:47<2:10:48,  5.78it/s[0.7856]]{'loss': 1.1901, 'learning_rate': 3.7833333333333336e-05, 'epoch': 0.73}\n",
            "Training:  24% 14700/60000 [42:04<2:10:12,  5.80it/s[1.1901]]{'loss': 0.5968, 'learning_rate': 3.775e-05, 'epoch': 0.73}\n",
            "Training:  25% 14800/60000 [42:21<2:09:46,  5.80it/s[0.5968]]{'loss': 0.6849, 'learning_rate': 3.766666666666667e-05, 'epoch': 0.74}\n",
            "Training:  25% 14900/60000 [42:38<2:09:26,  5.81it/s[0.6849]]{'loss': 0.9006, 'learning_rate': 3.7583333333333337e-05, 'epoch': 0.74}\n",
            "Training:  25% 15000/60000 [42:55<2:09:04,  5.81it/s[0.9006]]{'loss': 0.8144, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n",
            "Training:  25% 15100/60000 [43:12<2:08:40,  5.82it/s[0.8144]]{'loss': 0.6749, 'learning_rate': 3.7416666666666664e-05, 'epoch': 0.76}\n",
            "Training:  25% 15200/60000 [43:30<2:08:25,  5.81it/s[0.6749]]{'loss': 0.7578, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.76}\n",
            "Training:  26% 15300/60000 [43:47<2:08:08,  5.81it/s[0.7578]]{'loss': 0.6204, 'learning_rate': 3.7250000000000004e-05, 'epoch': 0.77}\n",
            "Training:  26% 15400/60000 [44:04<2:08:03,  5.80it/s[0.6204]]{'loss': 0.5355, 'learning_rate': 3.7166666666666664e-05, 'epoch': 0.77}\n",
            "Training:  26% 15500/60000 [44:21<2:07:50,  5.80it/s[0.5355]]{'loss': 0.9281, 'learning_rate': 3.708333333333334e-05, 'epoch': 0.78}\n",
            "Training:  26% 15600/60000 [44:39<2:07:31,  5.80it/s[0.9281]]{'loss': 1.0218, 'learning_rate': 3.7e-05, 'epoch': 0.78}\n",
            "Training:  26% 15700/60000 [44:56<2:07:25,  5.79it/s[1.0218]]{'loss': 1.2669, 'learning_rate': 3.6916666666666665e-05, 'epoch': 0.79}\n",
            "Training:  26% 15800/60000 [45:13<2:07:20,  5.79it/s[1.2669]]{'loss': 1.1295, 'learning_rate': 3.683333333333334e-05, 'epoch': 0.79}\n",
            "Training:  26% 15900/60000 [45:31<2:07:43,  5.75it/s[1.1295]]{'loss': 1.1684, 'learning_rate': 3.675e-05, 'epoch': 0.8}\n",
            "Training:  27% 16000/60000 [45:48<2:07:21,  5.76it/s[1.1684]]{'loss': 0.5063, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n",
            "Training:  27% 16100/60000 [46:06<2:07:00,  5.76it/s[0.5063]]{'loss': 0.9546, 'learning_rate': 3.658333333333334e-05, 'epoch': 0.81}\n",
            "Training:  27% 16200/60000 [46:23<2:06:39,  5.76it/s[0.9546]]{'loss': 0.6809, 'learning_rate': 3.65e-05, 'epoch': 0.81}\n",
            "Training:  27% 16300/60000 [46:40<2:06:31,  5.76it/s[0.6809]]{'loss': 0.8077, 'learning_rate': 3.641666666666667e-05, 'epoch': 0.81}\n",
            "Training:  27% 16400/60000 [46:58<2:06:05,  5.76it/s[0.8077]]{'loss': 0.8556, 'learning_rate': 3.633333333333333e-05, 'epoch': 0.82}\n",
            "Training:  28% 16500/60000 [47:15<2:06:03,  5.75it/s[0.8556]]{'loss': 1.2214, 'learning_rate': 3.625e-05, 'epoch': 0.82}\n",
            "Training:  28% 16600/60000 [47:32<2:05:38,  5.76it/s[1.2214]]{'loss': 0.8602, 'learning_rate': 3.6166666666666674e-05, 'epoch': 0.83}\n",
            "Training:  28% 16700/60000 [47:50<2:05:16,  5.76it/s[0.8602]]{'loss': 0.7724, 'learning_rate': 3.6083333333333334e-05, 'epoch': 0.83}\n",
            "Training:  28% 16800/60000 [48:07<2:05:00,  5.76it/s[0.7724]]{'loss': 0.7143, 'learning_rate': 3.6e-05, 'epoch': 0.84}\n",
            "Training:  28% 16900/60000 [48:24<2:04:24,  5.77it/s[0.7143]]{'loss': 0.9002, 'learning_rate': 3.591666666666667e-05, 'epoch': 0.84}\n",
            "Training:  28% 17000/60000 [48:42<2:03:54,  5.78it/s[0.9002]]{'loss': 1.1084, 'learning_rate': 3.5833333333333335e-05, 'epoch': 0.85}\n",
            "Training:  28% 17100/60000 [48:59<2:03:33,  5.79it/s[1.1084]]{'loss': 0.7964, 'learning_rate': 3.575e-05, 'epoch': 0.85}\n",
            "Training:  29% 17200/60000 [49:16<2:03:01,  5.80it/s[0.7964]]{'loss': 0.8583, 'learning_rate': 3.566666666666667e-05, 'epoch': 0.86}\n",
            "Training:  29% 17300/60000 [49:33<2:02:43,  5.80it/s[0.8583]]{'loss': 0.6225, 'learning_rate': 3.5583333333333335e-05, 'epoch': 0.86}\n",
            "Training:  29% 17400/60000 [49:50<2:02:17,  5.81it/s[0.6225]]{'loss': 0.6394, 'learning_rate': 3.55e-05, 'epoch': 0.87}\n",
            "Training:  29% 17500/60000 [50:08<2:01:52,  5.81it/s[0.6394]]{'loss': 0.9299, 'learning_rate': 3.541666666666667e-05, 'epoch': 0.88}\n",
            "Training:  29% 17600/60000 [50:25<2:01:24,  5.82it/s[0.9299]]{'loss': 0.4647, 'learning_rate': 3.5333333333333336e-05, 'epoch': 0.88}\n",
            "Training:  30% 17700/60000 [50:42<2:00:58,  5.83it/s[0.4647]]{'loss': 0.7842, 'learning_rate': 3.525e-05, 'epoch': 0.89}\n",
            "Training:  30% 17800/60000 [50:59<2:00:50,  5.82it/s[0.7842]]{'loss': 0.9536, 'learning_rate': 3.516666666666667e-05, 'epoch': 0.89}\n",
            "Training:  30% 17900/60000 [51:16<2:00:41,  5.81it/s[0.9536]]{'loss': 0.6579, 'learning_rate': 3.508333333333334e-05, 'epoch': 0.9}\n",
            "Training:  30% 18000/60000 [51:34<2:00:30,  5.81it/s[0.6579]]{'loss': 1.3044, 'learning_rate': 3.5e-05, 'epoch': 0.9}\n",
            "Training:  30% 18100/60000 [51:51<2:00:29,  5.80it/s[1.3044]]{'loss': 0.7129, 'learning_rate': 3.491666666666667e-05, 'epoch': 0.91}\n",
            "Training:  30% 18200/60000 [52:08<2:00:10,  5.80it/s[0.7129]]{'loss': 0.8716, 'learning_rate': 3.483333333333334e-05, 'epoch': 0.91}\n",
            "Training:  30% 18300/60000 [52:25<1:59:56,  5.79it/s[0.8716]]{'loss': 1.1412, 'learning_rate': 3.475e-05, 'epoch': 0.92}\n",
            "Training:  31% 18400/60000 [52:43<1:59:35,  5.80it/s[1.1412]]{'loss': 0.7732, 'learning_rate': 3.466666666666667e-05, 'epoch': 0.92}\n",
            "Training:  31% 18500/60000 [53:00<1:59:18,  5.80it/s[0.7732]]{'loss': 1.0257, 'learning_rate': 3.458333333333333e-05, 'epoch': 0.93}\n",
            "Training:  31% 18600/60000 [53:17<1:58:59,  5.80it/s[1.0257]]{'loss': 0.8439, 'learning_rate': 3.45e-05, 'epoch': 0.93}\n",
            "Training:  31% 18700/60000 [53:34<1:58:37,  5.80it/s[0.8439]]{'loss': 0.7322, 'learning_rate': 3.441666666666667e-05, 'epoch': 0.94}\n",
            "Training:  31% 18800/60000 [53:51<1:58:07,  5.81it/s[0.7322]]{'loss': 0.7079, 'learning_rate': 3.433333333333333e-05, 'epoch': 0.94}\n",
            "Training:  32% 18900/60000 [54:09<1:57:55,  5.81it/s[0.7079]]{'loss': 0.7188, 'learning_rate': 3.4250000000000006e-05, 'epoch': 0.94}\n",
            "Training:  32% 19000/60000 [54:26<1:57:28,  5.82it/s[0.7188]]{'loss': 0.9903, 'learning_rate': 3.4166666666666666e-05, 'epoch': 0.95}\n",
            "Training:  32% 19100/60000 [54:43<1:57:21,  5.81it/s[0.9903]]{'loss': 0.8749, 'learning_rate': 3.408333333333333e-05, 'epoch': 0.95}\n",
            "Training:  32% 19200/60000 [55:00<1:57:07,  5.81it/s[0.8749]]{'loss': 0.6279, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.96}\n",
            "Training:  32% 19300/60000 [55:18<1:56:57,  5.80it/s[0.6279]]{'loss': 0.7355, 'learning_rate': 3.391666666666667e-05, 'epoch': 0.96}\n",
            "Training:  32% 19400/60000 [55:35<1:56:45,  5.80it/s[0.7355]]{'loss': 0.6799, 'learning_rate': 3.3833333333333334e-05, 'epoch': 0.97}\n",
            "Training:  32% 19500/60000 [55:52<1:56:35,  5.79it/s[0.6799]]{'loss': 0.6653, 'learning_rate': 3.375000000000001e-05, 'epoch': 0.97}\n",
            "Training:  33% 19600/60000 [56:10<1:56:23,  5.79it/s[0.6653]]{'loss': 0.954, 'learning_rate': 3.366666666666667e-05, 'epoch': 0.98}\n",
            "Training:  33% 19700/60000 [56:27<1:56:19,  5.77it/s[0.954]]{'loss': 0.5064, 'learning_rate': 3.3583333333333334e-05, 'epoch': 0.98}\n",
            "Training:  33% 19800/60000 [56:44<1:56:01,  5.77it/s[0.5064]]{'loss': 1.1409, 'learning_rate': 3.35e-05, 'epoch': 0.99}\n",
            "Training:  33% 19900/60000 [57:02<1:55:46,  5.77it/s[1.1409]]{'loss': 0.697, 'learning_rate': 3.341666666666667e-05, 'epoch': 0.99}\n",
            "Training:  33% 20000/60000 [57:19<1:55:25,  5.78it/s[0.697]] {'loss': 0.7912, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
            "Training:  33% 20000/60000 [57:30<1:55:25,  5.78it/s[0.697]]{'eval_loss': 0.6777912378311157, 'eval_accuracy': 0.8466, 'eval_runtime': 35.0591, 'eval_samples_per_second': 142.616, 'eval_steps_per_second': 17.827, 'epoch': 1.0}\n",
            "Training:  34% 20100/60000 [58:51<4:23:41,  2.52it/s[0.697]]{'loss': 0.4492, 'learning_rate': 3.325e-05, 'epoch': 1.0}\n",
            "Training:  34% 20200/60000 [59:08<3:38:28,  3.04it/s[0.4492]]{'loss': 0.4862, 'learning_rate': 3.316666666666667e-05, 'epoch': 1.01}\n",
            "Training:  34% 20300/60000 [59:25<3:06:43,  3.54it/s[0.4862]]{'loss': 0.4881, 'learning_rate': 3.3083333333333336e-05, 'epoch': 1.01}\n",
            "Training:  34% 20400/60000 [59:43<2:44:41,  4.01it/s[0.4881]]{'loss': 0.1306, 'learning_rate': 3.3e-05, 'epoch': 1.02}\n",
            "Training:  34% 20500/60000 [1:00:00<2:29:02,  4.42it/s[0.1306]]{'loss': 0.3259, 'learning_rate': 3.291666666666667e-05, 'epoch': 1.02}\n",
            "Training:  34% 20600/60000 [1:00:17<2:18:03,  4.76it/s[0.3259]]{'loss': 0.1985, 'learning_rate': 3.283333333333333e-05, 'epoch': 1.03}\n",
            "Training:  34% 20700/60000 [1:00:34<2:10:18,  5.03it/s[0.1985]]{'loss': 0.5095, 'learning_rate': 3.275e-05, 'epoch': 1.03}\n",
            "Training:  35% 20800/60000 [1:00:52<2:04:52,  5.23it/s[0.5095]]{'loss': 0.3251, 'learning_rate': 3.266666666666667e-05, 'epoch': 1.04}\n",
            "Training:  35% 20900/60000 [1:01:09<2:00:45,  5.40it/s[0.3251]]{'loss': 0.4294, 'learning_rate': 3.258333333333333e-05, 'epoch': 1.04}\n",
            "Training:  35% 21000/60000 [1:01:26<1:57:57,  5.51it/s[0.4294]]{'loss': 0.344, 'learning_rate': 3.2500000000000004e-05, 'epoch': 1.05}\n",
            "Training:  35% 21100/60000 [1:01:43<1:55:49,  5.60it/s[0.344]]{'loss': 0.5248, 'learning_rate': 3.2416666666666664e-05, 'epoch': 1.05}\n",
            "Training:  35% 21200/60000 [1:02:01<1:54:43,  5.64it/s[0.5248]]{'loss': 0.3149, 'learning_rate': 3.233333333333333e-05, 'epoch': 1.06}\n",
            "Training:  36% 21300/60000 [1:02:18<1:53:30,  5.68it/s[0.3149]]{'loss': 0.4967, 'learning_rate': 3.2250000000000005e-05, 'epoch': 1.06}\n",
            "Training:  36% 21400/60000 [1:02:35<1:52:12,  5.73it/s[0.4967]]{'loss': 0.335, 'learning_rate': 3.2166666666666665e-05, 'epoch': 1.07}\n",
            "Training:  36% 21500/60000 [1:02:52<1:51:23,  5.76it/s[0.335]]{'loss': 0.3702, 'learning_rate': 3.208333333333334e-05, 'epoch': 1.07}\n",
            "Training:  36% 21600/60000 [1:03:09<1:50:50,  5.77it/s[0.3702]]{'loss': 0.3524, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.08}\n",
            "Training:  36% 21700/60000 [1:03:27<1:50:10,  5.79it/s[0.3524]]{'loss': 0.3296, 'learning_rate': 3.1916666666666665e-05, 'epoch': 1.08}\n",
            "Training:  36% 21800/60000 [1:03:44<1:50:00,  5.79it/s[0.3296]]{'loss': 0.3811, 'learning_rate': 3.183333333333334e-05, 'epoch': 1.09}\n",
            "Training:  36% 21900/60000 [1:04:01<1:49:37,  5.79it/s[0.3811]]{'loss': 0.749, 'learning_rate': 3.175e-05, 'epoch': 1.09}\n",
            "Training:  37% 22000/60000 [1:04:18<1:49:21,  5.79it/s[0.749]]{'loss': 0.23, 'learning_rate': 3.1666666666666666e-05, 'epoch': 1.1}\n",
            "Training:  37% 22100/60000 [1:04:36<1:49:07,  5.79it/s[0.23]]{'loss': 0.3668, 'learning_rate': 3.158333333333334e-05, 'epoch': 1.1}\n",
            "Training:  37% 22200/60000 [1:04:53<1:48:35,  5.80it/s[0.3668]]{'loss': 0.4818, 'learning_rate': 3.15e-05, 'epoch': 1.11}\n",
            "Training:  37% 22300/60000 [1:05:10<1:48:15,  5.80it/s[0.4818]]{'loss': 0.1477, 'learning_rate': 3.141666666666667e-05, 'epoch': 1.11}\n",
            "Training:  37% 22400/60000 [1:05:27<1:48:03,  5.80it/s[0.1477]]{'loss': 0.2325, 'learning_rate': 3.1333333333333334e-05, 'epoch': 1.12}\n",
            "Training:  38% 22500/60000 [1:05:45<1:47:54,  5.79it/s[0.2325]]{'loss': 0.1998, 'learning_rate': 3.125e-05, 'epoch': 1.12}\n",
            "Training:  38% 22600/60000 [1:06:02<1:47:31,  5.80it/s[0.1998]]{'loss': 0.1959, 'learning_rate': 3.116666666666667e-05, 'epoch': 1.13}\n",
            "Training:  38% 22700/60000 [1:06:19<1:47:05,  5.80it/s[0.1959]]{'loss': 0.2643, 'learning_rate': 3.1083333333333334e-05, 'epoch': 1.14}\n",
            "Training:  38% 22800/60000 [1:06:36<1:46:45,  5.81it/s[0.2643]]{'loss': 0.5582, 'learning_rate': 3.1e-05, 'epoch': 1.14}\n",
            "Training:  38% 22900/60000 [1:06:53<1:46:31,  5.80it/s[0.5582]]{'loss': 0.4477, 'learning_rate': 3.091666666666667e-05, 'epoch': 1.15}\n",
            "Training:  38% 23000/60000 [1:07:11<1:46:13,  5.81it/s[0.4477]]{'loss': 0.3341, 'learning_rate': 3.0833333333333335e-05, 'epoch': 1.15}\n",
            "Training:  38% 23100/60000 [1:07:28<1:45:47,  5.81it/s[0.3341]]{'loss': 0.3957, 'learning_rate': 3.075e-05, 'epoch': 1.16}\n",
            "Training:  39% 23200/60000 [1:07:45<1:45:29,  5.81it/s[0.3957]]{'loss': 0.306, 'learning_rate': 3.066666666666667e-05, 'epoch': 1.16}\n",
            "Training:  39% 23300/60000 [1:08:02<1:45:04,  5.82it/s[0.306]]{'loss': 0.2899, 'learning_rate': 3.0583333333333336e-05, 'epoch': 1.17}\n",
            "Training:  39% 23400/60000 [1:08:19<1:44:49,  5.82it/s[0.2899]]{'loss': 0.8829, 'learning_rate': 3.05e-05, 'epoch': 1.17}\n",
            "Training:  39% 23500/60000 [1:08:36<1:44:21,  5.83it/s[0.8829]]{'loss': 0.3112, 'learning_rate': 3.0416666666666666e-05, 'epoch': 1.18}\n",
            "Training:  39% 23600/60000 [1:08:53<1:43:56,  5.84it/s[0.3112]]{'loss': 0.6363, 'learning_rate': 3.0333333333333337e-05, 'epoch': 1.18}\n",
            "Training:  40% 23700/60000 [1:09:11<1:43:35,  5.84it/s[0.6363]]{'loss': 0.6356, 'learning_rate': 3.025e-05, 'epoch': 1.19}\n",
            "Training:  40% 23800/60000 [1:09:28<1:43:16,  5.84it/s[0.6356]]{'loss': 0.4202, 'learning_rate': 3.016666666666667e-05, 'epoch': 1.19}\n",
            "Training:  40% 23900/60000 [1:09:45<1:43:07,  5.83it/s[0.4202]]{'loss': 0.2595, 'learning_rate': 3.0083333333333337e-05, 'epoch': 1.2}\n",
            "Training:  40% 24000/60000 [1:10:02<1:43:02,  5.82it/s[0.2595]]{'loss': 0.2502, 'learning_rate': 3e-05, 'epoch': 1.2}\n",
            "Training:  40% 24100/60000 [1:10:19<1:42:41,  5.83it/s[0.2502]]{'loss': 0.353, 'learning_rate': 2.991666666666667e-05, 'epoch': 1.21}\n",
            "Training:  40% 24200/60000 [1:10:36<1:42:16,  5.83it/s[0.353]]{'loss': 0.3948, 'learning_rate': 2.9833333333333335e-05, 'epoch': 1.21}\n",
            "Training:  40% 24300/60000 [1:10:53<1:42:00,  5.83it/s[0.3948]]{'loss': 0.2959, 'learning_rate': 2.975e-05, 'epoch': 1.22}\n",
            "Training:  41% 24400/60000 [1:11:11<1:41:37,  5.84it/s[0.2959]]{'loss': 0.2869, 'learning_rate': 2.9666666666666672e-05, 'epoch': 1.22}\n",
            "Training:  41% 24500/60000 [1:11:28<1:41:16,  5.84it/s[0.2869]]{'loss': 0.696, 'learning_rate': 2.9583333333333335e-05, 'epoch': 1.23}\n",
            "Training:  41% 24600/60000 [1:11:45<1:41:03,  5.84it/s[0.696]]{'loss': 0.479, 'learning_rate': 2.95e-05, 'epoch': 1.23}\n",
            "Training:  41% 24700/60000 [1:12:02<1:40:51,  5.83it/s[0.479]]{'loss': 0.3675, 'learning_rate': 2.941666666666667e-05, 'epoch': 1.23}\n",
            "Training:  41% 24800/60000 [1:12:19<1:40:44,  5.82it/s[0.3675]]{'loss': 0.224, 'learning_rate': 2.9333333333333336e-05, 'epoch': 1.24}\n",
            "Training:  42% 24900/60000 [1:12:36<1:40:34,  5.82it/s[0.224]]{'loss': 0.2782, 'learning_rate': 2.925e-05, 'epoch': 1.25}\n",
            "Training:  42% 25000/60000 [1:12:54<1:40:19,  5.81it/s[0.2782]]{'loss': 0.3995, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n",
            "Training:  42% 25100/60000 [1:13:11<1:39:56,  5.82it/s[0.3995]]{'loss': 0.1446, 'learning_rate': 2.9083333333333333e-05, 'epoch': 1.25}\n",
            "Training:  42% 25200/60000 [1:13:28<1:39:51,  5.81it/s[0.1446]]{'loss': 0.348, 'learning_rate': 2.9e-05, 'epoch': 1.26}\n",
            "Training:  42% 25300/60000 [1:13:45<1:39:30,  5.81it/s[0.348]]{'loss': 0.4896, 'learning_rate': 2.891666666666667e-05, 'epoch': 1.27}\n",
            "Training:  42% 25400/60000 [1:14:03<1:39:15,  5.81it/s[0.4896]]{'loss': 0.375, 'learning_rate': 2.8833333333333334e-05, 'epoch': 1.27}\n",
            "Training:  42% 25500/60000 [1:14:20<1:38:46,  5.82it/s[0.375]] {'loss': 0.392, 'learning_rate': 2.8749999999999997e-05, 'epoch': 1.27}\n",
            "Training:  43% 25600/60000 [1:14:37<1:38:24,  5.83it/s[0.392]]{'loss': 0.3183, 'learning_rate': 2.8666666666666668e-05, 'epoch': 1.28}\n",
            "Training:  43% 25700/60000 [1:14:54<1:38:11,  5.82it/s[0.3183]]{'loss': 0.1165, 'learning_rate': 2.8583333333333335e-05, 'epoch': 1.28}\n",
            "Training:  43% 25800/60000 [1:15:11<1:38:04,  5.81it/s[0.1165]]{'loss': 0.3942, 'learning_rate': 2.8499999999999998e-05, 'epoch': 1.29}\n",
            "Training:  43% 25900/60000 [1:15:28<1:37:45,  5.81it/s[0.3942]]{'loss': 0.6829, 'learning_rate': 2.841666666666667e-05, 'epoch': 1.29}\n",
            "Training:  43% 26000/60000 [1:15:46<1:37:24,  5.82it/s[0.6829]]{'loss': 0.4486, 'learning_rate': 2.8333333333333335e-05, 'epoch': 1.3}\n",
            "Training:  44% 26100/60000 [1:16:03<1:36:57,  5.83it/s[0.4486]]{'loss': 0.2748, 'learning_rate': 2.825e-05, 'epoch': 1.3}\n",
            "Training:  44% 26200/60000 [1:16:20<1:36:36,  5.83it/s[0.2748]]{'loss': 0.5558, 'learning_rate': 2.816666666666667e-05, 'epoch': 1.31}\n",
            "Training:  44% 26300/60000 [1:16:37<1:36:12,  5.84it/s[0.5558]]{'loss': 0.4288, 'learning_rate': 2.8083333333333333e-05, 'epoch': 1.31}\n",
            "Training:  44% 26400/60000 [1:16:54<1:35:59,  5.83it/s[0.4288]]{'loss': 0.3526, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.32}\n",
            "Training:  44% 26500/60000 [1:17:11<1:35:50,  5.83it/s[0.3526]]{'loss': 0.697, 'learning_rate': 2.791666666666667e-05, 'epoch': 1.32}\n",
            "Training:  44% 26600/60000 [1:17:28<1:35:32,  5.83it/s[0.697]] {'loss': 0.3879, 'learning_rate': 2.7833333333333333e-05, 'epoch': 1.33}\n",
            "Training:  44% 26700/60000 [1:17:46<1:35:08,  5.83it/s[0.3879]]{'loss': 0.2057, 'learning_rate': 2.7750000000000004e-05, 'epoch': 1.33}\n",
            "Training:  45% 26800/60000 [1:18:03<1:34:52,  5.83it/s[0.2057]]{'loss': 0.3515, 'learning_rate': 2.7666666666666667e-05, 'epoch': 1.34}\n",
            "Training:  45% 26900/60000 [1:18:20<1:34:34,  5.83it/s[0.3515]]{'loss': 0.1871, 'learning_rate': 2.7583333333333334e-05, 'epoch': 1.34}\n",
            "Training:  45% 27000/60000 [1:18:37<1:34:17,  5.83it/s[0.1871]]{'loss': 0.7849, 'learning_rate': 2.7500000000000004e-05, 'epoch': 1.35}\n",
            "Training:  45% 27100/60000 [1:18:54<1:34:02,  5.83it/s[0.7849]]{'loss': 0.1917, 'learning_rate': 2.7416666666666668e-05, 'epoch': 1.35}\n",
            "Training:  45% 27200/60000 [1:19:11<1:33:47,  5.83it/s[0.1917]]{'loss': 0.3538, 'learning_rate': 2.733333333333333e-05, 'epoch': 1.36}\n",
            "Training:  46% 27300/60000 [1:19:29<1:33:32,  5.83it/s[0.3538]]{'loss': 0.3426, 'learning_rate': 2.725e-05, 'epoch': 1.36}\n",
            "Training:  46% 27400/60000 [1:19:46<1:33:16,  5.83it/s[0.3426]]{'loss': 0.2884, 'learning_rate': 2.716666666666667e-05, 'epoch': 1.37}\n",
            "Training:  46% 27500/60000 [1:20:03<1:32:59,  5.83it/s[0.2884]]{'loss': 0.4734, 'learning_rate': 2.7083333333333332e-05, 'epoch': 1.38}\n",
            "Training:  46% 27600/60000 [1:20:20<1:32:46,  5.82it/s[0.4734]]{'loss': 0.4822, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.38}\n",
            "Training:  46% 27700/60000 [1:20:37<1:32:38,  5.81it/s[0.4822]]{'loss': 0.2902, 'learning_rate': 2.691666666666667e-05, 'epoch': 1.39}\n",
            "Training:  46% 27800/60000 [1:20:54<1:32:15,  5.82it/s[0.2902]]{'loss': 0.2836, 'learning_rate': 2.6833333333333333e-05, 'epoch': 1.39}\n",
            "Training:  46% 27900/60000 [1:21:12<1:32:00,  5.81it/s[0.2836]]{'loss': 0.3845, 'learning_rate': 2.6750000000000003e-05, 'epoch': 1.4}\n",
            "Training:  47% 28000/60000 [1:21:29<1:31:41,  5.82it/s[0.3845]]{'loss': 0.5585, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}\n",
            "Training:  47% 28100/60000 [1:21:46<1:31:26,  5.81it/s[0.5585]]{'loss': 0.4595, 'learning_rate': 2.6583333333333333e-05, 'epoch': 1.41}\n",
            "Training:  47% 28200/60000 [1:22:03<1:31:02,  5.82it/s[0.4595]]{'loss': 0.4195, 'learning_rate': 2.6500000000000004e-05, 'epoch': 1.41}\n",
            "Training:  47% 28300/60000 [1:22:20<1:30:37,  5.83it/s[0.4195]]{'loss': 0.3573, 'learning_rate': 2.6416666666666667e-05, 'epoch': 1.42}\n",
            "Training:  47% 28400/60000 [1:22:37<1:30:22,  5.83it/s[0.3573]]{'loss': 0.445, 'learning_rate': 2.633333333333333e-05, 'epoch': 1.42}\n",
            "Training:  48% 28500/60000 [1:22:55<1:30:05,  5.83it/s[0.445]]{'loss': 0.5781, 'learning_rate': 2.625e-05, 'epoch': 1.43}\n",
            "Training:  48% 28600/60000 [1:23:12<1:29:51,  5.82it/s[0.5781]]{'loss': 0.548, 'learning_rate': 2.6166666666666668e-05, 'epoch': 1.43}\n",
            "Training:  48% 28700/60000 [1:23:29<1:29:36,  5.82it/s[0.548]] {'loss': 0.4761, 'learning_rate': 2.608333333333333e-05, 'epoch': 1.44}\n",
            "Training:  48% 28800/60000 [1:23:46<1:29:19,  5.82it/s[0.4761]]{'loss': 0.6696, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.44}\n",
            "Training:  48% 28900/60000 [1:24:03<1:28:57,  5.83it/s[0.6696]]{'loss': 0.1583, 'learning_rate': 2.5916666666666665e-05, 'epoch': 1.45}\n",
            "Training:  48% 29000/60000 [1:24:20<1:28:39,  5.83it/s[0.1583]]{'loss': 0.4984, 'learning_rate': 2.5833333333333336e-05, 'epoch': 1.45}\n",
            "Training:  48% 29100/60000 [1:24:38<1:28:27,  5.82it/s[0.4984]]{'loss': 0.4825, 'learning_rate': 2.5750000000000002e-05, 'epoch': 1.46}\n",
            "Training:  49% 29200/60000 [1:24:55<1:28:07,  5.83it/s[0.4825]]{'loss': 0.4422, 'learning_rate': 2.5666666666666666e-05, 'epoch': 1.46}\n",
            "Training:  49% 29300/60000 [1:25:12<1:27:49,  5.83it/s[0.4422]]{'loss': 0.4075, 'learning_rate': 2.5583333333333336e-05, 'epoch': 1.47}\n",
            "Training:  49% 29400/60000 [1:25:29<1:27:32,  5.83it/s[0.4075]]{'loss': 0.6237, 'learning_rate': 2.5500000000000003e-05, 'epoch': 1.47}\n",
            "Training:  49% 29500/60000 [1:25:46<1:27:15,  5.83it/s[0.6237]]{'loss': 0.4668, 'learning_rate': 2.5416666666666667e-05, 'epoch': 1.48}\n",
            "Training:  49% 29600/60000 [1:26:04<1:27:04,  5.82it/s[0.4668]]{'loss': 0.2528, 'learning_rate': 2.5333333333333337e-05, 'epoch': 1.48}\n",
            "Training:  50% 29700/60000 [1:26:21<1:26:43,  5.82it/s[0.2528]]{'loss': 0.4839, 'learning_rate': 2.525e-05, 'epoch': 1.48}\n",
            "Training:  50% 29800/60000 [1:26:38<1:26:29,  5.82it/s[0.4839]]{'loss': 0.6613, 'learning_rate': 2.5166666666666667e-05, 'epoch': 1.49}\n",
            "Training:  50% 29900/60000 [1:26:55<1:26:10,  5.82it/s[0.6613]]{'loss': 0.7247, 'learning_rate': 2.5083333333333338e-05, 'epoch': 1.5}\n",
            "Training:  50% 30000/60000 [1:27:12<1:26:02,  5.81it/s[0.7247]]{'loss': 0.4779, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n",
            "Training:  50% 30100/60000 [1:27:30<1:25:39,  5.82it/s[0.4779]]{'loss': 0.4745, 'learning_rate': 2.4916666666666668e-05, 'epoch': 1.5}\n",
            "Training:  50% 30200/60000 [1:27:47<1:25:15,  5.83it/s[0.4745]]{'loss': 0.9682, 'learning_rate': 2.4833333333333335e-05, 'epoch': 1.51}\n",
            "Training:  50% 30300/60000 [1:28:04<1:24:53,  5.83it/s[0.9682]]{'loss': 0.6158, 'learning_rate': 2.4750000000000002e-05, 'epoch': 1.52}\n",
            "Training:  51% 30400/60000 [1:28:21<1:24:29,  5.84it/s[0.6158]]{'loss': 0.3463, 'learning_rate': 2.466666666666667e-05, 'epoch': 1.52}\n",
            "Training:  51% 30500/60000 [1:28:38<1:24:13,  5.84it/s[0.3463]]{'loss': 0.476, 'learning_rate': 2.4583333333333332e-05, 'epoch': 1.52}\n",
            "Training:  51% 30600/60000 [1:28:55<1:24:03,  5.83it/s[0.476]]{'loss': 0.6352, 'learning_rate': 2.45e-05, 'epoch': 1.53}\n",
            "Training:  51% 30700/60000 [1:29:12<1:23:39,  5.84it/s[0.6352]]{'loss': 0.5242, 'learning_rate': 2.441666666666667e-05, 'epoch': 1.54}\n",
            "Training:  51% 30800/60000 [1:29:29<1:23:23,  5.84it/s[0.5242]]{'loss': 0.3758, 'learning_rate': 2.4333333333333336e-05, 'epoch': 1.54}\n",
            "Training:  52% 30900/60000 [1:29:46<1:23:02,  5.84it/s[0.3758]]{'loss': 0.4747, 'learning_rate': 2.425e-05, 'epoch': 1.54}\n",
            "Training:  52% 31000/60000 [1:30:04<1:22:43,  5.84it/s[0.4747]]{'loss': 0.6656, 'learning_rate': 2.4166666666666667e-05, 'epoch': 1.55}\n",
            "Training:  52% 31100/60000 [1:30:21<1:22:28,  5.84it/s[0.6656]]{'loss': 0.4365, 'learning_rate': 2.4083333333333337e-05, 'epoch': 1.56}\n",
            "Training:  52% 31200/60000 [1:30:38<1:22:13,  5.84it/s[0.4365]]{'loss': 0.2941, 'learning_rate': 2.4e-05, 'epoch': 1.56}\n",
            "Training:  52% 31300/60000 [1:30:55<1:21:54,  5.84it/s[0.2941]]{'loss': 0.3903, 'learning_rate': 2.3916666666666668e-05, 'epoch': 1.56}\n",
            "Training:  52% 31400/60000 [1:31:12<1:21:48,  5.83it/s[0.3903]]{'loss': 0.4463, 'learning_rate': 2.3833333333333334e-05, 'epoch': 1.57}\n",
            "Training:  52% 31500/60000 [1:31:30<1:21:46,  5.81it/s[0.4463]]{'loss': 0.4931, 'learning_rate': 2.375e-05, 'epoch': 1.57}\n",
            "Training:  53% 31600/60000 [1:31:47<1:21:37,  5.80it/s[0.4931]]{'loss': 0.1775, 'learning_rate': 2.3666666666666668e-05, 'epoch': 1.58}\n",
            "Training:  53% 31700/60000 [1:32:05<1:22:09,  5.74it/s[0.1775]]{'loss': 0.2166, 'learning_rate': 2.3583333333333335e-05, 'epoch': 1.58}\n",
            "Training:  53% 31800/60000 [1:32:23<1:22:38,  5.69it/s[0.2166]]{'loss': 0.2362, 'learning_rate': 2.35e-05, 'epoch': 1.59}\n",
            "Training:  53% 31900/60000 [1:32:41<1:22:47,  5.66it/s[0.2362]]{'loss': 0.4349, 'learning_rate': 2.341666666666667e-05, 'epoch': 1.59}\n",
            "Training:  53% 32000/60000 [1:32:58<1:22:06,  5.68it/s[0.4349]]{'loss': 0.3062, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n",
            "Training:  54% 32100/60000 [1:33:15<1:21:13,  5.72it/s[0.3062]]{'loss': 0.5808, 'learning_rate': 2.3250000000000003e-05, 'epoch': 1.6}\n",
            "Training:  54% 32200/60000 [1:33:32<1:20:29,  5.76it/s[0.5808]]{'loss': 0.5972, 'learning_rate': 2.3166666666666666e-05, 'epoch': 1.61}\n",
            "Training:  54% 32300/60000 [1:33:49<1:19:52,  5.78it/s[0.5972]]{'loss': 0.2981, 'learning_rate': 2.3083333333333333e-05, 'epoch': 1.61}\n",
            "Training:  54% 32400/60000 [1:34:07<1:19:30,  5.79it/s[0.2981]]{'loss': 0.3605, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.62}\n",
            "Training:  54% 32500/60000 [1:34:24<1:19:04,  5.80it/s[0.3605]]{'loss': 0.16, 'learning_rate': 2.2916666666666667e-05, 'epoch': 1.62}\n",
            "Training:  54% 32600/60000 [1:34:41<1:18:45,  5.80it/s[0.16]]{'loss': 0.1929, 'learning_rate': 2.2833333333333334e-05, 'epoch': 1.63}\n",
            "Training:  55% 32700/60000 [1:34:58<1:18:20,  5.81it/s[0.1929]]{'loss': 0.4324, 'learning_rate': 2.275e-05, 'epoch': 1.64}\n",
            "Training:  55% 32800/60000 [1:35:15<1:17:58,  5.81it/s[0.4324]]{'loss': 0.1686, 'learning_rate': 2.2666666666666668e-05, 'epoch': 1.64}\n",
            "Training:  55% 32900/60000 [1:35:33<1:17:46,  5.81it/s[0.1686]]{'loss': 0.4249, 'learning_rate': 2.2583333333333335e-05, 'epoch': 1.65}\n",
            "Training:  55% 33000/60000 [1:35:50<1:17:24,  5.81it/s[0.4249]]{'loss': 0.6065, 'learning_rate': 2.25e-05, 'epoch': 1.65}\n",
            "Training:  55% 33100/60000 [1:36:07<1:16:59,  5.82it/s[0.6065]]{'loss': 0.3438, 'learning_rate': 2.2416666666666665e-05, 'epoch': 1.66}\n",
            "Training:  55% 33200/60000 [1:36:24<1:16:32,  5.84it/s[0.3438]]{'loss': 0.2365, 'learning_rate': 2.2333333333333335e-05, 'epoch': 1.66}\n",
            "Training:  56% 33300/60000 [1:36:41<1:16:13,  5.84it/s[0.2365]]{'loss': 0.2982, 'learning_rate': 2.2250000000000002e-05, 'epoch': 1.67}\n",
            "Training:  56% 33400/60000 [1:36:58<1:16:00,  5.83it/s[0.2982]]{'loss': 0.3408, 'learning_rate': 2.216666666666667e-05, 'epoch': 1.67}\n",
            "Training:  56% 33500/60000 [1:37:15<1:15:42,  5.83it/s[0.3408]]{'loss': 0.4723, 'learning_rate': 2.2083333333333333e-05, 'epoch': 1.68}\n",
            "Training:  56% 33600/60000 [1:37:32<1:15:20,  5.84it/s[0.4723]]{'loss': 0.4438, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.68}\n",
            "Training:  56% 33700/60000 [1:37:50<1:15:08,  5.83it/s[0.4438]]{'loss': 0.6652, 'learning_rate': 2.191666666666667e-05, 'epoch': 1.69}\n",
            "Training:  56% 33800/60000 [1:38:07<1:14:57,  5.83it/s[0.6652]]{'loss': 0.4265, 'learning_rate': 2.1833333333333333e-05, 'epoch': 1.69}\n",
            "Training:  56% 33900/60000 [1:38:24<1:14:47,  5.82it/s[0.4265]]{'loss': 0.3049, 'learning_rate': 2.175e-05, 'epoch': 1.69}\n",
            "Training:  57% 34000/60000 [1:38:41<1:14:26,  5.82it/s[0.3049]]{'loss': 0.2501, 'learning_rate': 2.1666666666666667e-05, 'epoch': 1.7}\n",
            "Training:  57% 34100/60000 [1:38:58<1:14:06,  5.83it/s[0.2501]]{'loss': 0.4572, 'learning_rate': 2.1583333333333334e-05, 'epoch': 1.71}\n",
            "Training:  57% 34200/60000 [1:39:16<1:13:45,  5.83it/s[0.4572]]{'loss': 0.3075, 'learning_rate': 2.15e-05, 'epoch': 1.71}\n",
            "Training:  57% 34300/60000 [1:39:33<1:13:27,  5.83it/s[0.3075]]{'loss': 0.2927, 'learning_rate': 2.1416666666666668e-05, 'epoch': 1.71}\n",
            "Training:  57% 34400/60000 [1:39:50<1:13:09,  5.83it/s[0.2927]]{'loss': 0.3661, 'learning_rate': 2.1333333333333335e-05, 'epoch': 1.72}\n",
            "Training:  57% 34500/60000 [1:40:07<1:13:00,  5.82it/s[0.3661]]{'loss': 0.4583, 'learning_rate': 2.125e-05, 'epoch': 1.73}\n",
            "Training:  58% 34600/60000 [1:40:24<1:12:45,  5.82it/s[0.4583]]{'loss': 0.5703, 'learning_rate': 2.116666666666667e-05, 'epoch': 1.73}\n",
            "Training:  58% 34700/60000 [1:40:41<1:12:28,  5.82it/s[0.5703]]{'loss': 0.2916, 'learning_rate': 2.1083333333333335e-05, 'epoch': 1.73}\n",
            "Training:  58% 34800/60000 [1:40:59<1:12:07,  5.82it/s[0.2916]]{'loss': 0.426, 'learning_rate': 2.1e-05, 'epoch': 1.74}\n",
            "Training:  58% 34900/60000 [1:41:16<1:11:45,  5.83it/s[0.426]]{'loss': 0.3362, 'learning_rate': 2.091666666666667e-05, 'epoch': 1.75}\n",
            "Training:  58% 35000/60000 [1:41:33<1:11:25,  5.83it/s[0.3362]]{'loss': 0.3858, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n",
            "Training:  58% 35100/60000 [1:41:50<1:11:12,  5.83it/s[0.3858]]{'loss': 0.3538, 'learning_rate': 2.075e-05, 'epoch': 1.75}\n",
            "Training:  59% 35200/60000 [1:42:07<1:11:00,  5.82it/s[0.3538]]{'loss': 0.8353, 'learning_rate': 2.0666666666666666e-05, 'epoch': 1.76}\n",
            "Training:  59% 35300/60000 [1:42:25<1:10:46,  5.82it/s[0.8353]]{'loss': 0.223, 'learning_rate': 2.0583333333333333e-05, 'epoch': 1.77}\n",
            "Training:  59% 35400/60000 [1:42:42<1:10:33,  5.81it/s[0.223]]{'loss': 0.4169, 'learning_rate': 2.05e-05, 'epoch': 1.77}\n",
            "Training:  59% 35500/60000 [1:42:59<1:10:16,  5.81it/s[0.4169]]{'loss': 0.3954, 'learning_rate': 2.0416666666666667e-05, 'epoch': 1.77}\n",
            "Training:  59% 35600/60000 [1:43:16<1:10:06,  5.80it/s[0.3954]]{'loss': 0.5202, 'learning_rate': 2.0333333333333334e-05, 'epoch': 1.78}\n",
            "Training:  60% 35700/60000 [1:43:34<1:09:49,  5.80it/s[0.5202]]{'loss': 0.4745, 'learning_rate': 2.025e-05, 'epoch': 1.79}\n",
            "Training:  60% 35800/60000 [1:43:51<1:09:35,  5.80it/s[0.4745]]{'loss': 0.4985, 'learning_rate': 2.0166666666666668e-05, 'epoch': 1.79}\n",
            "Training:  60% 35900/60000 [1:44:08<1:09:19,  5.79it/s[0.4985]]{'loss': 0.5065, 'learning_rate': 2.0083333333333335e-05, 'epoch': 1.79}\n",
            "Training:  60% 36000/60000 [1:44:25<1:09:07,  5.79it/s[0.5065]]{'loss': 0.0927, 'learning_rate': 2e-05, 'epoch': 1.8}\n",
            "Training:  60% 36100/60000 [1:44:43<1:09:00,  5.77it/s[0.0927]]{'loss': 0.2918, 'learning_rate': 1.9916666666666665e-05, 'epoch': 1.81}\n",
            "Training:  60% 36200/60000 [1:45:00<1:08:36,  5.78it/s[0.2918]]{'loss': 0.4951, 'learning_rate': 1.9833333333333335e-05, 'epoch': 1.81}\n",
            "Training:  60% 36300/60000 [1:45:17<1:08:21,  5.78it/s[0.4951]]{'loss': 0.2665, 'learning_rate': 1.9750000000000002e-05, 'epoch': 1.81}\n",
            "Training:  61% 36400/60000 [1:45:35<1:08:04,  5.78it/s[0.2665]]{'loss': 0.2074, 'learning_rate': 1.9666666666666666e-05, 'epoch': 1.82}\n",
            "Training:  61% 36500/60000 [1:45:52<1:07:43,  5.78it/s[0.2074]]{'loss': 0.4188, 'learning_rate': 1.9583333333333333e-05, 'epoch': 1.82}\n",
            "Training:  61% 36600/60000 [1:46:09<1:07:18,  5.79it/s[0.4188]]{'loss': 0.1627, 'learning_rate': 1.9500000000000003e-05, 'epoch': 1.83}\n",
            "Training:  61% 36700/60000 [1:46:26<1:06:56,  5.80it/s[0.1627]]{'loss': 0.2412, 'learning_rate': 1.9416666666666667e-05, 'epoch': 1.83}\n",
            "Training:  61% 36800/60000 [1:46:44<1:06:36,  5.81it/s[0.2412]]{'loss': 0.4501, 'learning_rate': 1.9333333333333333e-05, 'epoch': 1.84}\n",
            "Training:  62% 36900/60000 [1:47:01<1:06:20,  5.80it/s[0.4501]]{'loss': 0.2988, 'learning_rate': 1.925e-05, 'epoch': 1.84}\n",
            "Training:  62% 37000/60000 [1:47:18<1:06:09,  5.79it/s[0.2988]]{'loss': 0.3207, 'learning_rate': 1.9166666666666667e-05, 'epoch': 1.85}\n",
            "Training:  62% 37100/60000 [1:47:35<1:05:47,  5.80it/s[0.3207]]{'loss': 0.3166, 'learning_rate': 1.9083333333333334e-05, 'epoch': 1.85}\n",
            "Training:  62% 37200/60000 [1:47:52<1:05:27,  5.81it/s[0.3166]]{'loss': 0.3938, 'learning_rate': 1.9e-05, 'epoch': 1.86}\n",
            "Training:  62% 37300/60000 [1:48:10<1:05:13,  5.80it/s[0.3938]]{'loss': 0.0415, 'learning_rate': 1.8916666666666668e-05, 'epoch': 1.86}\n",
            "Training:  62% 37400/60000 [1:48:27<1:04:58,  5.80it/s[0.0415]]{'loss': 0.2271, 'learning_rate': 1.8833333333333335e-05, 'epoch': 1.87}\n",
            "Training:  62% 37500/60000 [1:48:44<1:04:48,  5.79it/s[0.2271]]{'loss': 0.5933, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}\n",
            "Training:  63% 37600/60000 [1:49:02<1:04:25,  5.80it/s[0.5933]]{'loss': 0.3287, 'learning_rate': 1.866666666666667e-05, 'epoch': 1.88}\n",
            "Training:  63% 37700/60000 [1:49:19<1:04:10,  5.79it/s[0.3287]]{'loss': 0.5493, 'learning_rate': 1.8583333333333332e-05, 'epoch': 1.89}\n",
            "Training:  63% 37800/60000 [1:49:36<1:03:53,  5.79it/s[0.5493]]{'loss': 0.1121, 'learning_rate': 1.85e-05, 'epoch': 1.89}\n",
            "Training:  63% 37900/60000 [1:49:53<1:03:39,  5.79it/s[0.1121]]{'loss': 0.1616, 'learning_rate': 1.841666666666667e-05, 'epoch': 1.9}\n",
            "Training:  63% 38000/60000 [1:50:11<1:03:28,  5.78it/s[0.1616]]{'loss': 0.0587, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.9}\n",
            "Training:  64% 38100/60000 [1:50:28<1:03:15,  5.77it/s[0.0587]]{'loss': 0.3053, 'learning_rate': 1.825e-05, 'epoch': 1.91}\n",
            "Training:  64% 38200/60000 [1:50:45<1:02:49,  5.78it/s[0.3053]]{'loss': 0.4154, 'learning_rate': 1.8166666666666667e-05, 'epoch': 1.91}\n",
            "Training:  64% 38300/60000 [1:51:03<1:02:30,  5.79it/s[0.4154]]{'loss': 0.5083, 'learning_rate': 1.8083333333333337e-05, 'epoch': 1.92}\n",
            "Training:  64% 38400/60000 [1:51:20<1:02:08,  5.79it/s[0.5083]]{'loss': 0.5898, 'learning_rate': 1.8e-05, 'epoch': 1.92}\n",
            "Training:  64% 38500/60000 [1:51:37<1:01:51,  5.79it/s[0.5898]]{'loss': 0.5006, 'learning_rate': 1.7916666666666667e-05, 'epoch': 1.93}\n",
            "Training:  64% 38600/60000 [1:51:54<1:01:26,  5.80it/s[0.5006]]{'loss': 0.2885, 'learning_rate': 1.7833333333333334e-05, 'epoch': 1.93}\n",
            "Training:  64% 38700/60000 [1:52:12<1:01:13,  5.80it/s[0.2885]]{'loss': 0.5481, 'learning_rate': 1.775e-05, 'epoch': 1.94}\n",
            "Training:  65% 38800/60000 [1:52:29<1:00:59,  5.79it/s[0.5481]]{'loss': 0.3745, 'learning_rate': 1.7666666666666668e-05, 'epoch': 1.94}\n",
            "Training:  65% 38900/60000 [1:52:46<1:00:46,  5.79it/s[0.3745]]{'loss': 0.346, 'learning_rate': 1.7583333333333335e-05, 'epoch': 1.94}\n",
            "Training:  65% 39000/60000 [1:53:03<1:00:27,  5.79it/s[0.346]]{'loss': 0.3298, 'learning_rate': 1.75e-05, 'epoch': 1.95}\n",
            "Training:  65% 39100/60000 [1:53:21<1:00:13,  5.78it/s[0.3298]]{'loss': 0.6335, 'learning_rate': 1.741666666666667e-05, 'epoch': 1.96}\n",
            "Training:  65% 39200/60000 [1:53:38<59:58,  5.78it/s[0.6335]]  {'loss': 0.4652, 'learning_rate': 1.7333333333333336e-05, 'epoch': 1.96}\n",
            "Training:  66% 39300/60000 [1:53:56<59:47,  5.77it/s[0.4652]]{'loss': 0.2644, 'learning_rate': 1.725e-05, 'epoch': 1.96}\n",
            "Training:  66% 39400/60000 [1:54:13<59:35,  5.76it/s[0.2644]]{'loss': 0.4842, 'learning_rate': 1.7166666666666666e-05, 'epoch': 1.97}\n",
            "Training:  66% 39500/60000 [1:54:30<59:12,  5.77it/s[0.4842]]{'loss': 0.3058, 'learning_rate': 1.7083333333333333e-05, 'epoch': 1.98}\n",
            "Training:  66% 39600/60000 [1:54:47<58:47,  5.78it/s[0.3058]]{'loss': 0.4468, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.98}\n",
            "Training:  66% 39700/60000 [1:55:05<58:22,  5.80it/s[0.4468]]{'loss': 0.1834, 'learning_rate': 1.6916666666666667e-05, 'epoch': 1.98}\n",
            "Training:  66% 39800/60000 [1:55:22<58:06,  5.79it/s[0.1834]]{'loss': 0.2591, 'learning_rate': 1.6833333333333334e-05, 'epoch': 1.99}\n",
            "Training:  66% 39900/60000 [1:55:39<57:49,  5.79it/s[0.2591]]{'loss': 0.2074, 'learning_rate': 1.675e-05, 'epoch': 2.0}\n",
            "Training:  67% 40000/60000 [1:55:56<57:32,  5.79it/s[0.2074]]{'loss': 0.6155, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
            "Training:  67% 40000/60000 [1:56:10<57:32,  5.79it/s[0.2074]]{'eval_loss': 1.0112457275390625, 'eval_accuracy': 0.839, 'eval_runtime': 34.9311, 'eval_samples_per_second': 143.139, 'eval_steps_per_second': 17.892, 'epoch': 2.0}\n",
            "Training:  67% 40100/60000 [1:57:24<2:06:51,  2.61it/s[0.2074]]{'loss': 0.0002, 'learning_rate': 1.6583333333333334e-05, 'epoch': 2.0}\n",
            "Training:  67% 40200/60000 [1:57:41<1:45:25,  3.13it/s[0.0002]]{'loss': 0.3752, 'learning_rate': 1.65e-05, 'epoch': 2.01}\n",
            "Training:  67% 40300/60000 [1:57:58<1:30:26,  3.63it/s[0.3752]]{'loss': 0.1248, 'learning_rate': 1.6416666666666665e-05, 'epoch': 2.02}\n",
            "Training:  67% 40400/60000 [1:58:15<1:19:53,  4.09it/s[0.1248]]{'loss': 0.1769, 'learning_rate': 1.6333333333333335e-05, 'epoch': 2.02}\n",
            "Training:  68% 40500/60000 [1:58:33<1:12:22,  4.49it/s[0.1769]]{'loss': 0.1488, 'learning_rate': 1.6250000000000002e-05, 'epoch': 2.02}\n",
            "Training:  68% 40600/60000 [1:58:50<1:07:00,  4.82it/s[0.1488]]{'loss': 0.0996, 'learning_rate': 1.6166666666666665e-05, 'epoch': 2.03}\n",
            "Training:  68% 40700/60000 [1:59:07<1:03:10,  5.09it/s[0.0996]]{'loss': 0.1293, 'learning_rate': 1.6083333333333332e-05, 'epoch': 2.04}\n",
            "Training:  68% 40800/60000 [1:59:24<1:00:25,  5.30it/s[0.1293]]{'loss': 0.0193, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.04}\n",
            "Training:  68% 40900/60000 [1:59:41<58:26,  5.45it/s[0.0193]]  {'loss': 0.0639, 'learning_rate': 1.591666666666667e-05, 'epoch': 2.04}\n",
            "Training:  68% 41000/60000 [1:59:58<56:56,  5.56it/s[0.0639]]{'loss': 0.2446, 'learning_rate': 1.5833333333333333e-05, 'epoch': 2.05}\n",
            "Training:  68% 41100/60000 [2:00:15<55:51,  5.64it/s[0.2446]]{'loss': 0.1124, 'learning_rate': 1.575e-05, 'epoch': 2.06}\n",
            "Training:  69% 41200/60000 [2:00:33<55:10,  5.68it/s[0.1124]]{'loss': 0.1569, 'learning_rate': 1.5666666666666667e-05, 'epoch': 2.06}\n",
            "Training:  69% 41300/60000 [2:00:50<54:34,  5.71it/s[0.1569]]{'loss': 0.1472, 'learning_rate': 1.5583333333333334e-05, 'epoch': 2.06}\n",
            "Training:  69% 41400/60000 [2:01:07<54:01,  5.74it/s[0.1472]]{'loss': 0.2265, 'learning_rate': 1.55e-05, 'epoch': 2.07}\n",
            "Training:  69% 41500/60000 [2:01:24<53:31,  5.76it/s[0.2265]]{'loss': 0.1322, 'learning_rate': 1.5416666666666668e-05, 'epoch': 2.08}\n",
            "Training:  69% 41600/60000 [2:01:42<53:06,  5.78it/s[0.1322]]{'loss': 0.1926, 'learning_rate': 1.5333333333333334e-05, 'epoch': 2.08}\n",
            "Training:  70% 41700/60000 [2:01:59<52:50,  5.77it/s[0.1926]]{'loss': 0.0006, 'learning_rate': 1.525e-05, 'epoch': 2.08}\n",
            "Training:  70% 41800/60000 [2:02:16<52:28,  5.78it/s[0.0006]]{'loss': 0.2653, 'learning_rate': 1.5166666666666668e-05, 'epoch': 2.09}\n",
            "Training:  70% 41900/60000 [2:02:33<51:59,  5.80it/s[0.2653]]{'loss': 0.0551, 'learning_rate': 1.5083333333333335e-05, 'epoch': 2.1}\n",
            "Training:  70% 42000/60000 [2:02:50<51:37,  5.81it/s[0.0551]]{'loss': 0.0017, 'learning_rate': 1.5e-05, 'epoch': 2.1}\n",
            "Training:  70% 42100/60000 [2:03:08<51:23,  5.81it/s[0.0017]]{'loss': 0.1789, 'learning_rate': 1.4916666666666667e-05, 'epoch': 2.1}\n",
            "Training:  70% 42200/60000 [2:03:25<51:00,  5.82it/s[0.1789]]{'loss': 0.2034, 'learning_rate': 1.4833333333333336e-05, 'epoch': 2.11}\n",
            "Training:  70% 42300/60000 [2:03:42<50:40,  5.82it/s[0.2034]]{'loss': 0.1109, 'learning_rate': 1.475e-05, 'epoch': 2.12}\n",
            "Training:  71% 42400/60000 [2:03:59<50:25,  5.82it/s[0.1109]]{'loss': 0.0258, 'learning_rate': 1.4666666666666668e-05, 'epoch': 2.12}\n",
            "Training:  71% 42500/60000 [2:04:16<50:10,  5.81it/s[0.0258]]{'loss': 0.0058, 'learning_rate': 1.4583333333333335e-05, 'epoch': 2.12}\n",
            "Training:  71% 42600/60000 [2:04:34<49:58,  5.80it/s[0.0058]]{'loss': 0.2568, 'learning_rate': 1.45e-05, 'epoch': 2.13}\n",
            "Training:  71% 42700/60000 [2:04:51<49:40,  5.81it/s[0.2568]]{'loss': 0.1364, 'learning_rate': 1.4416666666666667e-05, 'epoch': 2.13}\n",
            "Training:  71% 42800/60000 [2:05:08<49:25,  5.80it/s[0.1364]]{'loss': 0.045, 'learning_rate': 1.4333333333333334e-05, 'epoch': 2.14}\n",
            "Training:  72% 42900/60000 [2:05:25<49:05,  5.81it/s[0.045]]{'loss': 0.2794, 'learning_rate': 1.4249999999999999e-05, 'epoch': 2.15}\n",
            "Training:  72% 43000/60000 [2:05:43<48:54,  5.79it/s[0.2794]]{'loss': 0.0411, 'learning_rate': 1.4166666666666668e-05, 'epoch': 2.15}\n",
            "Training:  72% 43100/60000 [2:06:00<48:38,  5.79it/s[0.0411]]{'loss': 0.0124, 'learning_rate': 1.4083333333333335e-05, 'epoch': 2.15}\n",
            "Training:  72% 43200/60000 [2:06:17<48:20,  5.79it/s[0.0124]]{'loss': 0.0779, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.16}\n",
            "Training:  72% 43300/60000 [2:06:34<48:00,  5.80it/s[0.0779]]{'loss': 0.0715, 'learning_rate': 1.3916666666666667e-05, 'epoch': 2.17}\n",
            "Training:  72% 43400/60000 [2:06:52<47:48,  5.79it/s[0.0715]]{'loss': 0.1519, 'learning_rate': 1.3833333333333334e-05, 'epoch': 2.17}\n",
            "Training:  72% 43500/60000 [2:07:09<47:27,  5.79it/s[0.1519]]{'loss': 0.0284, 'learning_rate': 1.3750000000000002e-05, 'epoch': 2.17}\n",
            "Training:  73% 43600/60000 [2:07:26<47:03,  5.81it/s[0.0284]]{'loss': 0.1813, 'learning_rate': 1.3666666666666666e-05, 'epoch': 2.18}\n",
            "Training:  73% 43700/60000 [2:07:43<46:45,  5.81it/s[0.1813]]{'loss': 0.1886, 'learning_rate': 1.3583333333333334e-05, 'epoch': 2.19}\n",
            "Training:  73% 43800/60000 [2:08:01<46:34,  5.80it/s[0.1886]]{'loss': 0.1331, 'learning_rate': 1.3500000000000001e-05, 'epoch': 2.19}\n",
            "Training:  73% 43900/60000 [2:08:18<46:21,  5.79it/s[0.1331]]{'loss': 0.088, 'learning_rate': 1.3416666666666666e-05, 'epoch': 2.19}\n",
            "Training:  73% 44000/60000 [2:08:35<46:02,  5.79it/s[0.088]]{'loss': 0.0015, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}\n",
            "Training:  74% 44100/60000 [2:08:52<45:42,  5.80it/s[0.0015]]{'loss': 0.0707, 'learning_rate': 1.3250000000000002e-05, 'epoch': 2.21}\n",
            "Training:  74% 44200/60000 [2:09:09<45:17,  5.81it/s[0.0707]]{'loss': 0.0118, 'learning_rate': 1.3166666666666665e-05, 'epoch': 2.21}\n",
            "Training:  74% 44300/60000 [2:09:27<44:57,  5.82it/s[0.0118]]{'loss': 0.1744, 'learning_rate': 1.3083333333333334e-05, 'epoch': 2.21}\n",
            "Training:  74% 44400/60000 [2:09:44<44:36,  5.83it/s[0.1744]]{'loss': 0.1521, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.22}\n",
            "Training:  74% 44500/60000 [2:10:01<44:18,  5.83it/s[0.1521]]{'loss': 0.0672, 'learning_rate': 1.2916666666666668e-05, 'epoch': 2.23}\n",
            "Training:  74% 44600/60000 [2:10:18<44:03,  5.83it/s[0.0672]]{'loss': 0.1356, 'learning_rate': 1.2833333333333333e-05, 'epoch': 2.23}\n",
            "Training:  74% 44700/60000 [2:10:35<43:39,  5.84it/s[0.1356]]{'loss': 0.0816, 'learning_rate': 1.2750000000000002e-05, 'epoch': 2.23}\n",
            "Training:  75% 44800/60000 [2:10:52<43:25,  5.83it/s[0.0816]]{'loss': 0.2496, 'learning_rate': 1.2666666666666668e-05, 'epoch': 2.24}\n",
            "Training:  75% 44900/60000 [2:11:10<43:32,  5.78it/s[0.2496]]{'loss': 0.0041, 'learning_rate': 1.2583333333333334e-05, 'epoch': 2.25}\n",
            "Training:  75% 45000/60000 [2:11:27<43:23,  5.76it/s[0.0041]]{'loss': 0.1193, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n",
            "Training:  75% 45100/60000 [2:11:45<43:07,  5.76it/s[0.1193]]{'loss': 0.2209, 'learning_rate': 1.2416666666666667e-05, 'epoch': 2.25}\n",
            "Training:  75% 45200/60000 [2:12:02<42:52,  5.75it/s[0.2209]]{'loss': 0.1424, 'learning_rate': 1.2333333333333334e-05, 'epoch': 2.26}\n",
            "Training:  76% 45300/60000 [2:12:20<42:41,  5.74it/s[0.1424]]{'loss': 0.0966, 'learning_rate': 1.225e-05, 'epoch': 2.27}\n",
            "Training:  76% 45400/60000 [2:12:37<42:38,  5.71it/s[0.0966]]{'loss': 0.124, 'learning_rate': 1.2166666666666668e-05, 'epoch': 2.27}\n",
            "Training:  76% 45500/60000 [2:12:55<42:16,  5.72it/s[0.124]]{'loss': 0.0881, 'learning_rate': 1.2083333333333333e-05, 'epoch': 2.27}\n",
            "Training:  76% 45600/60000 [2:13:12<41:45,  5.75it/s[0.0881]]{'loss': 0.143, 'learning_rate': 1.2e-05, 'epoch': 2.28}\n",
            "Training:  76% 45700/60000 [2:13:29<41:20,  5.77it/s[0.143]]{'loss': 0.0916, 'learning_rate': 1.1916666666666667e-05, 'epoch': 2.29}\n",
            "Training:  76% 45800/60000 [2:13:47<40:57,  5.78it/s[0.0916]]{'loss': 0.1152, 'learning_rate': 1.1833333333333334e-05, 'epoch': 2.29}\n",
            "Training:  76% 45900/60000 [2:14:04<40:35,  5.79it/s[0.1152]]{'loss': 0.0838, 'learning_rate': 1.175e-05, 'epoch': 2.29}\n",
            "Training:  77% 46000/60000 [2:14:21<40:10,  5.81it/s[0.0838]]{'loss': 0.0854, 'learning_rate': 1.1666666666666668e-05, 'epoch': 2.3}\n",
            "Training:  77% 46100/60000 [2:14:38<39:47,  5.82it/s[0.0854]]{'loss': 0.1588, 'learning_rate': 1.1583333333333333e-05, 'epoch': 2.31}\n",
            "Training:  77% 46200/60000 [2:14:55<39:27,  5.83it/s[0.1588]]{'loss': 0.1222, 'learning_rate': 1.1500000000000002e-05, 'epoch': 2.31}\n",
            "Training:  77% 46300/60000 [2:15:12<39:11,  5.82it/s[0.1222]]{'loss': 0.0391, 'learning_rate': 1.1416666666666667e-05, 'epoch': 2.31}\n",
            "Training:  77% 46400/60000 [2:15:30<39:01,  5.81it/s[0.0391]]{'loss': 0.1755, 'learning_rate': 1.1333333333333334e-05, 'epoch': 2.32}\n",
            "Training:  78% 46500/60000 [2:15:47<38:47,  5.80it/s[0.1755]]{'loss': 0.1724, 'learning_rate': 1.125e-05, 'epoch': 2.33}\n",
            "Training:  78% 46600/60000 [2:16:04<38:29,  5.80it/s[0.1724]]{'loss': 0.0938, 'learning_rate': 1.1166666666666668e-05, 'epoch': 2.33}\n",
            "Training:  78% 46700/60000 [2:16:21<38:07,  5.81it/s[0.0938]]{'loss': 0.0688, 'learning_rate': 1.1083333333333335e-05, 'epoch': 2.33}\n",
            "Training:  78% 46800/60000 [2:16:39<37:57,  5.80it/s[0.0688]]{'loss': 0.2476, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.34}\n",
            "Training:  78% 46900/60000 [2:16:56<37:39,  5.80it/s[0.2476]]{'loss': 0.1702, 'learning_rate': 1.0916666666666667e-05, 'epoch': 2.34}\n",
            "Training:  78% 47000/60000 [2:17:13<37:24,  5.79it/s[0.1702]]{'loss': 0.0522, 'learning_rate': 1.0833333333333334e-05, 'epoch': 2.35}\n",
            "Training:  78% 47100/60000 [2:17:30<37:11,  5.78it/s[0.0522]]{'loss': 0.1364, 'learning_rate': 1.075e-05, 'epoch': 2.35}\n",
            "Training:  79% 47200/60000 [2:17:48<36:58,  5.77it/s[0.1364]]{'loss': 0.0006, 'learning_rate': 1.0666666666666667e-05, 'epoch': 2.36}\n",
            "Training:  79% 47300/60000 [2:18:05<36:47,  5.75it/s[0.0006]]{'loss': 0.4191, 'learning_rate': 1.0583333333333334e-05, 'epoch': 2.37}\n",
            "Training:  79% 47400/60000 [2:18:23<36:27,  5.76it/s[0.4191]]{'loss': 0.1712, 'learning_rate': 1.05e-05, 'epoch': 2.37}\n",
            "Training:  79% 47500/60000 [2:18:40<36:11,  5.76it/s[0.1712]]{'loss': 0.0005, 'learning_rate': 1.0416666666666668e-05, 'epoch': 2.38}\n",
            "Training:  79% 47600/60000 [2:18:57<35:52,  5.76it/s[0.0005]]{'loss': 0.1356, 'learning_rate': 1.0333333333333333e-05, 'epoch': 2.38}\n",
            "Training:  80% 47700/60000 [2:19:15<35:35,  5.76it/s[0.1356]]{'loss': 0.2183, 'learning_rate': 1.025e-05, 'epoch': 2.38}\n",
            "Training:  80% 47800/60000 [2:19:32<35:14,  5.77it/s[0.2183]]{'loss': 0.1195, 'learning_rate': 1.0166666666666667e-05, 'epoch': 2.39}\n",
            "Training:  80% 47900/60000 [2:19:49<34:56,  5.77it/s[0.1195]]{'loss': 0.1101, 'learning_rate': 1.0083333333333334e-05, 'epoch': 2.4}\n",
            "Training:  80% 48000/60000 [2:20:07<34:35,  5.78it/s[0.1101]]{'loss': 0.1516, 'learning_rate': 1e-05, 'epoch': 2.4}\n",
            "Training:  80% 48100/60000 [2:20:24<34:16,  5.79it/s[0.1516]]{'loss': 0.2145, 'learning_rate': 9.916666666666668e-06, 'epoch': 2.41}\n",
            "Training:  80% 48200/60000 [2:20:41<33:56,  5.79it/s[0.2145]]{'loss': 0.1236, 'learning_rate': 9.833333333333333e-06, 'epoch': 2.41}\n",
            "Training:  80% 48300/60000 [2:20:58<33:39,  5.79it/s[0.1236]]{'loss': 0.2947, 'learning_rate': 9.750000000000002e-06, 'epoch': 2.42}\n",
            "Training:  81% 48400/60000 [2:21:16<33:23,  5.79it/s[0.2947]]{'loss': 0.3321, 'learning_rate': 9.666666666666667e-06, 'epoch': 2.42}\n",
            "Training:  81% 48500/60000 [2:21:33<33:08,  5.78it/s[0.3321]]{'loss': 0.0942, 'learning_rate': 9.583333333333334e-06, 'epoch': 2.42}\n",
            "Training:  81% 48600/60000 [2:21:50<32:50,  5.78it/s[0.0942]]{'loss': 0.1185, 'learning_rate': 9.5e-06, 'epoch': 2.43}\n",
            "Training:  81% 48700/60000 [2:22:08<32:36,  5.78it/s[0.1185]]{'loss': 0.0576, 'learning_rate': 9.416666666666667e-06, 'epoch': 2.44}\n",
            "Training:  81% 48800/60000 [2:22:25<32:19,  5.77it/s[0.0576]]{'loss': 0.2015, 'learning_rate': 9.333333333333334e-06, 'epoch': 2.44}\n",
            "Training:  82% 48900/60000 [2:22:42<32:00,  5.78it/s[0.2015]]{'loss': 0.1008, 'learning_rate': 9.25e-06, 'epoch': 2.44}\n",
            "Training:  82% 49000/60000 [2:22:59<31:40,  5.79it/s[0.1008]]{'loss': 0.0, 'learning_rate': 9.166666666666666e-06, 'epoch': 2.45}\n",
            "Training:  82% 49100/60000 [2:23:17<31:20,  5.80it/s[0.0]]{'loss': 0.0608, 'learning_rate': 9.083333333333333e-06, 'epoch': 2.46}\n",
            "Training:  82% 49200/60000 [2:23:34<30:58,  5.81it/s[0.0608]]{'loss': 0.04, 'learning_rate': 9e-06, 'epoch': 2.46}\n",
            "Training:  82% 49300/60000 [2:23:51<30:43,  5.81it/s[0.04]]{'loss': 0.214, 'learning_rate': 8.916666666666667e-06, 'epoch': 2.46}\n",
            "Training:  82% 49400/60000 [2:24:08<30:28,  5.80it/s[0.214]]{'loss': 0.1901, 'learning_rate': 8.833333333333334e-06, 'epoch': 2.47}\n",
            "Training:  82% 49500/60000 [2:24:26<30:15,  5.78it/s[0.1901]]{'loss': 0.2056, 'learning_rate': 8.75e-06, 'epoch': 2.48}\n",
            "Training:  83% 49600/60000 [2:24:43<29:55,  5.79it/s[0.2056]]{'loss': 0.0039, 'learning_rate': 8.666666666666668e-06, 'epoch': 2.48}\n",
            "Training:  83% 49700/60000 [2:25:00<29:34,  5.80it/s[0.0039]]{'loss': 0.5569, 'learning_rate': 8.583333333333333e-06, 'epoch': 2.48}\n",
            "Training:  83% 49800/60000 [2:25:17<29:15,  5.81it/s[0.5569]]{'loss': 0.2431, 'learning_rate': 8.500000000000002e-06, 'epoch': 2.49}\n",
            "Training:  83% 49900/60000 [2:25:34<28:58,  5.81it/s[0.2431]]{'loss': 0.0988, 'learning_rate': 8.416666666666667e-06, 'epoch': 2.5}\n",
            "Training:  83% 50000/60000 [2:25:52<28:42,  5.81it/s[0.0988]]{'loss': 0.0478, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n",
            "Training:  84% 50100/60000 [2:26:09<28:26,  5.80it/s[0.0478]]{'loss': 0.1178, 'learning_rate': 8.25e-06, 'epoch': 2.5}\n",
            "Training:  84% 50200/60000 [2:26:26<28:09,  5.80it/s[0.1178]]{'loss': 0.1414, 'learning_rate': 8.166666666666668e-06, 'epoch': 2.51}\n",
            "Training:  84% 50300/60000 [2:26:43<27:53,  5.80it/s[0.1414]]{'loss': 0.0017, 'learning_rate': 8.083333333333333e-06, 'epoch': 2.52}\n",
            "Training:  84% 50400/60000 [2:27:01<27:35,  5.80it/s[0.0017]]{'loss': 0.031, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.52}\n",
            "Training:  84% 50500/60000 [2:27:18<27:18,  5.80it/s[0.031]]{'loss': 0.4225, 'learning_rate': 7.916666666666667e-06, 'epoch': 2.52}\n",
            "Training:  84% 50600/60000 [2:27:35<26:59,  5.81it/s[0.4225]]{'loss': 0.0726, 'learning_rate': 7.833333333333333e-06, 'epoch': 2.53}\n",
            "Training:  84% 50700/60000 [2:27:52<26:42,  5.80it/s[0.0726]]{'loss': 0.0779, 'learning_rate': 7.75e-06, 'epoch': 2.54}\n",
            "Training:  85% 50800/60000 [2:28:09<26:22,  5.81it/s[0.0779]]{'loss': 0.0649, 'learning_rate': 7.666666666666667e-06, 'epoch': 2.54}\n",
            "Training:  85% 50900/60000 [2:28:27<26:03,  5.82it/s[0.0649]]{'loss': 0.0006, 'learning_rate': 7.583333333333334e-06, 'epoch': 2.54}\n",
            "Training:  85% 51000/60000 [2:28:44<25:44,  5.83it/s[0.0006]]{'loss': 0.0026, 'learning_rate': 7.5e-06, 'epoch': 2.55}\n",
            "Training:  85% 51100/60000 [2:29:01<25:28,  5.82it/s[0.0026]]{'loss': 0.1042, 'learning_rate': 7.416666666666668e-06, 'epoch': 2.56}\n",
            "Training:  85% 51200/60000 [2:29:18<25:10,  5.83it/s[0.1042]]{'loss': 0.2826, 'learning_rate': 7.333333333333334e-06, 'epoch': 2.56}\n",
            "Training:  86% 51300/60000 [2:29:35<24:57,  5.81it/s[0.2826]]{'loss': 0.0318, 'learning_rate': 7.25e-06, 'epoch': 2.56}\n",
            "Training:  86% 51400/60000 [2:29:53<24:41,  5.80it/s[0.0318]]{'loss': 0.1578, 'learning_rate': 7.166666666666667e-06, 'epoch': 2.57}\n",
            "Training:  86% 51500/60000 [2:30:10<24:23,  5.81it/s[0.1578]]{'loss': 0.0437, 'learning_rate': 7.083333333333334e-06, 'epoch': 2.58}\n",
            "Training:  86% 51600/60000 [2:30:27<24:10,  5.79it/s[0.0437]]{'loss': 0.0539, 'learning_rate': 7.000000000000001e-06, 'epoch': 2.58}\n",
            "Training:  86% 51700/60000 [2:30:45<23:54,  5.79it/s[0.0539]]{'loss': 0.0828, 'learning_rate': 6.916666666666667e-06, 'epoch': 2.58}\n",
            "Training:  86% 51800/60000 [2:31:02<23:35,  5.79it/s[0.0828]]{'loss': 0.2281, 'learning_rate': 6.833333333333333e-06, 'epoch': 2.59}\n",
            "Training:  86% 51900/60000 [2:31:19<23:17,  5.80it/s[0.2281]]{'loss': 0.0897, 'learning_rate': 6.750000000000001e-06, 'epoch': 2.59}\n",
            "Training:  87% 52000/60000 [2:31:36<23:00,  5.79it/s[0.0897]]{'loss': 0.1436, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}\n",
            "Training:  87% 52100/60000 [2:31:54<22:46,  5.78it/s[0.1436]]{'loss': 0.0029, 'learning_rate': 6.583333333333333e-06, 'epoch': 2.6}\n",
            "Training:  87% 52200/60000 [2:32:11<22:30,  5.77it/s[0.0029]]{'loss': 0.1534, 'learning_rate': 6.5000000000000004e-06, 'epoch': 2.61}\n",
            "Training:  87% 52300/60000 [2:32:28<22:11,  5.78it/s[0.1534]]{'loss': 0.1888, 'learning_rate': 6.4166666666666665e-06, 'epoch': 2.62}\n",
            "Training:  87% 52400/60000 [2:32:46<21:54,  5.78it/s[0.1888]]{'loss': 0.0483, 'learning_rate': 6.333333333333334e-06, 'epoch': 2.62}\n",
            "Training:  88% 52500/60000 [2:33:03<21:36,  5.79it/s[0.0483]]{'loss': 0.0671, 'learning_rate': 6.25e-06, 'epoch': 2.62}\n",
            "Training:  88% 52600/60000 [2:33:20<21:20,  5.78it/s[0.0671]]{'loss': 0.2335, 'learning_rate': 6.166666666666667e-06, 'epoch': 2.63}\n",
            "Training:  88% 52700/60000 [2:33:37<21:03,  5.78it/s[0.2335]]{'loss': 0.1318, 'learning_rate': 6.083333333333334e-06, 'epoch': 2.63}\n",
            "Training:  88% 52800/60000 [2:33:55<20:45,  5.78it/s[0.1318]]{'loss': 0.0003, 'learning_rate': 6e-06, 'epoch': 2.64}\n",
            "Training:  88% 52900/60000 [2:34:12<20:26,  5.79it/s[0.0003]]{'loss': 0.0316, 'learning_rate': 5.916666666666667e-06, 'epoch': 2.65}\n",
            "Training:  88% 53000/60000 [2:34:29<20:09,  5.79it/s[0.0316]]{'loss': 0.322, 'learning_rate': 5.833333333333334e-06, 'epoch': 2.65}\n",
            "Training:  88% 53100/60000 [2:34:46<19:50,  5.80it/s[0.322]]{'loss': 0.0882, 'learning_rate': 5.750000000000001e-06, 'epoch': 2.66}\n",
            "Training:  89% 53200/60000 [2:35:04<19:34,  5.79it/s[0.0882]]{'loss': 0.1292, 'learning_rate': 5.666666666666667e-06, 'epoch': 2.66}\n",
            "Training:  89% 53300/60000 [2:35:21<19:15,  5.80it/s[0.1292]]{'loss': 0.2147, 'learning_rate': 5.583333333333334e-06, 'epoch': 2.67}\n",
            "Training:  89% 53400/60000 [2:35:38<18:58,  5.80it/s[0.2147]]{'loss': 0.1492, 'learning_rate': 5.500000000000001e-06, 'epoch': 2.67}\n",
            "Training:  89% 53500/60000 [2:35:55<18:40,  5.80it/s[0.1492]]{'loss': 0.1705, 'learning_rate': 5.416666666666667e-06, 'epoch': 2.67}\n",
            "Training:  89% 53600/60000 [2:36:13<18:22,  5.80it/s[0.1705]]{'loss': 0.1482, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.68}\n",
            "Training:  90% 53700/60000 [2:36:30<18:05,  5.81it/s[0.1482]]{'loss': 0.2081, 'learning_rate': 5.25e-06, 'epoch': 2.69}\n",
            "Training:  90% 53800/60000 [2:36:47<17:49,  5.80it/s[0.2081]]{'loss': 0.0641, 'learning_rate': 5.166666666666667e-06, 'epoch': 2.69}\n",
            "Training:  90% 53900/60000 [2:37:05<17:34,  5.79it/s[0.0641]]{'loss': 0.1326, 'learning_rate': 5.0833333333333335e-06, 'epoch': 2.69}\n",
            "Training:  90% 54000/60000 [2:37:22<17:21,  5.76it/s[0.1326]]{'loss': 0.0004, 'learning_rate': 5e-06, 'epoch': 2.7}\n",
            "Training:  90% 54100/60000 [2:37:40<17:07,  5.74it/s[0.0004]]{'loss': 0.1082, 'learning_rate': 4.9166666666666665e-06, 'epoch': 2.71}\n",
            "Training:  90% 54200/60000 [2:37:57<16:51,  5.73it/s[0.1082]]{'loss': 0.249, 'learning_rate': 4.833333333333333e-06, 'epoch': 2.71}\n",
            "Training:  90% 54300/60000 [2:38:14<16:32,  5.74it/s[0.249]]{'loss': 0.0839, 'learning_rate': 4.75e-06, 'epoch': 2.71}\n",
            "Training:  91% 54400/60000 [2:38:32<16:14,  5.75it/s[0.0839]]{'loss': 0.2414, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.72}\n",
            "Training:  91% 54500/60000 [2:38:49<15:54,  5.76it/s[0.2414]]{'loss': 0.1235, 'learning_rate': 4.583333333333333e-06, 'epoch': 2.73}\n",
            "Training:  91% 54600/60000 [2:39:06<15:36,  5.76it/s[0.1235]]{'loss': 0.0477, 'learning_rate': 4.5e-06, 'epoch': 2.73}\n",
            "Training:  91% 54700/60000 [2:39:24<15:17,  5.78it/s[0.0477]]{'loss': 0.0018, 'learning_rate': 4.416666666666667e-06, 'epoch': 2.73}\n",
            "Training:  91% 54800/60000 [2:39:41<14:58,  5.79it/s[0.0018]]{'loss': 0.2513, 'learning_rate': 4.333333333333334e-06, 'epoch': 2.74}\n",
            "Training:  92% 54900/60000 [2:39:58<14:39,  5.80it/s[0.2513]]{'loss': 0.1491, 'learning_rate': 4.250000000000001e-06, 'epoch': 2.75}\n",
            "Training:  92% 55000/60000 [2:40:15<14:22,  5.80it/s[0.1491]]{'loss': 0.1593, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n",
            "Training:  92% 55100/60000 [2:40:32<14:04,  5.80it/s[0.1593]]{'loss': 0.0756, 'learning_rate': 4.083333333333334e-06, 'epoch': 2.75}\n",
            "Training:  92% 55200/60000 [2:40:50<13:48,  5.80it/s[0.0756]]{'loss': 0.0018, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.76}\n",
            "Training:  92% 55300/60000 [2:41:07<13:30,  5.80it/s[0.0018]]{'loss': 0.0698, 'learning_rate': 3.916666666666667e-06, 'epoch': 2.77}\n",
            "Training:  92% 55400/60000 [2:41:24<13:11,  5.81it/s[0.0698]]{'loss': 0.0006, 'learning_rate': 3.833333333333334e-06, 'epoch': 2.77}\n",
            "Training:  92% 55500/60000 [2:41:41<12:54,  5.81it/s[0.0006]]{'loss': 0.0001, 'learning_rate': 3.75e-06, 'epoch': 2.77}\n",
            "Training:  93% 55600/60000 [2:41:59<12:37,  5.81it/s[0.0001]]{'loss': 0.0572, 'learning_rate': 3.666666666666667e-06, 'epoch': 2.78}\n",
            "Training:  93% 55700/60000 [2:42:16<12:19,  5.81it/s[0.0572]]{'loss': 0.0001, 'learning_rate': 3.5833333333333335e-06, 'epoch': 2.79}\n",
            "Training:  93% 55800/60000 [2:42:33<12:03,  5.81it/s[0.0001]]{'loss': 0.1167, 'learning_rate': 3.5000000000000004e-06, 'epoch': 2.79}\n",
            "Training:  93% 55900/60000 [2:42:50<11:46,  5.81it/s[0.1167]]{'loss': 0.1393, 'learning_rate': 3.4166666666666664e-06, 'epoch': 2.79}\n",
            "Training:  93% 56000/60000 [2:43:07<11:28,  5.81it/s[0.1393]]{'loss': 0.0445, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}\n",
            "Training:  94% 56100/60000 [2:43:25<11:11,  5.81it/s[0.0445]]{'loss': 0.2204, 'learning_rate': 3.2500000000000002e-06, 'epoch': 2.81}\n",
            "Training:  94% 56200/60000 [2:43:42<10:54,  5.81it/s[0.2204]]{'loss': 0.2184, 'learning_rate': 3.166666666666667e-06, 'epoch': 2.81}\n",
            "Training:  94% 56300/60000 [2:43:59<10:37,  5.81it/s[0.2184]]{'loss': 0.114, 'learning_rate': 3.0833333333333336e-06, 'epoch': 2.81}\n",
            "Training:  94% 56400/60000 [2:44:16<10:20,  5.81it/s[0.114]]{'loss': 0.0454, 'learning_rate': 3e-06, 'epoch': 2.82}\n",
            "Training:  94% 56500/60000 [2:44:34<10:03,  5.80it/s[0.0454]]{'loss': 0.1775, 'learning_rate': 2.916666666666667e-06, 'epoch': 2.83}\n",
            "Training:  94% 56600/60000 [2:44:51<09:46,  5.79it/s[0.1775]]{'loss': 0.0799, 'learning_rate': 2.8333333333333335e-06, 'epoch': 2.83}\n",
            "Training:  94% 56700/60000 [2:45:08<09:29,  5.79it/s[0.0799]]{'loss': 0.0564, 'learning_rate': 2.7500000000000004e-06, 'epoch': 2.83}\n",
            "Training:  95% 56800/60000 [2:45:26<09:13,  5.79it/s[0.0564]]{'loss': 0.1865, 'learning_rate': 2.666666666666667e-06, 'epoch': 2.84}\n",
            "Training:  95% 56900/60000 [2:45:43<08:55,  5.79it/s[0.1865]]{'loss': 0.0464, 'learning_rate': 2.5833333333333333e-06, 'epoch': 2.84}\n",
            "Training:  95% 57000/60000 [2:46:00<08:37,  5.80it/s[0.0464]]{'loss': 0.2333, 'learning_rate': 2.5e-06, 'epoch': 2.85}\n",
            "Training:  95% 57100/60000 [2:46:17<08:20,  5.80it/s[0.2333]]{'loss': 0.0759, 'learning_rate': 2.4166666666666667e-06, 'epoch': 2.85}\n",
            "Training:  95% 57200/60000 [2:46:35<08:03,  5.79it/s[0.0759]]{'loss': 0.1031, 'learning_rate': 2.3333333333333336e-06, 'epoch': 2.86}\n",
            "Training:  96% 57300/60000 [2:46:52<07:46,  5.79it/s[0.1031]]{'loss': 0.4357, 'learning_rate': 2.25e-06, 'epoch': 2.87}\n",
            "Training:  96% 57400/60000 [2:47:09<07:29,  5.78it/s[0.4357]]{'loss': 0.1295, 'learning_rate': 2.166666666666667e-06, 'epoch': 2.87}\n",
            "Training:  96% 57500/60000 [2:47:27<07:12,  5.78it/s[0.1295]]{'loss': 0.0625, 'learning_rate': 2.0833333333333334e-06, 'epoch': 2.88}\n",
            "Training:  96% 57600/60000 [2:47:44<06:56,  5.77it/s[0.0625]]{'loss': 0.1292, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.88}\n",
            "Training:  96% 57700/60000 [2:48:01<06:38,  5.77it/s[0.1292]]{'loss': 0.2246, 'learning_rate': 1.916666666666667e-06, 'epoch': 2.88}\n",
            "Training:  96% 57800/60000 [2:48:19<06:22,  5.76it/s[0.2246]]{'loss': 0.0337, 'learning_rate': 1.8333333333333335e-06, 'epoch': 2.89}\n",
            "Training:  96% 57900/60000 [2:48:36<06:04,  5.76it/s[0.0337]]{'loss': 0.2391, 'learning_rate': 1.7500000000000002e-06, 'epoch': 2.9}\n",
            "Training:  97% 58000/60000 [2:48:53<05:47,  5.76it/s[0.2391]]{'loss': 0.1186, 'learning_rate': 1.6666666666666667e-06, 'epoch': 2.9}\n",
            "Training:  97% 58100/60000 [2:49:11<05:30,  5.76it/s[0.1186]]{'loss': 0.0817, 'learning_rate': 1.5833333333333336e-06, 'epoch': 2.91}\n",
            "Training:  97% 58200/60000 [2:49:28<05:12,  5.77it/s[0.0817]]{'loss': 0.059, 'learning_rate': 1.5e-06, 'epoch': 2.91}\n",
            "Training:  97% 58300/60000 [2:49:46<04:55,  5.76it/s[0.059]]{'loss': 0.1513, 'learning_rate': 1.4166666666666667e-06, 'epoch': 2.92}\n",
            "Training:  97% 58400/60000 [2:50:03<04:37,  5.76it/s[0.1513]]{'loss': 0.0644, 'learning_rate': 1.3333333333333334e-06, 'epoch': 2.92}\n",
            "Training:  98% 58500/60000 [2:50:20<04:20,  5.76it/s[0.0644]]{'loss': 0.2504, 'learning_rate': 1.25e-06, 'epoch': 2.92}\n",
            "Training:  98% 58600/60000 [2:50:38<04:02,  5.77it/s[0.2504]]{'loss': 0.2479, 'learning_rate': 1.1666666666666668e-06, 'epoch': 2.93}\n",
            "Training:  98% 58700/60000 [2:50:55<03:45,  5.77it/s[0.2479]]{'loss': 0.0421, 'learning_rate': 1.0833333333333335e-06, 'epoch': 2.94}\n",
            "Training:  98% 58800/60000 [2:51:12<03:27,  5.78it/s[0.0421]]{'loss': 0.0912, 'learning_rate': 1.0000000000000002e-06, 'epoch': 2.94}\n",
            "Training:  98% 58900/60000 [2:51:29<03:10,  5.78it/s[0.0912]]{'loss': 0.1978, 'learning_rate': 9.166666666666667e-07, 'epoch': 2.94}\n",
            "Training:  98% 59000/60000 [2:51:47<02:52,  5.78it/s[0.1978]]{'loss': 0.0784, 'learning_rate': 8.333333333333333e-07, 'epoch': 2.95}\n",
            "Training:  98% 59100/60000 [2:52:04<02:35,  5.78it/s[0.0784]]{'loss': 0.1651, 'learning_rate': 7.5e-07, 'epoch': 2.96}\n",
            "Training:  99% 59200/60000 [2:52:21<02:18,  5.76it/s[0.1651]]{'loss': 0.0295, 'learning_rate': 6.666666666666667e-07, 'epoch': 2.96}\n",
            "Training:  99% 59300/60000 [2:52:39<02:01,  5.77it/s[0.0295]]{'loss': 0.0858, 'learning_rate': 5.833333333333334e-07, 'epoch': 2.96}\n",
            "Training:  99% 59400/60000 [2:52:56<01:43,  5.78it/s[0.0858]]{'loss': 0.0937, 'learning_rate': 5.000000000000001e-07, 'epoch': 2.97}\n",
            "Training:  99% 59500/60000 [2:53:13<01:26,  5.77it/s[0.0937]]{'loss': 0.2871, 'learning_rate': 4.1666666666666667e-07, 'epoch': 2.98}\n",
            "Training:  99% 59600/60000 [2:53:31<01:09,  5.77it/s[0.2871]]{'loss': 0.3128, 'learning_rate': 3.3333333333333335e-07, 'epoch': 2.98}\n",
            "Training: 100% 59700/60000 [2:53:48<00:51,  5.77it/s[0.3128]]{'loss': 0.0501, 'learning_rate': 2.5000000000000004e-07, 'epoch': 2.98}\n",
            "Training: 100% 59800/60000 [2:54:05<00:34,  5.78it/s[0.0501]]{'loss': 0.1273, 'learning_rate': 1.6666666666666668e-07, 'epoch': 2.99}\n",
            "Training: 100% 59900/60000 [2:54:23<00:17,  5.77it/s[0.1273]]{'loss': 0.3043, 'learning_rate': 8.333333333333334e-08, 'epoch': 3.0}\n",
            "Training: 100% 60000/60000 [2:54:40<00:00,  5.76it/s[0.3043]]{'loss': 0.0661, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "Training: 100% 60000/60000 [2:54:50<00:00,  5.76it/s[0.3043]]{'eval_loss': 1.3602749109268188, 'eval_accuracy': 0.8336, 'eval_runtime': 35.0888, 'eval_samples_per_second': 142.496, 'eval_steps_per_second': 17.812, 'epoch': 3.0}\n",
            "{'train_runtime': 10596.7213, 'train_samples_per_second': 5.662, 'train_steps_per_second': 5.662, 'train_loss': 0.4764140326480071, 'epoch': 3.0}\n",
            "Training: 100% 60000/60000 [2:56:36<00:00,  5.66it/s[0.3043]]\n",
            "INFO:src.utils:Training completato.\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Training completato con metriche finali: {'train_runtime': 10596.7213, 'train_samples_per_second': 5.662, 'train_steps_per_second': 5.662, 'total_flos': 5.80011687936e+16, 'train_loss': 0.4764140326480071, 'epoch': 3.0, 'step': 60000}\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DLA_LLMSANALYSIS/main.py\", line 80, in <module>\n",
            "    main()\n",
            "  File \"/content/DLA_LLMSANALYSIS/main.py\", line 61, in main\n",
            "    model.train(output_dir=output_dir,\n",
            "  File \"/content/DLA_LLMSANALYSIS/src/architectures/model_gpt_neo_2_7b_imdb.py\", line 181, in train\n",
            "    json.dump(trainer.state.log_history, f, indent=4)\n",
            "    ^^^^\n",
            "NameError: name 'json' is not defined\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python main.py --model_config_key gpt_neo_2_7b --mode train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd2oTrp73ad3"
      },
      "source": [
        "### 3.3 **Evaluation decoder-only pre-trained**: EleutherAI/gpt-neo-2.7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LWr3w5Zf6Pjs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9669d3d-74b6-49e4-cd4f-524f604f2f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Usando la configurazione di default: {'model_name': 'EleutherAI/gpt-neo-2.7B', 'epochs': 3, 'train_batch_size': 1, 'eval_batch_size': 1, 'learning_rate': 0.0005, 'gradient_accumulation_steps': 8, 'repo_pretrained': 'models/pretrained/gpt-neo-2.7B', 'repo_finetuned': 'models/finetuned/gpt-neo-2.7B-imdb'}\n",
            "INFO:__main__:I modelli verranno salvati in: models/gpt_neo_2_7b\n",
            "2025-04-16 22:10:52.248205: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-16 22:10:52.306812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-16 22:10:53.204125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-2.7B and are newly initialized: ['transformer.h.29.attn.attention.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.11.attn.attention.bias', 'transformer.h.25.attn.attention.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.13.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'score.weight', 'transformer.h.17.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.31.attn.attention.bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.27.attn.attention.bias', 'transformer.h.19.attn.attention.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:PAD token id (tokenizer): 50256\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:PAD token id (model config): 50256\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Congelo i primi 31 blocchi di GPT-Neo.\n",
            "INFO:root:Caricamento dataset IMDb direttamente da Stanford...\n",
            "INFO:root:Dataset archive already exists, skipping download.\n",
            "INFO:root:Dataset already extracted, skipping extraction.\n",
            "INFO:root:Processing training data...\n",
            "INFO:root:Processing test data...\n",
            "INFO:root:Dataset caricato con successo: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "})\n",
            "INFO:root:Split disponibili: dict_keys(['train', 'test'])\n",
            "INFO:root:Numero di esempi - Train: 25000, Test: 25000\n",
            "INFO:root:Creazione split train/val/test...\n",
            "INFO:root:Train size: 20000, Val size: 5000, Test size: 25000\n",
            "INFO:__main__:Modalità EVAL: avvio dell'evaluation per il modello gpt_neo_2_7b.\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Valutazione sul modello pre-addestrato...\n",
            "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-2.7B and are newly initialized: ['transformer.h.29.attn.attention.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.11.attn.attention.bias', 'transformer.h.25.attn.attention.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.13.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'score.weight', 'transformer.h.17.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.31.attn.attention.bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.27.attn.attention.bias', 'transformer.h.19.attn.attention.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Carico il modello pre-addestrato da EleutherAI/gpt-neo-2.7B\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Inizio valutazione completa (pre-addestrato)...\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Esecuzione valutazione finale completa sul test set...\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:\n",
            "Metriche finali complete (test set):\n",
            "Accuracy: 0.4742\n",
            "Precision: 0.4576\n",
            "Recall: 0.2792\n",
            "F1 Score: 0.3468\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:\n",
            "Classification Report Test:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo     0.4814    0.6691    0.5600     12500\n",
            "    positivo     0.4576    0.2792    0.3468     12500\n",
            "\n",
            "    accuracy                         0.4742     25000\n",
            "   macro avg     0.4695    0.4742    0.4534     25000\n",
            "weighted avg     0.4695    0.4742    0.4534     25000\n",
            "\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Saved pretrained evaluation results to results/evaluation/pretrained/gpt-neo-2.7b-imdb.json\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Valutazione completata con risultati: {'accuracy': 0.47416, 'precision': 0.45764489902963545, 'recall': 0.2792, 'f1': 0.3468150650899335, 'classification_report': '              precision    recall  f1-score   support\\n\\n    negativo     0.4814    0.6691    0.5600     12500\\n    positivo     0.4576    0.2792    0.3468     12500\\n\\n    accuracy                         0.4742     25000\\n   macro avg     0.4695    0.4742    0.4534     25000\\nweighted avg     0.4695    0.4742    0.4534     25000\\n'}\n",
            "INFO:__main__:Risultati evaluation: {'accuracy': 0.47416, 'precision': 0.45764489902963545, 'recall': 0.2792, 'f1': 0.3468150650899335, 'classification_report': '              precision    recall  f1-score   support\\n\\n    negativo     0.4814    0.6691    0.5600     12500\\n    positivo     0.4576    0.2792    0.3468     12500\\n\\n    accuracy                         0.4742     25000\\n   macro avg     0.4695    0.4742    0.4534     25000\\nweighted avg     0.4695    0.4742    0.4534     25000\\n'}\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python main.py --model_config_key gpt_neo_2_7b --mode eval --eval_type pretrained --output_json_path \"results/evaluation/pretrained/gpt-neo-2.7b-imdb.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0mrDgNT3jmD"
      },
      "source": [
        "### 3.3 **Evaluation decoder-only fine-tuned**: EleutherAI/gpt-neo-2.7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5u8iVrhS6hYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39548ae4-463f-49ba-a94e-c7545b9d13b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Usando la configurazione di default: {'model_name': 'EleutherAI/gpt-neo-2.7B', 'epochs': 3, 'train_batch_size': 1, 'eval_batch_size': 1, 'learning_rate': 0.0005, 'gradient_accumulation_steps': 8, 'repo_pretrained': 'models/pretrained/gpt-neo-2.7B', 'repo_finetuned': 'models/finetuned/gpt-neo-2.7B-imdb'}\n",
            "INFO:__main__:I modelli verranno salvati in: models/gpt_neo_2_7b\n",
            "2025-04-16 22:22:23.454327: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-16 22:22:23.515412: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-16 22:22:24.432347: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-2.7B and are newly initialized: ['transformer.h.3.attn.attention.bias', 'transformer.h.29.attn.attention.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'score.weight', 'transformer.h.17.attn.attention.bias', 'transformer.h.19.attn.attention.bias', 'transformer.h.11.attn.attention.bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.31.attn.attention.bias', 'transformer.h.13.attn.attention.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.25.attn.attention.bias', 'transformer.h.27.attn.attention.bias', 'transformer.h.5.attn.attention.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:PAD token id (tokenizer): 50256\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:PAD token id (model config): 50256\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Congelo i primi 31 blocchi di GPT-Neo.\n",
            "INFO:root:Caricamento dataset IMDb direttamente da Stanford...\n",
            "INFO:root:Dataset archive already exists, skipping download.\n",
            "INFO:root:Dataset already extracted, skipping extraction.\n",
            "INFO:root:Processing training data...\n",
            "INFO:root:Processing test data...\n",
            "INFO:root:Dataset caricato con successo: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "})\n",
            "INFO:root:Split disponibili: dict_keys(['train', 'test'])\n",
            "INFO:root:Numero di esempi - Train: 25000, Test: 25000\n",
            "INFO:root:Creazione split train/val/test...\n",
            "INFO:root:Train size: 20000, Val size: 5000, Test size: 25000\n",
            "INFO:__main__:Modalità EVAL: avvio dell'evaluation per il modello gpt_neo_2_7b.\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Carico il modello fine-tunato da models/finetuned/gpt-neo-2.7B-imdb\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.99s/it]\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Inizio valutazione completa (fine-tunato)...\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Esecuzione valutazione finale completa sul test set...\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:\n",
            "Metriche finali complete (test set):\n",
            "Accuracy: 0.8412\n",
            "Precision: 0.8538\n",
            "Recall: 0.8234\n",
            "F1 Score: 0.8384\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:\n",
            "Classification Report Test:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo     0.8295    0.8590    0.8440     12500\n",
            "    positivo     0.8538    0.8234    0.8384     12500\n",
            "\n",
            "    accuracy                         0.8412     25000\n",
            "   macro avg     0.8417    0.8412    0.8412     25000\n",
            "weighted avg     0.8417    0.8412    0.8412     25000\n",
            "\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Saved fine-tuned evaluation results to results/evaluation/finetuned/gpt-neo-2.7b-imdb.json\n",
            "INFO:src.architectures.model_gpt_neo_2_7b_imdb:Valutazione completata con risultati: {'accuracy': 0.84124, 'precision': 0.853836582330983, 'recall': 0.82344, 'f1': 0.8383628588882102, 'classification_report': '              precision    recall  f1-score   support\\n\\n    negativo     0.8295    0.8590    0.8440     12500\\n    positivo     0.8538    0.8234    0.8384     12500\\n\\n    accuracy                         0.8412     25000\\n   macro avg     0.8417    0.8412    0.8412     25000\\nweighted avg     0.8417    0.8412    0.8412     25000\\n'}\n",
            "INFO:__main__:Risultati evaluation: {'accuracy': 0.84124, 'precision': 0.853836582330983, 'recall': 0.82344, 'f1': 0.8383628588882102, 'classification_report': '              precision    recall  f1-score   support\\n\\n    negativo     0.8295    0.8590    0.8440     12500\\n    positivo     0.8538    0.8234    0.8384     12500\\n\\n    accuracy                         0.8412     25000\\n   macro avg     0.8417    0.8412    0.8412     25000\\nweighted avg     0.8417    0.8412    0.8412     25000\\n'}\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python main.py --model_config_key gpt_neo_2_7b --mode eval --eval_type fine_tuned --output_json_path \"results/evaluation/finetuned/gpt-neo-2.7b-imdb.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90FeuYFe4jc6"
      },
      "source": [
        "### 3.4 **Evaluation Ensemble (Majority Voting) fine-tuned**: encoder-only, encoder-decoder and decoder-only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YgQakG-k4vAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df642ea7-64cc-4343-e6c2-090cbd9ed6ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Usando la configurazione di default: {'model_names': ['bart_base', 'bert_base_uncased', 'gpt_neo_2_7b'], 'train_batch_size': 8, 'eval_batch_size': 4, 'epochs': 3, 'repo': 'models/ensemble/majority-voting-imdb'}\n",
            "INFO:__main__:I modelli verranno salvati in: models/ensemble_majority_voting\n",
            "2025-04-16 22:40:12.908177: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-16 22:40:12.968194: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-16 22:40:13.828087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DLA_LLMSANALYSIS/main.py\", line 80, in <module>\n",
            "    main()\n",
            "  File \"/content/DLA_LLMSANALYSIS/main.py\", line 54, in main\n",
            "    model = get_model(args.model_config_key)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/DLA_LLMSANALYSIS/src/model_factory.py\", line 44, in get_model\n",
            "    return model_class(repo=config.get('repo'))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/DLA_LLMSANALYSIS/src/architectures/model_ensemble_majority_voting.py\", line 20, in __init__\n",
            "    self.members = [get_model(name, **kwargs) for name in member_names]\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/DLA_LLMSANALYSIS/src/architectures/model_ensemble_majority_voting.py\", line 20, in <listcomp>\n",
            "    self.members = [get_model(name, **kwargs) for name in member_names]\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/DLA_LLMSANALYSIS/src/model_factory.py\", line 25, in get_model\n",
            "    raise ValueError(f\"Unknown model configuration key: {model_config_key}\")\n",
            "ValueError: Unknown model configuration key: bert-base-uncased-imdb\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python main.py --model_config_key ensemble_majority_voting --mode eval --eval_type fine_tuned --output_json_path \"results/evaluation/finetuned/ensemble-majority-voting-imdb.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI4FJi0D4be_"
      },
      "source": [
        "## 4. **UPLOADING ALL MODELS ON HUGGINGFACE REPOSITORY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HqQci6tlD4L",
        "outputId": "c879ca13-a0a3-4aca-d655-2c583365f25a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-16 22:00:22.417936: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-16 22:00:22.477982: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-16 22:00:23.339023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "🚀 Upload in corso per: bert-base-uncased-imdb\n",
            "✅ Copiato file di evaluation: results/evaluation/finetuned/bert-base-uncased-imdb.json\n",
            "✅ Copiato file di validation: results/validation/finetuned/bert-base-uncased-imdb_metrics.json\n",
            "INFO:root:Uploading ./models/finetuned/bert-base-uncased-imdb to Hugging Face as wakaflocka17/bert-imdb-finetuned...\n",
            "pytorch_model.bin:   0% 0.00/438M [00:00<?, ?B/s]\n",
            "Upload 2 LFS files:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "training_args.bin: 100% 3.64k/3.64k [00:00<00:00, 14.4kB/s]\n",
            "pytorch_model.bin: 100% 438M/438M [00:33<00:00, 13.2MB/s]\n",
            "\n",
            "Upload 2 LFS files: 100% 2/2 [00:33<00:00, 16.72s/it]\n",
            "INFO:root:Upload completato per wakaflocka17/bert-imdb-finetuned\n",
            "🚀 Upload in corso per: bart-base-imdb\n",
            "✅ Copiato file di evaluation: results/evaluation/finetuned/bart-base-imdb.json\n",
            "✅ Copiato file di validation: results/validation/finetuned/bart-base-imdb_metrics.json\n",
            "INFO:root:Uploading ./models/finetuned/bart-base-imdb to Hugging Face as wakaflocka17/bart-imdb-finetuned...\n",
            "Upload 2 LFS files:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model.bin:   0% 0.00/560M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "training_args.bin:   0% 0.00/3.58k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "training_args.bin: 100% 3.58k/3.58k [00:00<00:00, 14.1kB/s]\n",
            "\n",
            "pytorch_model.bin:   5% 27.5M/560M [00:00<00:15, 35.2MB/s]\u001b[A\n",
            "pytorch_model.bin:   6% 34.4M/560M [00:02<00:56, 9.38MB/s]\u001b[A\n",
            "pytorch_model.bin:   9% 47.9M/560M [00:02<00:31, 16.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  10% 55.2M/560M [00:03<00:33, 15.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  11% 64.0M/560M [00:03<00:31, 15.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  14% 78.8M/560M [00:03<00:18, 25.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  15% 86.1M/560M [00:04<00:21, 22.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  17% 96.0M/560M [00:04<00:21, 21.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  20% 111M/560M [00:05<00:13, 32.4MB/s] \u001b[A\n",
            "pytorch_model.bin:  21% 118M/560M [00:05<00:16, 27.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  23% 128M/560M [00:06<00:18, 23.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  26% 143M/560M [00:06<00:11, 35.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  27% 151M/560M [00:06<00:14, 28.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  29% 160M/560M [00:07<00:16, 25.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  31% 175M/560M [00:07<00:10, 36.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  33% 182M/560M [00:07<00:13, 27.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  34% 192M/560M [00:08<00:15, 24.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  37% 207M/560M [00:08<00:09, 35.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  38% 214M/560M [00:08<00:13, 24.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  40% 224M/560M [00:09<00:14, 23.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  43% 239M/560M [00:09<00:09, 34.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  44% 246M/560M [00:09<00:11, 27.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  46% 256M/560M [00:11<00:19, 15.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  48% 271M/560M [00:11<00:12, 23.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  50% 278M/560M [00:11<00:13, 21.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  51% 288M/560M [00:12<00:13, 20.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  54% 303M/560M [00:12<00:08, 29.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  55% 310M/560M [00:13<00:10, 24.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  57% 320M/560M [00:13<00:10, 22.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  60% 335M/560M [00:13<00:06, 33.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  61% 342M/560M [00:14<00:08, 26.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  63% 352M/560M [00:14<00:08, 23.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  65% 367M/560M [00:14<00:05, 34.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  67% 374M/560M [00:15<00:06, 28.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  69% 384M/560M [00:15<00:07, 24.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  71% 399M/560M [00:15<00:04, 35.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  73% 406M/560M [00:16<00:05, 27.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  74% 416M/560M [00:16<00:05, 24.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  77% 431M/560M [00:16<00:03, 35.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  78% 439M/560M [00:17<00:04, 27.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  80% 448M/560M [00:17<00:04, 23.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  83% 463M/560M [00:18<00:02, 35.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  84% 471M/560M [00:18<00:03, 27.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  86% 480M/560M [00:19<00:03, 23.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  88% 495M/560M [00:19<00:01, 34.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  90% 502M/560M [00:19<00:02, 28.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  91% 512M/560M [00:20<00:01, 25.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  94% 527M/560M [00:20<00:00, 36.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  95% 534M/560M [00:20<00:01, 25.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  97% 544M/560M [00:21<00:00, 22.6MB/s]\u001b[A\n",
            "pytorch_model.bin: 100% 560M/560M [00:22<00:00, 25.1MB/s]\n",
            "Upload 2 LFS files: 100% 2/2 [00:22<00:00, 11.34s/it]\n",
            "INFO:root:Upload completato per wakaflocka17/bart-imdb-finetuned\n",
            "🚀 Upload in corso per: gpt-neo-2.7B-imdb\n",
            "⚠️ File di evaluation mancante: results/evaluation/finetuned/gpt-neo-2.7B-imdb.json\n",
            "✅ Copiato file di validation: results/validation/finetuned/gpt-neo-2.7B-imdb_metrics.json\n",
            "INFO:root:Uploading ./models/finetuned/gpt-neo-2.7B-imdb to Hugging Face as wakaflocka17/gptneo-imdb-finetuned...\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/10.1G [00:00<?, ?B/s]\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/634M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "training_args.bin:   0% 0.00/3.64k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 14.0M/10.1G [00:00<01:16, 131MB/s]\n",
            "pytorch_model-00002-of-00002.bin:   0% 606k/634M [00:00<02:04, 5.08MB/s]\u001b[A\n",
            "training_args.bin: 100% 3.64k/3.64k [00:00<00:00, 12.8kB/s]\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   1% 4.55M/634M [00:00<00:54, 11.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 5.82M/634M [00:00<00:56, 11.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 27.2M/10.1G [00:00<04:30, 37.3MB/s]\n",
            "pytorch_model-00002-of-00002.bin:   1% 9.40M/634M [00:00<00:42, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 48.0M/10.1G [00:01<04:16, 39.2MB/s]\n",
            "pytorch_model-00002-of-00002.bin:   3% 18.1M/634M [00:01<00:56, 10.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 64.0M/10.1G [00:02<07:16, 23.0MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   1% 75.9M/10.1G [00:02<05:06, 32.8MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   1% 82.4M/10.1G [00:02<06:35, 25.3MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   1% 110M/10.1G [00:03<04:34, 36.4MB/s] \n",
            "pytorch_model-00001-of-00002.bin:   1% 117M/10.1G [00:03<06:05, 27.3MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   1% 128M/10.1G [00:04<06:50, 24.3MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   1% 142M/10.1G [00:04<04:48, 34.6MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   1% 149M/10.1G [00:05<06:05, 27.2MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   2% 160M/10.1G [00:05<06:25, 25.8MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   2% 181M/10.1G [00:06<05:47, 28.6MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   2% 202M/10.1G [00:06<05:10, 31.9MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  12% 75.2M/634M [00:06<00:46, 12.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 208M/10.1G [00:07<06:52, 24.0MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   2% 240M/10.1G [00:08<04:25, 37.2MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  16% 99.2M/634M [00:08<00:35, 15.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 247M/10.1G [00:08<06:18, 26.1MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   3% 271M/10.1G [00:09<04:44, 34.6MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  20% 128M/634M [00:09<00:27, 18.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 143M/634M [00:09<00:16, 29.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 279M/10.1G [00:09<07:18, 22.4MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   3% 286M/10.1G [00:10<06:17, 26.0MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   3% 291M/10.1G [00:10<07:25, 22.0MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   3% 303M/10.1G [00:10<05:30, 29.7MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   3% 308M/10.1G [00:11<07:13, 22.6MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   3% 312M/10.1G [00:11<06:55, 23.5MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   3% 316M/10.1G [00:12<14:03, 11.6MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  33% 208M/634M [00:12<00:18, 23.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 318M/10.1G [00:12<14:01, 11.6MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   3% 320M/10.1G [00:13<24:31, 6.65MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  38% 240M/634M [00:13<00:16, 23.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 331M/10.1G [00:13<11:56, 13.6MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   3% 352M/10.1G [00:14<06:24, 25.3MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  41% 261M/634M [00:14<00:19, 19.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 358M/10.1G [00:14<07:46, 20.9MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   4% 384M/10.1G [00:15<05:02, 32.1MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  45% 288M/634M [00:15<00:16, 21.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 415M/10.1G [00:16<04:40, 34.5MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   4% 448M/10.1G [00:17<04:58, 32.4MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  50% 316M/634M [00:18<00:33, 9.54MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 455M/10.1G [00:18<08:45, 18.4MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  51% 322M/634M [00:19<00:39, 7.85MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 480M/10.1G [00:19<07:19, 21.9MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   5% 487M/10.1G [00:20<07:51, 20.4MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  56% 352M/634M [00:20<00:17, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 512M/10.1G [00:20<05:18, 30.1MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   5% 519M/10.1G [00:21<06:08, 26.0MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  61% 384M/634M [00:21<00:11, 20.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 542M/10.1G [00:21<04:47, 33.3MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   5% 550M/10.1G [00:22<05:57, 26.7MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  66% 416M/634M [00:22<00:09, 22.5MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 431M/634M [00:22<00:05, 34.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 575M/10.1G [00:23<05:40, 28.0MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  71% 448M/634M [00:23<00:07, 23.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 582M/10.1G [00:23<06:40, 23.8MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   6% 607M/10.1G [00:24<04:55, 32.1MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  76% 480M/634M [00:24<00:06, 23.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 614M/10.1G [00:24<06:00, 26.4MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   6% 639M/10.1G [00:25<04:37, 34.1MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  81% 512M/634M [00:25<00:05, 23.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 646M/10.1G [00:26<05:48, 27.2MB/s]\n",
            "pytorch_model-00001-of-00002.bin:   7% 671M/10.1G [00:26<04:51, 32.4MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  86% 544M/634M [00:27<00:03, 24.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 678M/10.1G [00:27<05:40, 27.7MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  90% 567M/634M [00:27<00:02, 28.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 576M/634M [00:28<00:02, 23.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 591M/634M [00:28<00:01, 35.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 702M/10.1G [00:28<07:31, 20.8MB/s]\n",
            "pytorch_model-00002-of-00002.bin:  96% 608M/634M [00:29<00:01, 24.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 710M/10.1G [00:29<07:49, 20.0MB/s]\n",
            "pytorch_model-00002-of-00002.bin: 100% 634M/634M [00:30<00:00, 21.0MB/s]\n",
            "pytorch_model-00001-of-00002.bin: 100% 10.1G/10.1G [06:22<00:00, 26.4MB/s]\n",
            "\n",
            "\n",
            "\n",
            "Upload 3 LFS files: 100% 3/3 [06:23<00:00, 127.69s/it]\n",
            "INFO:root:Upload completato per wakaflocka17/gptneo-imdb-finetuned\n",
            "❌ ATTENZIONE: Cartella modello non trovata → ./models/ensemble/majority-voting-imdb\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python src/upload_models.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cHsZ841dmtqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgWJHMN249ST"
      },
      "source": [
        "## 5. **GENERATE .JSON RESULTS AND PLOT RESULTS**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!venv/bin/python aggregate_results.py \\\n",
        "  --input_dir results \\\n",
        "  --output_file results_aggregati.json"
      ],
      "metadata": {
        "id": "wjyuK0n1TSQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJDBOZnk-Zi8",
        "outputId": "19c58918-700d-44fd-a11d-8beecac4e04c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/DLA_LLMSANALYSIS/src/plot_results.py\", line 1, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/matplotlib/__init__.py\", line 1296, in <module>\n",
            "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
            "    ~~~~~~~~^^^^^^^^^^^\n",
            "  File \"/content/DLA_LLMSANALYSIS/venv/lib/python3.11/site-packages/matplotlib/__init__.py\", line 771, in __setitem__\n",
            "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
            "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n"
          ]
        }
      ],
      "source": [
        "!venv/bin/python plot_results.py \\\n",
        "    --results_file \"results/aggregate_results.json\" \\\n",
        "    --output_dir \"plots\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!venv/bin/python ensemble_analysis.py --ensemble_file results/ensemble/majority-voting-imdb.json --output_dir plots\n"
      ],
      "metadata": {
        "id": "FT9WxoJ5NMkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tgWJHMN249ST"
      ],
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "486570958be54bffbbd4c78b7af876b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_59220456973f4921ad674326033ccacd"
          }
        },
        "a866c168021242c38b2179d6479f43f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f4cd9d5920946448d22e0a0b4b2cb68",
            "placeholder": "​",
            "style": "IPY_MODEL_adaf34cf19f44622a0ad2f830b83f481",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "6d043ef893114fb9a490759f3e5747df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_72301ed6b97f42ed8db78b431f77757b",
            "placeholder": "​",
            "style": "IPY_MODEL_a27ab2715a1349fa9e8e8cc197425f22",
            "value": ""
          }
        },
        "0bfab857f2d848c68829c93b1d9848bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_3a91c1c825b84cbc800cac7f3dc68ce1",
            "style": "IPY_MODEL_6f711c1b111842c2a23caea512df0738",
            "value": true
          }
        },
        "583b175b871f4d57a65abb6d768efdcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_2758d76e240b45f2a21e45186ebba5f2",
            "style": "IPY_MODEL_68cd9951e169457489b6b25bcf99de49",
            "tooltip": ""
          }
        },
        "a8d34e608a7f4fce9a48a3ebbbd54bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7be167703cd14338ab90898b7363893e",
            "placeholder": "​",
            "style": "IPY_MODEL_11db7eb028c0421abbf24d03fd6b064c",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "59220456973f4921ad674326033ccacd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "2f4cd9d5920946448d22e0a0b4b2cb68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adaf34cf19f44622a0ad2f830b83f481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72301ed6b97f42ed8db78b431f77757b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a27ab2715a1349fa9e8e8cc197425f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a91c1c825b84cbc800cac7f3dc68ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f711c1b111842c2a23caea512df0738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2758d76e240b45f2a21e45186ebba5f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68cd9951e169457489b6b25bcf99de49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7be167703cd14338ab90898b7363893e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11db7eb028c0421abbf24d03fd6b064c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1f6cfe894fe4002948b616b05ffec3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fe779a4eea341c88d8d03123b683f9c",
            "placeholder": "​",
            "style": "IPY_MODEL_43f884b1d57b4905a1ab9507761391ba",
            "value": "Connecting..."
          }
        },
        "7fe779a4eea341c88d8d03123b683f9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43f884b1d57b4905a1ab9507761391ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}